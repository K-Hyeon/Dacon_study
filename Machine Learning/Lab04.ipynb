{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b4dZz_X9FGRV"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4g8-8OctFGRY",
    "outputId": "93022e62-a890-495a-c2b0-cefa9e110295"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(train_raw_img, train_label), (test_raw_img, test_label) = load_mnist(flatten=False, normalize=False)\n",
    "print(train_raw_img.shape)\n",
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6MaNVxpFGRX"
   },
   "source": [
    "## 2. Preprocessing & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uQ8U02CCFGRZ",
    "outputId": "1d7515c9-e956-4bc8-dcce-a7a3a856bc34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
       " array([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_label, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 각 레이블 분포\n",
    "sns.countplot(train_label)\n",
    "plt.title(\"Distribution of Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xZnGA_YUFGRa",
    "outputId": "4dd40e63-d080-4e5e-93ef-1d857d6f8b9d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5923\n",
      "(1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# train_dataset split according to the number\n",
    "new_train_img = [[] for _ in range(10)]\n",
    "new_train_label = [[] for _ in range(10)]\n",
    "\n",
    "for i in range(len(train_label)) :\n",
    "    new_train_img[train_label[i]].append(train_raw_img[i])\n",
    "    new_train_label[train_label[i]].append(train_label[i])\n",
    "\n",
    "print(len(new_train_img[0])) # 0에 해당하는 image 개수\n",
    "print(new_train_img[0][0].shape) # 0에 해당하는 image중 첫번째 image의 shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 숫자 0의 이미지 샘픔들을 시각화\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(new_train_img[0][i].squeeze(), cmap=\"gray\")  # Change this line\n",
    "    plt.axis(\"off\")\n",
    "plt.suptitle(\"Samples of 0s\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[숫자 0]\n",
      "평균 픽셀 값: 44.21682790539819\n",
      "픽셀 값의 표준 편차: 88.66807293975512\n",
      "\n",
      "[0이 아닌 숫자]\n",
      "평균 픽셀 값: 32.12472983534816\n",
      "픽셀 값의 표준 편차: 77.28778949922649\n"
     ]
    }
   ],
   "source": [
    "# 0인 이미지와 0이 아닌 이미지의 픽셀 값의 평균 및 표준 편차를 계산\n",
    "mean_0 = np.mean(new_train_img[0])\n",
    "std_0 = np.std(new_train_img[0])\n",
    "\n",
    "non_zero_imgs = [img for i in range(1, 10) for img in new_train_img[i]]\n",
    "mean_non_0 = np.mean(non_zero_imgs)\n",
    "std_non_0 = np.std(non_zero_imgs)\n",
    "\n",
    "print(\"[숫자 0]\")\n",
    "print(f\"평균 픽셀 값: {mean_0}\")\n",
    "print(f\"픽셀 값의 표준 편차: {std_0}\")\n",
    "print(\"\\n[0이 아닌 숫자]\")\n",
    "print(f\"평균 픽셀 값: {mean_non_0}\")\n",
    "print(f\"픽셀 값의 표준 편차: {std_non_0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7tJ7XHTFGRb"
   },
   "source": [
    "## 3. Create a classifier that distinguishes between zero and non-zero (using logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MYkumpg9FGRb"
   },
   "outputs": [],
   "source": [
    "# idx에 해당하는 숫자를 대상으로 샘플 데이터 생성\n",
    "def make_sample(idx) :\n",
    "    sample_img = []\n",
    "    sample_label = []\n",
    "    \n",
    "    # data sampling \n",
    "    for i in range(10) :\n",
    "        if i == idx :\n",
    "            sample_img += new_train_img[i][:1000]\n",
    "            sample_label += (new_train_label[i][:1000])\n",
    "        else :\n",
    "            sample_img += new_train_img[i][:111]\n",
    "            sample_label += (new_train_label[i][:111])\n",
    "\n",
    "    sample_img = np.array(sample_img)\n",
    "    sample_label = np.array(sample_label)\n",
    "    \n",
    "    # normalization (set value 0 ~ 1)\n",
    "    sample_img = sample_img.astype('float')/255\n",
    "    \n",
    "    # target number는 1, 아니면 0\n",
    "    sample_label = np.where(sample_label==idx, 1 ,0)\n",
    "    \n",
    "    # reshape\n",
    "    sample_img = sample_img.reshape(len(sample_img.squeeze()), -1)\n",
    "    sample_label = sample_label.reshape(len(sample_label.squeeze()), -1)\n",
    "    \n",
    "    return sample_img, sample_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1IhWbvTIFGRb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999, 1)\n"
     ]
    }
   ],
   "source": [
    "# idx = target number\n",
    "train_X, train_y = make_sample(idx = 0)\n",
    "# bias 추가\n",
    "train_X = np.insert(train_X, 0, 1, axis=1)\n",
    "\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy loss\n",
    "def CrossEntropyLoss(preds, y) :  \n",
    "    delta = 1e-7\n",
    "    loss = np.sum(-y*np.log(preds+delta)-(1-y)*np.log(1-preds+delta))/len(preds)\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "e8O2C8MLFGRc",
    "outputId": "28f55cf1-0bce-4c5b-8356-62b331f5aa90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  8.063079315186753\n"
     ]
    }
   ],
   "source": [
    "loss = CrossEntropyLoss(np.zeros((len(train_X), 1)), train_y)\n",
    "print('loss : ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "def train(X, y) :\n",
    "    w = np.random.randn(len(X[0]), 1) # \n",
    "    lr = 0.01 # learning rate(수정)\n",
    "    step = 0\n",
    "    acc = 0\n",
    "    \n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    \n",
    "    while (acc < 0.85) :\n",
    "        step += 1\n",
    "        correct = 0\n",
    "        \n",
    "        # predict\n",
    "        preds = 1 / (1+np.exp(-X.dot(w)))\n",
    "        loss = CrossEntropyLoss(preds, y)\n",
    "        \n",
    "        result = np.where(preds>0.5, 1, 0)\n",
    "        acc = np.sum(np.where(result==y, True, False))/len(preds)\n",
    "        \n",
    "        print(\"total step : %d \" % step)\n",
    "        print(\"error : %f, accuarcy : %f\" % (loss, acc))\n",
    "        \n",
    "        loss_history.append(loss)\n",
    "        acc_history.append(acc)\n",
    "        \n",
    "        # gradient descent 수행\n",
    "        gradient = np.dot(X.T, (preds - y)) / len(X)\n",
    "        w -= lr * gradient\n",
    "    \n",
    "    # loss와 accuracy 시각화\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(loss_history)\n",
    "    plt.title(\"Loss history\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(acc_history)\n",
    "    plt.title(\"Accuracy history\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "\n",
    "    plt.show()\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "nWXi1kuJFGRc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 1 \n",
      "error : 5.711405, accuarcy : 0.393197\n",
      "total step : 2 \n",
      "error : 5.625301, accuarcy : 0.391196\n",
      "total step : 3 \n",
      "error : 5.538689, accuarcy : 0.389195\n",
      "total step : 4 \n",
      "error : 5.451702, accuarcy : 0.387694\n",
      "total step : 5 \n",
      "error : 5.364486, accuarcy : 0.388194\n",
      "total step : 6 \n",
      "error : 5.277197, accuarcy : 0.389695\n",
      "total step : 7 \n",
      "error : 5.190005, accuarcy : 0.387694\n",
      "total step : 8 \n",
      "error : 5.103085, accuarcy : 0.386693\n",
      "total step : 9 \n",
      "error : 5.016618, accuarcy : 0.387694\n",
      "total step : 10 \n",
      "error : 4.930783, accuarcy : 0.391696\n",
      "total step : 11 \n",
      "error : 4.845751, accuarcy : 0.389695\n",
      "total step : 12 \n",
      "error : 4.761687, accuarcy : 0.391696\n",
      "total step : 13 \n",
      "error : 4.678744, accuarcy : 0.395198\n",
      "total step : 14 \n",
      "error : 4.597068, accuarcy : 0.393697\n",
      "total step : 15 \n",
      "error : 4.516789, accuarcy : 0.394197\n",
      "total step : 16 \n",
      "error : 4.438025, accuarcy : 0.396698\n",
      "total step : 17 \n",
      "error : 4.360875, accuarcy : 0.398699\n",
      "total step : 18 \n",
      "error : 4.285425, accuarcy : 0.399700\n",
      "total step : 19 \n",
      "error : 4.211747, accuarcy : 0.399200\n",
      "total step : 20 \n",
      "error : 4.139910, accuarcy : 0.399700\n",
      "total step : 21 \n",
      "error : 4.069972, accuarcy : 0.397699\n",
      "total step : 22 \n",
      "error : 4.001991, accuarcy : 0.401201\n",
      "total step : 23 \n",
      "error : 3.936013, accuarcy : 0.404202\n",
      "total step : 24 \n",
      "error : 3.872076, accuarcy : 0.410205\n",
      "total step : 25 \n",
      "error : 3.810204, accuarcy : 0.412206\n",
      "total step : 26 \n",
      "error : 3.750407, accuarcy : 0.412706\n",
      "total step : 27 \n",
      "error : 3.692677, accuarcy : 0.418209\n",
      "total step : 28 \n",
      "error : 3.636992, accuarcy : 0.422711\n",
      "total step : 29 \n",
      "error : 3.583315, accuarcy : 0.427214\n",
      "total step : 30 \n",
      "error : 3.531596, accuarcy : 0.435218\n",
      "total step : 31 \n",
      "error : 3.481777, accuarcy : 0.437719\n",
      "total step : 32 \n",
      "error : 3.433791, accuarcy : 0.441221\n",
      "total step : 33 \n",
      "error : 3.387565, accuarcy : 0.446223\n",
      "total step : 34 \n",
      "error : 3.343027, accuarcy : 0.446223\n",
      "total step : 35 \n",
      "error : 3.300101, accuarcy : 0.452226\n",
      "total step : 36 \n",
      "error : 3.258714, accuarcy : 0.454727\n",
      "total step : 37 \n",
      "error : 3.218793, accuarcy : 0.457229\n",
      "total step : 38 \n",
      "error : 3.180269, accuarcy : 0.457229\n",
      "total step : 39 \n",
      "error : 3.143072, accuarcy : 0.459230\n",
      "total step : 40 \n",
      "error : 3.107140, accuarcy : 0.463232\n",
      "total step : 41 \n",
      "error : 3.072411, accuarcy : 0.464232\n",
      "total step : 42 \n",
      "error : 3.038825, accuarcy : 0.465733\n",
      "total step : 43 \n",
      "error : 3.006328, accuarcy : 0.466733\n",
      "total step : 44 \n",
      "error : 2.974865, accuarcy : 0.469735\n",
      "total step : 45 \n",
      "error : 2.944386, accuarcy : 0.472236\n",
      "total step : 46 \n",
      "error : 2.914842, accuarcy : 0.476238\n",
      "total step : 47 \n",
      "error : 2.886189, accuarcy : 0.480740\n",
      "total step : 48 \n",
      "error : 2.858381, accuarcy : 0.484742\n",
      "total step : 49 \n",
      "error : 2.831378, accuarcy : 0.487244\n",
      "total step : 50 \n",
      "error : 2.805140, accuarcy : 0.490245\n",
      "total step : 51 \n",
      "error : 2.779631, accuarcy : 0.491246\n",
      "total step : 52 \n",
      "error : 2.754815, accuarcy : 0.494247\n",
      "total step : 53 \n",
      "error : 2.730658, accuarcy : 0.498249\n",
      "total step : 54 \n",
      "error : 2.707130, accuarcy : 0.500250\n",
      "total step : 55 \n",
      "error : 2.684200, accuarcy : 0.503752\n",
      "total step : 56 \n",
      "error : 2.661840, accuarcy : 0.507254\n",
      "total step : 57 \n",
      "error : 2.640024, accuarcy : 0.508254\n",
      "total step : 58 \n",
      "error : 2.618727, accuarcy : 0.509755\n",
      "total step : 59 \n",
      "error : 2.597925, accuarcy : 0.512756\n",
      "total step : 60 \n",
      "error : 2.577595, accuarcy : 0.516758\n",
      "total step : 61 \n",
      "error : 2.557717, accuarcy : 0.517259\n",
      "total step : 62 \n",
      "error : 2.538270, accuarcy : 0.520760\n",
      "total step : 63 \n",
      "error : 2.519236, accuarcy : 0.521761\n",
      "total step : 64 \n",
      "error : 2.500596, accuarcy : 0.523762\n",
      "total step : 65 \n",
      "error : 2.482334, accuarcy : 0.529265\n",
      "total step : 66 \n",
      "error : 2.464434, accuarcy : 0.530765\n",
      "total step : 67 \n",
      "error : 2.446880, accuarcy : 0.532766\n",
      "total step : 68 \n",
      "error : 2.429658, accuarcy : 0.534267\n",
      "total step : 69 \n",
      "error : 2.412755, accuarcy : 0.535268\n",
      "total step : 70 \n",
      "error : 2.396158, accuarcy : 0.537269\n",
      "total step : 71 \n",
      "error : 2.379855, accuarcy : 0.538769\n",
      "total step : 72 \n",
      "error : 2.363834, accuarcy : 0.540770\n",
      "total step : 73 \n",
      "error : 2.348085, accuarcy : 0.542271\n",
      "total step : 74 \n",
      "error : 2.332596, accuarcy : 0.543772\n",
      "total step : 75 \n",
      "error : 2.317359, accuarcy : 0.548274\n",
      "total step : 76 \n",
      "error : 2.302364, accuarcy : 0.550775\n",
      "total step : 77 \n",
      "error : 2.287602, accuarcy : 0.552276\n",
      "total step : 78 \n",
      "error : 2.273065, accuarcy : 0.553777\n",
      "total step : 79 \n",
      "error : 2.258744, accuarcy : 0.555278\n",
      "total step : 80 \n",
      "error : 2.244633, accuarcy : 0.556278\n",
      "total step : 81 \n",
      "error : 2.230725, accuarcy : 0.559780\n",
      "total step : 82 \n",
      "error : 2.217012, accuarcy : 0.560280\n",
      "total step : 83 \n",
      "error : 2.203488, accuarcy : 0.561281\n",
      "total step : 84 \n",
      "error : 2.190147, accuarcy : 0.561781\n",
      "total step : 85 \n",
      "error : 2.176983, accuarcy : 0.563782\n",
      "total step : 86 \n",
      "error : 2.163991, accuarcy : 0.565783\n",
      "total step : 87 \n",
      "error : 2.151165, accuarcy : 0.569285\n",
      "total step : 88 \n",
      "error : 2.138501, accuarcy : 0.570285\n",
      "total step : 89 \n",
      "error : 2.125994, accuarcy : 0.571286\n",
      "total step : 90 \n",
      "error : 2.113640, accuarcy : 0.573787\n",
      "total step : 91 \n",
      "error : 2.101433, accuarcy : 0.575288\n",
      "total step : 92 \n",
      "error : 2.089371, accuarcy : 0.576288\n",
      "total step : 93 \n",
      "error : 2.077450, accuarcy : 0.579290\n",
      "total step : 94 \n",
      "error : 2.065665, accuarcy : 0.581291\n",
      "total step : 95 \n",
      "error : 2.054013, accuarcy : 0.586293\n",
      "total step : 96 \n",
      "error : 2.042491, accuarcy : 0.588294\n",
      "total step : 97 \n",
      "error : 2.031096, accuarcy : 0.589295\n",
      "total step : 98 \n",
      "error : 2.019826, accuarcy : 0.590795\n",
      "total step : 99 \n",
      "error : 2.008676, accuarcy : 0.591296\n",
      "total step : 100 \n",
      "error : 1.997644, accuarcy : 0.592796\n",
      "total step : 101 \n",
      "error : 1.986729, accuarcy : 0.593297\n",
      "total step : 102 \n",
      "error : 1.975927, accuarcy : 0.594297\n",
      "total step : 103 \n",
      "error : 1.965236, accuarcy : 0.596298\n",
      "total step : 104 \n",
      "error : 1.954654, accuarcy : 0.598799\n",
      "total step : 105 \n",
      "error : 1.944178, accuarcy : 0.599300\n",
      "total step : 106 \n",
      "error : 1.933807, accuarcy : 0.600800\n",
      "total step : 107 \n",
      "error : 1.923539, accuarcy : 0.602801\n",
      "total step : 108 \n",
      "error : 1.913372, accuarcy : 0.602301\n",
      "total step : 109 \n",
      "error : 1.903304, accuarcy : 0.603802\n",
      "total step : 110 \n",
      "error : 1.893334, accuarcy : 0.604802\n",
      "total step : 111 \n",
      "error : 1.883459, accuarcy : 0.606303\n",
      "total step : 112 \n",
      "error : 1.873679, accuarcy : 0.607304\n",
      "total step : 113 \n",
      "error : 1.863991, accuarcy : 0.609805\n",
      "total step : 114 \n",
      "error : 1.854394, accuarcy : 0.612306\n",
      "total step : 115 \n",
      "error : 1.844887, accuarcy : 0.612806\n",
      "total step : 116 \n",
      "error : 1.835469, accuarcy : 0.612806\n",
      "total step : 117 \n",
      "error : 1.826138, accuarcy : 0.613807\n",
      "total step : 118 \n",
      "error : 1.816893, accuarcy : 0.614807\n",
      "total step : 119 \n",
      "error : 1.807733, accuarcy : 0.617809\n",
      "total step : 120 \n",
      "error : 1.798657, accuarcy : 0.618809\n",
      "total step : 121 \n",
      "error : 1.789663, accuarcy : 0.620810\n",
      "total step : 122 \n",
      "error : 1.780750, accuarcy : 0.622311\n",
      "total step : 123 \n",
      "error : 1.771918, accuarcy : 0.622311\n",
      "total step : 124 \n",
      "error : 1.763165, accuarcy : 0.622811\n",
      "total step : 125 \n",
      "error : 1.754490, accuarcy : 0.624312\n",
      "total step : 126 \n",
      "error : 1.745893, accuarcy : 0.625313\n",
      "total step : 127 \n",
      "error : 1.737372, accuarcy : 0.626313\n",
      "total step : 128 \n",
      "error : 1.728927, accuarcy : 0.627814\n",
      "total step : 129 \n",
      "error : 1.720556, accuarcy : 0.629315\n",
      "total step : 130 \n",
      "error : 1.712259, accuarcy : 0.630315\n",
      "total step : 131 \n",
      "error : 1.704035, accuarcy : 0.631816\n",
      "total step : 132 \n",
      "error : 1.695883, accuarcy : 0.634317\n",
      "total step : 133 \n",
      "error : 1.687802, accuarcy : 0.635318\n",
      "total step : 134 \n",
      "error : 1.679792, accuarcy : 0.635318\n",
      "total step : 135 \n",
      "error : 1.671852, accuarcy : 0.636318\n",
      "total step : 136 \n",
      "error : 1.663980, accuarcy : 0.636818\n",
      "total step : 137 \n",
      "error : 1.656176, accuarcy : 0.638819\n",
      "total step : 138 \n",
      "error : 1.648440, accuarcy : 0.639320\n",
      "total step : 139 \n",
      "error : 1.640771, accuarcy : 0.641321\n",
      "total step : 140 \n",
      "error : 1.633168, accuarcy : 0.642821\n",
      "total step : 141 \n",
      "error : 1.625630, accuarcy : 0.644322\n",
      "total step : 142 \n",
      "error : 1.618157, accuarcy : 0.645823\n",
      "total step : 143 \n",
      "error : 1.610748, accuarcy : 0.645823\n",
      "total step : 144 \n",
      "error : 1.603402, accuarcy : 0.646823\n",
      "total step : 145 \n",
      "error : 1.596119, accuarcy : 0.648324\n",
      "total step : 146 \n",
      "error : 1.588898, accuarcy : 0.649825\n",
      "total step : 147 \n",
      "error : 1.581738, accuarcy : 0.650825\n",
      "total step : 148 \n",
      "error : 1.574640, accuarcy : 0.651326\n",
      "total step : 149 \n",
      "error : 1.567601, accuarcy : 0.652826\n",
      "total step : 150 \n",
      "error : 1.560622, accuarcy : 0.654827\n",
      "total step : 151 \n",
      "error : 1.553703, accuarcy : 0.656328\n",
      "total step : 152 \n",
      "error : 1.546841, accuarcy : 0.656828\n",
      "total step : 153 \n",
      "error : 1.540038, accuarcy : 0.659830\n",
      "total step : 154 \n",
      "error : 1.533291, accuarcy : 0.660330\n",
      "total step : 155 \n",
      "error : 1.526602, accuarcy : 0.661831\n",
      "total step : 156 \n",
      "error : 1.519968, accuarcy : 0.662331\n",
      "total step : 157 \n",
      "error : 1.513391, accuarcy : 0.663332\n",
      "total step : 158 \n",
      "error : 1.506868, accuarcy : 0.663832\n",
      "total step : 159 \n",
      "error : 1.500400, accuarcy : 0.664332\n",
      "total step : 160 \n",
      "error : 1.493986, accuarcy : 0.664332\n",
      "total step : 161 \n",
      "error : 1.487625, accuarcy : 0.665333\n",
      "total step : 162 \n",
      "error : 1.481317, accuarcy : 0.666333\n",
      "total step : 163 \n",
      "error : 1.475062, accuarcy : 0.667834\n",
      "total step : 164 \n",
      "error : 1.468858, accuarcy : 0.668834\n",
      "total step : 165 \n",
      "error : 1.462706, accuarcy : 0.669835\n",
      "total step : 166 \n",
      "error : 1.456605, accuarcy : 0.672836\n",
      "total step : 167 \n",
      "error : 1.450554, accuarcy : 0.672836\n",
      "total step : 168 \n",
      "error : 1.444553, accuarcy : 0.676338\n",
      "total step : 169 \n",
      "error : 1.438602, accuarcy : 0.677839\n",
      "total step : 170 \n",
      "error : 1.432699, accuarcy : 0.678839\n",
      "total step : 171 \n",
      "error : 1.426845, accuarcy : 0.679840\n",
      "total step : 172 \n",
      "error : 1.421039, accuarcy : 0.680340\n",
      "total step : 173 \n",
      "error : 1.415280, accuarcy : 0.680840\n",
      "total step : 174 \n",
      "error : 1.409569, accuarcy : 0.680840\n",
      "total step : 175 \n",
      "error : 1.403903, accuarcy : 0.681341\n",
      "total step : 176 \n",
      "error : 1.398284, accuarcy : 0.683342\n",
      "total step : 177 \n",
      "error : 1.392711, accuarcy : 0.684342\n",
      "total step : 178 \n",
      "error : 1.387183, accuarcy : 0.684342\n",
      "total step : 179 \n",
      "error : 1.381699, accuarcy : 0.684842\n",
      "total step : 180 \n",
      "error : 1.376260, accuarcy : 0.685343\n",
      "total step : 181 \n",
      "error : 1.370864, accuarcy : 0.686343\n",
      "total step : 182 \n",
      "error : 1.365512, accuarcy : 0.687344\n",
      "total step : 183 \n",
      "error : 1.360203, accuarcy : 0.687844\n",
      "total step : 184 \n",
      "error : 1.354937, accuarcy : 0.688844\n",
      "total step : 185 \n",
      "error : 1.349713, accuarcy : 0.690845\n",
      "total step : 186 \n",
      "error : 1.344530, accuarcy : 0.691346\n",
      "total step : 187 \n",
      "error : 1.339389, accuarcy : 0.692346\n",
      "total step : 188 \n",
      "error : 1.334288, accuarcy : 0.693847\n",
      "total step : 189 \n",
      "error : 1.329228, accuarcy : 0.693847\n",
      "total step : 190 \n",
      "error : 1.324208, accuarcy : 0.695848\n",
      "total step : 191 \n",
      "error : 1.319228, accuarcy : 0.696848\n",
      "total step : 192 \n",
      "error : 1.314287, accuarcy : 0.698849\n",
      "total step : 193 \n",
      "error : 1.309385, accuarcy : 0.700850\n",
      "total step : 194 \n",
      "error : 1.304522, accuarcy : 0.701851\n",
      "total step : 195 \n",
      "error : 1.299696, accuarcy : 0.702851\n",
      "total step : 196 \n",
      "error : 1.294909, accuarcy : 0.703852\n",
      "total step : 197 \n",
      "error : 1.290158, accuarcy : 0.704352\n",
      "total step : 198 \n",
      "error : 1.285445, accuarcy : 0.706353\n",
      "total step : 199 \n",
      "error : 1.280768, accuarcy : 0.708354\n",
      "total step : 200 \n",
      "error : 1.276128, accuarcy : 0.709355\n",
      "total step : 201 \n",
      "error : 1.271523, accuarcy : 0.709355\n",
      "total step : 202 \n",
      "error : 1.266954, accuarcy : 0.709355\n",
      "total step : 203 \n",
      "error : 1.262420, accuarcy : 0.710855\n",
      "total step : 204 \n",
      "error : 1.257921, accuarcy : 0.712356\n",
      "total step : 205 \n",
      "error : 1.253457, accuarcy : 0.712356\n",
      "total step : 206 \n",
      "error : 1.249026, accuarcy : 0.712856\n",
      "total step : 207 \n",
      "error : 1.244630, accuarcy : 0.714357\n",
      "total step : 208 \n",
      "error : 1.240266, accuarcy : 0.714857\n",
      "total step : 209 \n",
      "error : 1.235936, accuarcy : 0.714857\n",
      "total step : 210 \n",
      "error : 1.231639, accuarcy : 0.715358\n",
      "total step : 211 \n",
      "error : 1.227374, accuarcy : 0.715858\n",
      "total step : 212 \n",
      "error : 1.223141, accuarcy : 0.716858\n",
      "total step : 213 \n",
      "error : 1.218940, accuarcy : 0.718859\n",
      "total step : 214 \n",
      "error : 1.214771, accuarcy : 0.718859\n",
      "total step : 215 \n",
      "error : 1.210633, accuarcy : 0.718859\n",
      "total step : 216 \n",
      "error : 1.206526, accuarcy : 0.720360\n",
      "total step : 217 \n",
      "error : 1.202449, accuarcy : 0.721361\n",
      "total step : 218 \n",
      "error : 1.198402, accuarcy : 0.722361\n",
      "total step : 219 \n",
      "error : 1.194386, accuarcy : 0.723362\n",
      "total step : 220 \n",
      "error : 1.190399, accuarcy : 0.723862\n",
      "total step : 221 \n",
      "error : 1.186442, accuarcy : 0.724362\n",
      "total step : 222 \n",
      "error : 1.182513, accuarcy : 0.724862\n",
      "total step : 223 \n",
      "error : 1.178614, accuarcy : 0.725363\n",
      "total step : 224 \n",
      "error : 1.174743, accuarcy : 0.727364\n",
      "total step : 225 \n",
      "error : 1.170900, accuarcy : 0.728864\n",
      "total step : 226 \n",
      "error : 1.167086, accuarcy : 0.729365\n",
      "total step : 227 \n",
      "error : 1.163299, accuarcy : 0.732366\n",
      "total step : 228 \n",
      "error : 1.159539, accuarcy : 0.732366\n",
      "total step : 229 \n",
      "error : 1.155806, accuarcy : 0.732366\n",
      "total step : 230 \n",
      "error : 1.152101, accuarcy : 0.732866\n",
      "total step : 231 \n",
      "error : 1.148422, accuarcy : 0.733367\n",
      "total step : 232 \n",
      "error : 1.144770, accuarcy : 0.733867\n",
      "total step : 233 \n",
      "error : 1.141143, accuarcy : 0.733867\n",
      "total step : 234 \n",
      "error : 1.137543, accuarcy : 0.734367\n",
      "total step : 235 \n",
      "error : 1.133968, accuarcy : 0.735868\n",
      "total step : 236 \n",
      "error : 1.130418, accuarcy : 0.735868\n",
      "total step : 237 \n",
      "error : 1.126894, accuarcy : 0.735368\n",
      "total step : 238 \n",
      "error : 1.123395, accuarcy : 0.735368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 239 \n",
      "error : 1.119920, accuarcy : 0.735368\n",
      "total step : 240 \n",
      "error : 1.116469, accuarcy : 0.736368\n",
      "total step : 241 \n",
      "error : 1.113043, accuarcy : 0.737369\n",
      "total step : 242 \n",
      "error : 1.109641, accuarcy : 0.737869\n",
      "total step : 243 \n",
      "error : 1.106262, accuarcy : 0.738369\n",
      "total step : 244 \n",
      "error : 1.102907, accuarcy : 0.739370\n",
      "total step : 245 \n",
      "error : 1.099576, accuarcy : 0.739370\n",
      "total step : 246 \n",
      "error : 1.096267, accuarcy : 0.739870\n",
      "total step : 247 \n",
      "error : 1.092981, accuarcy : 0.740370\n",
      "total step : 248 \n",
      "error : 1.089718, accuarcy : 0.741371\n",
      "total step : 249 \n",
      "error : 1.086477, accuarcy : 0.741371\n",
      "total step : 250 \n",
      "error : 1.083259, accuarcy : 0.741371\n",
      "total step : 251 \n",
      "error : 1.080062, accuarcy : 0.740870\n",
      "total step : 252 \n",
      "error : 1.076888, accuarcy : 0.742371\n",
      "total step : 253 \n",
      "error : 1.073734, accuarcy : 0.743372\n",
      "total step : 254 \n",
      "error : 1.070603, accuarcy : 0.743372\n",
      "total step : 255 \n",
      "error : 1.067492, accuarcy : 0.744872\n",
      "total step : 256 \n",
      "error : 1.064403, accuarcy : 0.745373\n",
      "total step : 257 \n",
      "error : 1.061334, accuarcy : 0.746373\n",
      "total step : 258 \n",
      "error : 1.058286, accuarcy : 0.747374\n",
      "total step : 259 \n",
      "error : 1.055258, accuarcy : 0.747874\n",
      "total step : 260 \n",
      "error : 1.052251, accuarcy : 0.748374\n",
      "total step : 261 \n",
      "error : 1.049264, accuarcy : 0.748374\n",
      "total step : 262 \n",
      "error : 1.046296, accuarcy : 0.748874\n",
      "total step : 263 \n",
      "error : 1.043349, accuarcy : 0.749875\n",
      "total step : 264 \n",
      "error : 1.040420, accuarcy : 0.751376\n",
      "total step : 265 \n",
      "error : 1.037512, accuarcy : 0.751876\n",
      "total step : 266 \n",
      "error : 1.034622, accuarcy : 0.751876\n",
      "total step : 267 \n",
      "error : 1.031751, accuarcy : 0.752376\n",
      "total step : 268 \n",
      "error : 1.028899, accuarcy : 0.753377\n",
      "total step : 269 \n",
      "error : 1.026066, accuarcy : 0.754377\n",
      "total step : 270 \n",
      "error : 1.023252, accuarcy : 0.756878\n",
      "total step : 271 \n",
      "error : 1.020455, accuarcy : 0.756878\n",
      "total step : 272 \n",
      "error : 1.017677, accuarcy : 0.757379\n",
      "total step : 273 \n",
      "error : 1.014917, accuarcy : 0.758379\n",
      "total step : 274 \n",
      "error : 1.012175, accuarcy : 0.759380\n",
      "total step : 275 \n",
      "error : 1.009450, accuarcy : 0.759380\n",
      "total step : 276 \n",
      "error : 1.006743, accuarcy : 0.759380\n",
      "total step : 277 \n",
      "error : 1.004054, accuarcy : 0.759380\n",
      "total step : 278 \n",
      "error : 1.001381, accuarcy : 0.759880\n",
      "total step : 279 \n",
      "error : 0.998726, accuarcy : 0.761381\n",
      "total step : 280 \n",
      "error : 0.996088, accuarcy : 0.761381\n",
      "total step : 281 \n",
      "error : 0.993466, accuarcy : 0.762881\n",
      "total step : 282 \n",
      "error : 0.990861, accuarcy : 0.763882\n",
      "total step : 283 \n",
      "error : 0.988273, accuarcy : 0.763882\n",
      "total step : 284 \n",
      "error : 0.985701, accuarcy : 0.764882\n",
      "total step : 285 \n",
      "error : 0.983145, accuarcy : 0.765383\n",
      "total step : 286 \n",
      "error : 0.980605, accuarcy : 0.765383\n",
      "total step : 287 \n",
      "error : 0.978082, accuarcy : 0.765383\n",
      "total step : 288 \n",
      "error : 0.975574, accuarcy : 0.765383\n",
      "total step : 289 \n",
      "error : 0.973081, accuarcy : 0.765883\n",
      "total step : 290 \n",
      "error : 0.970605, accuarcy : 0.765883\n",
      "total step : 291 \n",
      "error : 0.968143, accuarcy : 0.765883\n",
      "total step : 292 \n",
      "error : 0.965697, accuarcy : 0.766383\n",
      "total step : 293 \n",
      "error : 0.963266, accuarcy : 0.765883\n",
      "total step : 294 \n",
      "error : 0.960850, accuarcy : 0.766883\n",
      "total step : 295 \n",
      "error : 0.958449, accuarcy : 0.768884\n",
      "total step : 296 \n",
      "error : 0.956063, accuarcy : 0.768884\n",
      "total step : 297 \n",
      "error : 0.953692, accuarcy : 0.769385\n",
      "total step : 298 \n",
      "error : 0.951335, accuarcy : 0.769385\n",
      "total step : 299 \n",
      "error : 0.948992, accuarcy : 0.770385\n",
      "total step : 300 \n",
      "error : 0.946664, accuarcy : 0.770385\n",
      "total step : 301 \n",
      "error : 0.944350, accuarcy : 0.771886\n",
      "total step : 302 \n",
      "error : 0.942050, accuarcy : 0.772886\n",
      "total step : 303 \n",
      "error : 0.939763, accuarcy : 0.772886\n",
      "total step : 304 \n",
      "error : 0.937491, accuarcy : 0.773387\n",
      "total step : 305 \n",
      "error : 0.935232, accuarcy : 0.773887\n",
      "total step : 306 \n",
      "error : 0.932987, accuarcy : 0.774387\n",
      "total step : 307 \n",
      "error : 0.930756, accuarcy : 0.774387\n",
      "total step : 308 \n",
      "error : 0.928538, accuarcy : 0.774387\n",
      "total step : 309 \n",
      "error : 0.926333, accuarcy : 0.775888\n",
      "total step : 310 \n",
      "error : 0.924141, accuarcy : 0.776388\n",
      "total step : 311 \n",
      "error : 0.921962, accuarcy : 0.776888\n",
      "total step : 312 \n",
      "error : 0.919796, accuarcy : 0.778389\n",
      "total step : 313 \n",
      "error : 0.917643, accuarcy : 0.779390\n",
      "total step : 314 \n",
      "error : 0.915503, accuarcy : 0.779890\n",
      "total step : 315 \n",
      "error : 0.913375, accuarcy : 0.780390\n",
      "total step : 316 \n",
      "error : 0.911260, accuarcy : 0.780390\n",
      "total step : 317 \n",
      "error : 0.909157, accuarcy : 0.779890\n",
      "total step : 318 \n",
      "error : 0.907067, accuarcy : 0.780390\n",
      "total step : 319 \n",
      "error : 0.904989, accuarcy : 0.780390\n",
      "total step : 320 \n",
      "error : 0.902923, accuarcy : 0.780890\n",
      "total step : 321 \n",
      "error : 0.900869, accuarcy : 0.781891\n",
      "total step : 322 \n",
      "error : 0.898827, accuarcy : 0.781891\n",
      "total step : 323 \n",
      "error : 0.896796, accuarcy : 0.782391\n",
      "total step : 324 \n",
      "error : 0.894778, accuarcy : 0.782391\n",
      "total step : 325 \n",
      "error : 0.892771, accuarcy : 0.782891\n",
      "total step : 326 \n",
      "error : 0.890775, accuarcy : 0.783392\n",
      "total step : 327 \n",
      "error : 0.888792, accuarcy : 0.782891\n",
      "total step : 328 \n",
      "error : 0.886819, accuarcy : 0.782891\n",
      "total step : 329 \n",
      "error : 0.884858, accuarcy : 0.783892\n",
      "total step : 330 \n",
      "error : 0.882907, accuarcy : 0.784392\n",
      "total step : 331 \n",
      "error : 0.880968, accuarcy : 0.784392\n",
      "total step : 332 \n",
      "error : 0.879040, accuarcy : 0.784392\n",
      "total step : 333 \n",
      "error : 0.877123, accuarcy : 0.784892\n",
      "total step : 334 \n",
      "error : 0.875217, accuarcy : 0.785393\n",
      "total step : 335 \n",
      "error : 0.873321, accuarcy : 0.785393\n",
      "total step : 336 \n",
      "error : 0.871436, accuarcy : 0.785893\n",
      "total step : 337 \n",
      "error : 0.869562, accuarcy : 0.785893\n",
      "total step : 338 \n",
      "error : 0.867698, accuarcy : 0.786393\n",
      "total step : 339 \n",
      "error : 0.865845, accuarcy : 0.786893\n",
      "total step : 340 \n",
      "error : 0.864001, accuarcy : 0.787394\n",
      "total step : 341 \n",
      "error : 0.862169, accuarcy : 0.787894\n",
      "total step : 342 \n",
      "error : 0.860346, accuarcy : 0.787894\n",
      "total step : 343 \n",
      "error : 0.858533, accuarcy : 0.788894\n",
      "total step : 344 \n",
      "error : 0.856730, accuarcy : 0.789395\n",
      "total step : 345 \n",
      "error : 0.854938, accuarcy : 0.789895\n",
      "total step : 346 \n",
      "error : 0.853155, accuarcy : 0.789895\n",
      "total step : 347 \n",
      "error : 0.851381, accuarcy : 0.789895\n",
      "total step : 348 \n",
      "error : 0.849618, accuarcy : 0.789895\n",
      "total step : 349 \n",
      "error : 0.847864, accuarcy : 0.789895\n",
      "total step : 350 \n",
      "error : 0.846120, accuarcy : 0.789895\n",
      "total step : 351 \n",
      "error : 0.844385, accuarcy : 0.790895\n",
      "total step : 352 \n",
      "error : 0.842659, accuarcy : 0.790895\n",
      "total step : 353 \n",
      "error : 0.840943, accuarcy : 0.790895\n",
      "total step : 354 \n",
      "error : 0.839236, accuarcy : 0.791396\n",
      "total step : 355 \n",
      "error : 0.837538, accuarcy : 0.791896\n",
      "total step : 356 \n",
      "error : 0.835850, accuarcy : 0.792396\n",
      "total step : 357 \n",
      "error : 0.834170, accuarcy : 0.792396\n",
      "total step : 358 \n",
      "error : 0.832499, accuarcy : 0.793397\n",
      "total step : 359 \n",
      "error : 0.830838, accuarcy : 0.793897\n",
      "total step : 360 \n",
      "error : 0.829185, accuarcy : 0.794397\n",
      "total step : 361 \n",
      "error : 0.827540, accuarcy : 0.794397\n",
      "total step : 362 \n",
      "error : 0.825905, accuarcy : 0.794897\n",
      "total step : 363 \n",
      "error : 0.824278, accuarcy : 0.795898\n",
      "total step : 364 \n",
      "error : 0.822659, accuarcy : 0.796398\n",
      "total step : 365 \n",
      "error : 0.821049, accuarcy : 0.796898\n",
      "total step : 366 \n",
      "error : 0.819448, accuarcy : 0.797899\n",
      "total step : 367 \n",
      "error : 0.817855, accuarcy : 0.798399\n",
      "total step : 368 \n",
      "error : 0.816270, accuarcy : 0.798399\n",
      "total step : 369 \n",
      "error : 0.814693, accuarcy : 0.798399\n",
      "total step : 370 \n",
      "error : 0.813125, accuarcy : 0.798899\n",
      "total step : 371 \n",
      "error : 0.811564, accuarcy : 0.798899\n",
      "total step : 372 \n",
      "error : 0.810012, accuarcy : 0.799400\n",
      "total step : 373 \n",
      "error : 0.808468, accuarcy : 0.799900\n",
      "total step : 374 \n",
      "error : 0.806931, accuarcy : 0.800900\n",
      "total step : 375 \n",
      "error : 0.805403, accuarcy : 0.801401\n",
      "total step : 376 \n",
      "error : 0.803882, accuarcy : 0.801401\n",
      "total step : 377 \n",
      "error : 0.802369, accuarcy : 0.801901\n",
      "total step : 378 \n",
      "error : 0.800863, accuarcy : 0.802401\n",
      "total step : 379 \n",
      "error : 0.799366, accuarcy : 0.802901\n",
      "total step : 380 \n",
      "error : 0.797875, accuarcy : 0.802901\n",
      "total step : 381 \n",
      "error : 0.796393, accuarcy : 0.802901\n",
      "total step : 382 \n",
      "error : 0.794918, accuarcy : 0.802901\n",
      "total step : 383 \n",
      "error : 0.793450, accuarcy : 0.803402\n",
      "total step : 384 \n",
      "error : 0.791989, accuarcy : 0.803402\n",
      "total step : 385 \n",
      "error : 0.790536, accuarcy : 0.803902\n",
      "total step : 386 \n",
      "error : 0.789090, accuarcy : 0.805903\n",
      "total step : 387 \n",
      "error : 0.787651, accuarcy : 0.805903\n",
      "total step : 388 \n",
      "error : 0.786219, accuarcy : 0.805903\n",
      "total step : 389 \n",
      "error : 0.784795, accuarcy : 0.806903\n",
      "total step : 390 \n",
      "error : 0.783377, accuarcy : 0.807404\n",
      "total step : 391 \n",
      "error : 0.781967, accuarcy : 0.808404\n",
      "total step : 392 \n",
      "error : 0.780563, accuarcy : 0.808904\n",
      "total step : 393 \n",
      "error : 0.779166, accuarcy : 0.808904\n",
      "total step : 394 \n",
      "error : 0.777776, accuarcy : 0.808904\n",
      "total step : 395 \n",
      "error : 0.776393, accuarcy : 0.808904\n",
      "total step : 396 \n",
      "error : 0.775016, accuarcy : 0.808904\n",
      "total step : 397 \n",
      "error : 0.773646, accuarcy : 0.809405\n",
      "total step : 398 \n",
      "error : 0.772283, accuarcy : 0.809405\n",
      "total step : 399 \n",
      "error : 0.770926, accuarcy : 0.809905\n",
      "total step : 400 \n",
      "error : 0.769576, accuarcy : 0.810405\n",
      "total step : 401 \n",
      "error : 0.768232, accuarcy : 0.810405\n",
      "total step : 402 \n",
      "error : 0.766895, accuarcy : 0.810405\n",
      "total step : 403 \n",
      "error : 0.765564, accuarcy : 0.810405\n",
      "total step : 404 \n",
      "error : 0.764239, accuarcy : 0.810405\n",
      "total step : 405 \n",
      "error : 0.762921, accuarcy : 0.810905\n",
      "total step : 406 \n",
      "error : 0.761609, accuarcy : 0.810905\n",
      "total step : 407 \n",
      "error : 0.760303, accuarcy : 0.812406\n",
      "total step : 408 \n",
      "error : 0.759003, accuarcy : 0.812906\n",
      "total step : 409 \n",
      "error : 0.757709, accuarcy : 0.812906\n",
      "total step : 410 \n",
      "error : 0.756421, accuarcy : 0.812906\n",
      "total step : 411 \n",
      "error : 0.755140, accuarcy : 0.812906\n",
      "total step : 412 \n",
      "error : 0.753864, accuarcy : 0.812906\n",
      "total step : 413 \n",
      "error : 0.752595, accuarcy : 0.812906\n",
      "total step : 414 \n",
      "error : 0.751331, accuarcy : 0.813907\n",
      "total step : 415 \n",
      "error : 0.750073, accuarcy : 0.814907\n",
      "total step : 416 \n",
      "error : 0.748821, accuarcy : 0.815408\n",
      "total step : 417 \n",
      "error : 0.747574, accuarcy : 0.815408\n",
      "total step : 418 \n",
      "error : 0.746334, accuarcy : 0.815408\n",
      "total step : 419 \n",
      "error : 0.745099, accuarcy : 0.815908\n",
      "total step : 420 \n",
      "error : 0.743869, accuarcy : 0.816408\n",
      "total step : 421 \n",
      "error : 0.742645, accuarcy : 0.816408\n",
      "total step : 422 \n",
      "error : 0.741427, accuarcy : 0.816908\n",
      "total step : 423 \n",
      "error : 0.740215, accuarcy : 0.817409\n",
      "total step : 424 \n",
      "error : 0.739007, accuarcy : 0.817409\n",
      "total step : 425 \n",
      "error : 0.737806, accuarcy : 0.817409\n",
      "total step : 426 \n",
      "error : 0.736609, accuarcy : 0.817909\n",
      "total step : 427 \n",
      "error : 0.735418, accuarcy : 0.818409\n",
      "total step : 428 \n",
      "error : 0.734233, accuarcy : 0.818409\n",
      "total step : 429 \n",
      "error : 0.733052, accuarcy : 0.818409\n",
      "total step : 430 \n",
      "error : 0.731877, accuarcy : 0.818409\n",
      "total step : 431 \n",
      "error : 0.730707, accuarcy : 0.819410\n",
      "total step : 432 \n",
      "error : 0.729543, accuarcy : 0.819410\n",
      "total step : 433 \n",
      "error : 0.728383, accuarcy : 0.819410\n",
      "total step : 434 \n",
      "error : 0.727229, accuarcy : 0.818909\n",
      "total step : 435 \n",
      "error : 0.726080, accuarcy : 0.818909\n",
      "total step : 436 \n",
      "error : 0.724935, accuarcy : 0.819410\n",
      "total step : 437 \n",
      "error : 0.723796, accuarcy : 0.819910\n",
      "total step : 438 \n",
      "error : 0.722662, accuarcy : 0.820410\n",
      "total step : 439 \n",
      "error : 0.721532, accuarcy : 0.820410\n",
      "total step : 440 \n",
      "error : 0.720408, accuarcy : 0.820910\n",
      "total step : 441 \n",
      "error : 0.719288, accuarcy : 0.821411\n",
      "total step : 442 \n",
      "error : 0.718174, accuarcy : 0.821411\n",
      "total step : 443 \n",
      "error : 0.717064, accuarcy : 0.821411\n",
      "total step : 444 \n",
      "error : 0.715959, accuarcy : 0.821911\n",
      "total step : 445 \n",
      "error : 0.714858, accuarcy : 0.822411\n",
      "total step : 446 \n",
      "error : 0.713763, accuarcy : 0.822411\n",
      "total step : 447 \n",
      "error : 0.712672, accuarcy : 0.822411\n",
      "total step : 448 \n",
      "error : 0.711585, accuarcy : 0.823412\n",
      "total step : 449 \n",
      "error : 0.710504, accuarcy : 0.823412\n",
      "total step : 450 \n",
      "error : 0.709426, accuarcy : 0.824412\n",
      "total step : 451 \n",
      "error : 0.708354, accuarcy : 0.823912\n",
      "total step : 452 \n",
      "error : 0.707286, accuarcy : 0.824412\n",
      "total step : 453 \n",
      "error : 0.706222, accuarcy : 0.824412\n",
      "total step : 454 \n",
      "error : 0.705163, accuarcy : 0.824912\n",
      "total step : 455 \n",
      "error : 0.704109, accuarcy : 0.825413\n",
      "total step : 456 \n",
      "error : 0.703058, accuarcy : 0.825413\n",
      "total step : 457 \n",
      "error : 0.702012, accuarcy : 0.825413\n",
      "total step : 458 \n",
      "error : 0.700971, accuarcy : 0.825913\n",
      "total step : 459 \n",
      "error : 0.699934, accuarcy : 0.825913\n",
      "total step : 460 \n",
      "error : 0.698901, accuarcy : 0.826913\n",
      "total step : 461 \n",
      "error : 0.697872, accuarcy : 0.827914\n",
      "total step : 462 \n",
      "error : 0.696848, accuarcy : 0.827914\n",
      "total step : 463 \n",
      "error : 0.695828, accuarcy : 0.828414\n",
      "total step : 464 \n",
      "error : 0.694812, accuarcy : 0.828414\n",
      "total step : 465 \n",
      "error : 0.693800, accuarcy : 0.828414\n",
      "total step : 466 \n",
      "error : 0.692792, accuarcy : 0.828414\n",
      "total step : 467 \n",
      "error : 0.691789, accuarcy : 0.828414\n",
      "total step : 468 \n",
      "error : 0.690789, accuarcy : 0.828414\n",
      "total step : 469 \n",
      "error : 0.689794, accuarcy : 0.829415\n",
      "total step : 470 \n",
      "error : 0.688802, accuarcy : 0.829415\n",
      "total step : 471 \n",
      "error : 0.687815, accuarcy : 0.829415\n",
      "total step : 472 \n",
      "error : 0.686831, accuarcy : 0.829415\n",
      "total step : 473 \n",
      "error : 0.685852, accuarcy : 0.829915\n",
      "total step : 474 \n",
      "error : 0.684876, accuarcy : 0.829915\n",
      "total step : 475 \n",
      "error : 0.683904, accuarcy : 0.829915\n",
      "total step : 476 \n",
      "error : 0.682937, accuarcy : 0.830415\n",
      "total step : 477 \n",
      "error : 0.681973, accuarcy : 0.830915\n",
      "total step : 478 \n",
      "error : 0.681012, accuarcy : 0.830915\n",
      "total step : 479 \n",
      "error : 0.680056, accuarcy : 0.830915\n",
      "total step : 480 \n",
      "error : 0.679103, accuarcy : 0.830915\n",
      "total step : 481 \n",
      "error : 0.678155, accuarcy : 0.830415\n",
      "total step : 482 \n",
      "error : 0.677209, accuarcy : 0.830415\n",
      "total step : 483 \n",
      "error : 0.676268, accuarcy : 0.830915\n",
      "total step : 484 \n",
      "error : 0.675330, accuarcy : 0.831416\n",
      "total step : 485 \n",
      "error : 0.674396, accuarcy : 0.831416\n",
      "total step : 486 \n",
      "error : 0.673466, accuarcy : 0.831416\n",
      "total step : 487 \n",
      "error : 0.672539, accuarcy : 0.832416\n",
      "total step : 488 \n",
      "error : 0.671616, accuarcy : 0.832416\n",
      "total step : 489 \n",
      "error : 0.670696, accuarcy : 0.832416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 490 \n",
      "error : 0.669780, accuarcy : 0.832416\n",
      "total step : 491 \n",
      "error : 0.668867, accuarcy : 0.832916\n",
      "total step : 492 \n",
      "error : 0.667958, accuarcy : 0.832916\n",
      "total step : 493 \n",
      "error : 0.667053, accuarcy : 0.832916\n",
      "total step : 494 \n",
      "error : 0.666151, accuarcy : 0.832916\n",
      "total step : 495 \n",
      "error : 0.665252, accuarcy : 0.832916\n",
      "total step : 496 \n",
      "error : 0.664357, accuarcy : 0.833917\n",
      "total step : 497 \n",
      "error : 0.663465, accuarcy : 0.834417\n",
      "total step : 498 \n",
      "error : 0.662576, accuarcy : 0.834417\n",
      "total step : 499 \n",
      "error : 0.661691, accuarcy : 0.834417\n",
      "total step : 500 \n",
      "error : 0.660809, accuarcy : 0.834417\n",
      "total step : 501 \n",
      "error : 0.659931, accuarcy : 0.834417\n",
      "total step : 502 \n",
      "error : 0.659056, accuarcy : 0.834917\n",
      "total step : 503 \n",
      "error : 0.658184, accuarcy : 0.835418\n",
      "total step : 504 \n",
      "error : 0.657315, accuarcy : 0.835918\n",
      "total step : 505 \n",
      "error : 0.656450, accuarcy : 0.836418\n",
      "total step : 506 \n",
      "error : 0.655587, accuarcy : 0.836418\n",
      "total step : 507 \n",
      "error : 0.654728, accuarcy : 0.837419\n",
      "total step : 508 \n",
      "error : 0.653873, accuarcy : 0.837419\n",
      "total step : 509 \n",
      "error : 0.653020, accuarcy : 0.837419\n",
      "total step : 510 \n",
      "error : 0.652170, accuarcy : 0.837419\n",
      "total step : 511 \n",
      "error : 0.651324, accuarcy : 0.837419\n",
      "total step : 512 \n",
      "error : 0.650481, accuarcy : 0.837419\n",
      "total step : 513 \n",
      "error : 0.649641, accuarcy : 0.837419\n",
      "total step : 514 \n",
      "error : 0.648803, accuarcy : 0.837919\n",
      "total step : 515 \n",
      "error : 0.647969, accuarcy : 0.838419\n",
      "total step : 516 \n",
      "error : 0.647138, accuarcy : 0.838919\n",
      "total step : 517 \n",
      "error : 0.646310, accuarcy : 0.839420\n",
      "total step : 518 \n",
      "error : 0.645485, accuarcy : 0.839420\n",
      "total step : 519 \n",
      "error : 0.644663, accuarcy : 0.839420\n",
      "total step : 520 \n",
      "error : 0.643844, accuarcy : 0.839420\n",
      "total step : 521 \n",
      "error : 0.643028, accuarcy : 0.839420\n",
      "total step : 522 \n",
      "error : 0.642215, accuarcy : 0.839420\n",
      "total step : 523 \n",
      "error : 0.641404, accuarcy : 0.839920\n",
      "total step : 524 \n",
      "error : 0.640597, accuarcy : 0.840420\n",
      "total step : 525 \n",
      "error : 0.639792, accuarcy : 0.840920\n",
      "total step : 526 \n",
      "error : 0.638991, accuarcy : 0.841421\n",
      "total step : 527 \n",
      "error : 0.638192, accuarcy : 0.841421\n",
      "total step : 528 \n",
      "error : 0.637396, accuarcy : 0.841921\n",
      "total step : 529 \n",
      "error : 0.636603, accuarcy : 0.841921\n",
      "total step : 530 \n",
      "error : 0.635813, accuarcy : 0.841921\n",
      "total step : 531 \n",
      "error : 0.635025, accuarcy : 0.842421\n",
      "total step : 532 \n",
      "error : 0.634240, accuarcy : 0.842421\n",
      "total step : 533 \n",
      "error : 0.633458, accuarcy : 0.842421\n",
      "total step : 534 \n",
      "error : 0.632679, accuarcy : 0.842421\n",
      "total step : 535 \n",
      "error : 0.631902, accuarcy : 0.842421\n",
      "total step : 536 \n",
      "error : 0.631128, accuarcy : 0.842421\n",
      "total step : 537 \n",
      "error : 0.630357, accuarcy : 0.842421\n",
      "total step : 538 \n",
      "error : 0.629588, accuarcy : 0.843422\n",
      "total step : 539 \n",
      "error : 0.628823, accuarcy : 0.843422\n",
      "total step : 540 \n",
      "error : 0.628059, accuarcy : 0.843422\n",
      "total step : 541 \n",
      "error : 0.627299, accuarcy : 0.843422\n",
      "total step : 542 \n",
      "error : 0.626541, accuarcy : 0.843922\n",
      "total step : 543 \n",
      "error : 0.625785, accuarcy : 0.843922\n",
      "total step : 544 \n",
      "error : 0.625033, accuarcy : 0.844422\n",
      "total step : 545 \n",
      "error : 0.624282, accuarcy : 0.844422\n",
      "total step : 546 \n",
      "error : 0.623535, accuarcy : 0.844422\n",
      "total step : 547 \n",
      "error : 0.622790, accuarcy : 0.844422\n",
      "total step : 548 \n",
      "error : 0.622047, accuarcy : 0.844922\n",
      "total step : 549 \n",
      "error : 0.621307, accuarcy : 0.844922\n",
      "total step : 550 \n",
      "error : 0.620569, accuarcy : 0.845423\n",
      "total step : 551 \n",
      "error : 0.619834, accuarcy : 0.845923\n",
      "total step : 552 \n",
      "error : 0.619102, accuarcy : 0.845923\n",
      "total step : 553 \n",
      "error : 0.618372, accuarcy : 0.845923\n",
      "total step : 554 \n",
      "error : 0.617644, accuarcy : 0.845923\n",
      "total step : 555 \n",
      "error : 0.616919, accuarcy : 0.845923\n",
      "total step : 556 \n",
      "error : 0.616196, accuarcy : 0.846923\n",
      "total step : 557 \n",
      "error : 0.615475, accuarcy : 0.846923\n",
      "total step : 558 \n",
      "error : 0.614757, accuarcy : 0.846923\n",
      "total step : 559 \n",
      "error : 0.614042, accuarcy : 0.846923\n",
      "total step : 560 \n",
      "error : 0.613328, accuarcy : 0.846923\n",
      "total step : 561 \n",
      "error : 0.612618, accuarcy : 0.847424\n",
      "total step : 562 \n",
      "error : 0.611909, accuarcy : 0.847424\n",
      "total step : 563 \n",
      "error : 0.611203, accuarcy : 0.847424\n",
      "total step : 564 \n",
      "error : 0.610499, accuarcy : 0.847424\n",
      "total step : 565 \n",
      "error : 0.609797, accuarcy : 0.847424\n",
      "total step : 566 \n",
      "error : 0.609098, accuarcy : 0.847424\n",
      "total step : 567 \n",
      "error : 0.608401, accuarcy : 0.847424\n",
      "total step : 568 \n",
      "error : 0.607706, accuarcy : 0.847424\n",
      "total step : 569 \n",
      "error : 0.607014, accuarcy : 0.847924\n",
      "total step : 570 \n",
      "error : 0.606324, accuarcy : 0.848424\n",
      "total step : 571 \n",
      "error : 0.605636, accuarcy : 0.848424\n",
      "total step : 572 \n",
      "error : 0.604950, accuarcy : 0.848424\n",
      "total step : 573 \n",
      "error : 0.604267, accuarcy : 0.848924\n",
      "total step : 574 \n",
      "error : 0.603585, accuarcy : 0.848924\n",
      "total step : 575 \n",
      "error : 0.602906, accuarcy : 0.848924\n",
      "total step : 576 \n",
      "error : 0.602229, accuarcy : 0.848924\n",
      "total step : 577 \n",
      "error : 0.601554, accuarcy : 0.848924\n",
      "total step : 578 \n",
      "error : 0.600882, accuarcy : 0.848924\n",
      "total step : 579 \n",
      "error : 0.600211, accuarcy : 0.848924\n",
      "total step : 580 \n",
      "error : 0.599543, accuarcy : 0.848924\n",
      "total step : 581 \n",
      "error : 0.598877, accuarcy : 0.848924\n",
      "total step : 582 \n",
      "error : 0.598213, accuarcy : 0.849425\n",
      "total step : 583 \n",
      "error : 0.597551, accuarcy : 0.849925\n",
      "total step : 584 \n",
      "error : 0.596891, accuarcy : 0.849925\n",
      "total step : 585 \n",
      "error : 0.596233, accuarcy : 0.849925\n",
      "total step : 586 \n",
      "error : 0.595577, accuarcy : 0.849925\n",
      "total step : 587 \n",
      "error : 0.594924, accuarcy : 0.849925\n",
      "total step : 588 \n",
      "error : 0.594272, accuarcy : 0.849925\n",
      "total step : 589 \n",
      "error : 0.593623, accuarcy : 0.849425\n",
      "total step : 590 \n",
      "error : 0.592975, accuarcy : 0.849425\n",
      "total step : 591 \n",
      "error : 0.592330, accuarcy : 0.849425\n",
      "total step : 592 \n",
      "error : 0.591686, accuarcy : 0.849425\n",
      "total step : 593 \n",
      "error : 0.591045, accuarcy : 0.849425\n",
      "total step : 594 \n",
      "error : 0.590405, accuarcy : 0.849425\n",
      "total step : 595 \n",
      "error : 0.589768, accuarcy : 0.849425\n",
      "total step : 596 \n",
      "error : 0.589132, accuarcy : 0.849925\n",
      "total step : 597 \n",
      "error : 0.588499, accuarcy : 0.849925\n",
      "total step : 598 \n",
      "error : 0.587867, accuarcy : 0.850425\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save weight\n",
    "w = train(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  0.8319\n"
     ]
    }
   ],
   "source": [
    "# eval (accuracy)\n",
    "def eval(idx, w) :\n",
    "    test_X = test_raw_img.astype('float')/255    \n",
    "    test_X = test_X.reshape(len(test_X.squeeze()), -1)\n",
    "    # bias\n",
    "    test_X = np.insert(test_X, 0, 1, axis=1) \n",
    "\n",
    "    test_y = np.where(test_label==idx, 1 ,0)\n",
    "    test_y = test_y.reshape(len(test_y.squeeze()), -1)\n",
    "    \n",
    "    preds = 1/(1+np.exp(-test_X.dot(w)))\n",
    "    result = np.where(preds>0.5, 1, 0)\n",
    "    \n",
    "    acc = np.sum(np.where(result==test_y, True, False))/len(preds)\n",
    "    print('accuracy : ', acc)\n",
    "\n",
    "eval(idx=0, w=w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itKbnbmNFGRd"
   },
   "source": [
    "### 2. multi class single label classification (using logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while의 조건만 바뀜\n",
    "# train\n",
    "def train(X, y) :\n",
    "    w = np.random.randn(len(X[0]), 1) # \n",
    "    lr = 0.01 # learning rate(수정)\n",
    "    step = 0\n",
    "    acc = 0\n",
    "    \n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    \n",
    "    while (acc < 0.8) :\n",
    "        step += 1\n",
    "        correct = 0\n",
    "        \n",
    "        # predict\n",
    "        preds = 1 / (1+np.exp(-X.dot(w)))\n",
    "        loss = CrossEntropyLoss(preds, y)\n",
    "        \n",
    "        result = np.where(preds>0.5, 1, 0)\n",
    "        acc = np.sum(np.where(result==y, True, False))/len(preds)\n",
    "        \n",
    "        print(\"total step : %d \" % step)\n",
    "        print(\"error : %f, accuarcy : %f\" % (loss, acc))\n",
    "        \n",
    "        loss_history.append(loss)\n",
    "        acc_history.append(acc)\n",
    "        \n",
    "        # gradient descent 수행\n",
    "        gradient = np.dot(X.T, (preds - y)) / len(X)\n",
    "        w -= lr * gradient\n",
    "    \n",
    "    # loss와 accuracy 시각화\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(loss_history)\n",
    "    plt.title(\"Loss history\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(acc_history)\n",
    "    plt.title(\"Accuracy history\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "\n",
    "    plt.show()\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for number 0\n",
      "total step : 1 \n",
      "error : 1.668747, accuarcy : 0.691846\n",
      "total step : 2 \n",
      "error : 1.649441, accuarcy : 0.693847\n",
      "total step : 3 \n",
      "error : 1.630689, accuarcy : 0.696348\n",
      "total step : 4 \n",
      "error : 1.612478, accuarcy : 0.698349\n",
      "total step : 5 \n",
      "error : 1.594795, accuarcy : 0.698349\n",
      "total step : 6 \n",
      "error : 1.577625, accuarcy : 0.702351\n",
      "total step : 7 \n",
      "error : 1.560956, accuarcy : 0.700850\n",
      "total step : 8 \n",
      "error : 1.544775, accuarcy : 0.700850\n",
      "total step : 9 \n",
      "error : 1.529069, accuarcy : 0.701351\n",
      "total step : 10 \n",
      "error : 1.513825, accuarcy : 0.704352\n",
      "total step : 11 \n",
      "error : 1.499031, accuarcy : 0.706853\n",
      "total step : 12 \n",
      "error : 1.484675, accuarcy : 0.710355\n",
      "total step : 13 \n",
      "error : 1.470744, accuarcy : 0.709855\n",
      "total step : 14 \n",
      "error : 1.457226, accuarcy : 0.710855\n",
      "total step : 15 \n",
      "error : 1.444110, accuarcy : 0.714357\n",
      "total step : 16 \n",
      "error : 1.431385, accuarcy : 0.713857\n",
      "total step : 17 \n",
      "error : 1.419037, accuarcy : 0.713357\n",
      "total step : 18 \n",
      "error : 1.407057, accuarcy : 0.715358\n",
      "total step : 19 \n",
      "error : 1.395434, accuarcy : 0.716358\n",
      "total step : 20 \n",
      "error : 1.384155, accuarcy : 0.718359\n",
      "total step : 21 \n",
      "error : 1.373212, accuarcy : 0.719860\n",
      "total step : 22 \n",
      "error : 1.362592, accuarcy : 0.720860\n",
      "total step : 23 \n",
      "error : 1.352286, accuarcy : 0.720360\n",
      "total step : 24 \n",
      "error : 1.342284, accuarcy : 0.720360\n",
      "total step : 25 \n",
      "error : 1.332575, accuarcy : 0.721361\n",
      "total step : 26 \n",
      "error : 1.323150, accuarcy : 0.722361\n",
      "total step : 27 \n",
      "error : 1.314000, accuarcy : 0.722861\n",
      "total step : 28 \n",
      "error : 1.305115, accuarcy : 0.724362\n",
      "total step : 29 \n",
      "error : 1.296487, accuarcy : 0.723362\n",
      "total step : 30 \n",
      "error : 1.288105, accuarcy : 0.723862\n",
      "total step : 31 \n",
      "error : 1.279963, accuarcy : 0.723362\n",
      "total step : 32 \n",
      "error : 1.272051, accuarcy : 0.724362\n",
      "total step : 33 \n",
      "error : 1.264361, accuarcy : 0.725363\n",
      "total step : 34 \n",
      "error : 1.256885, accuarcy : 0.726863\n",
      "total step : 35 \n",
      "error : 1.249617, accuarcy : 0.727864\n",
      "total step : 36 \n",
      "error : 1.242548, accuarcy : 0.729365\n",
      "total step : 37 \n",
      "error : 1.235671, accuarcy : 0.730365\n",
      "total step : 38 \n",
      "error : 1.228980, accuarcy : 0.731866\n",
      "total step : 39 \n",
      "error : 1.222467, accuarcy : 0.733867\n",
      "total step : 40 \n",
      "error : 1.216127, accuarcy : 0.733867\n",
      "total step : 41 \n",
      "error : 1.209953, accuarcy : 0.734867\n",
      "total step : 42 \n",
      "error : 1.203939, accuarcy : 0.735368\n",
      "total step : 43 \n",
      "error : 1.198078, accuarcy : 0.736368\n",
      "total step : 44 \n",
      "error : 1.192367, accuarcy : 0.735868\n",
      "total step : 45 \n",
      "error : 1.186798, accuarcy : 0.736868\n",
      "total step : 46 \n",
      "error : 1.181367, accuarcy : 0.737369\n",
      "total step : 47 \n",
      "error : 1.176070, accuarcy : 0.738369\n",
      "total step : 48 \n",
      "error : 1.170900, accuarcy : 0.739370\n",
      "total step : 49 \n",
      "error : 1.165853, accuarcy : 0.740370\n",
      "total step : 50 \n",
      "error : 1.160925, accuarcy : 0.740870\n",
      "total step : 51 \n",
      "error : 1.156112, accuarcy : 0.741371\n",
      "total step : 52 \n",
      "error : 1.151409, accuarcy : 0.741371\n",
      "total step : 53 \n",
      "error : 1.146812, accuarcy : 0.742371\n",
      "total step : 54 \n",
      "error : 1.142317, accuarcy : 0.742871\n",
      "total step : 55 \n",
      "error : 1.137921, accuarcy : 0.743872\n",
      "total step : 56 \n",
      "error : 1.133619, accuarcy : 0.743872\n",
      "total step : 57 \n",
      "error : 1.129409, accuarcy : 0.745873\n",
      "total step : 58 \n",
      "error : 1.125287, accuarcy : 0.746373\n",
      "total step : 59 \n",
      "error : 1.121249, accuarcy : 0.746873\n",
      "total step : 60 \n",
      "error : 1.117294, accuarcy : 0.746873\n",
      "total step : 61 \n",
      "error : 1.113417, accuarcy : 0.747874\n",
      "total step : 62 \n",
      "error : 1.109616, accuarcy : 0.748874\n",
      "total step : 63 \n",
      "error : 1.105887, accuarcy : 0.749375\n",
      "total step : 64 \n",
      "error : 1.102229, accuarcy : 0.750375\n",
      "total step : 65 \n",
      "error : 1.098639, accuarcy : 0.749875\n",
      "total step : 66 \n",
      "error : 1.095114, accuarcy : 0.749875\n",
      "total step : 67 \n",
      "error : 1.091652, accuarcy : 0.749375\n",
      "total step : 68 \n",
      "error : 1.088251, accuarcy : 0.750875\n",
      "total step : 69 \n",
      "error : 1.084908, accuarcy : 0.751376\n",
      "total step : 70 \n",
      "error : 1.081621, accuarcy : 0.751876\n",
      "total step : 71 \n",
      "error : 1.078389, accuarcy : 0.751376\n",
      "total step : 72 \n",
      "error : 1.075209, accuarcy : 0.750375\n",
      "total step : 73 \n",
      "error : 1.072080, accuarcy : 0.750875\n",
      "total step : 74 \n",
      "error : 1.068999, accuarcy : 0.751876\n",
      "total step : 75 \n",
      "error : 1.065965, accuarcy : 0.751376\n",
      "total step : 76 \n",
      "error : 1.062976, accuarcy : 0.752376\n",
      "total step : 77 \n",
      "error : 1.060032, accuarcy : 0.752876\n",
      "total step : 78 \n",
      "error : 1.057129, accuarcy : 0.753377\n",
      "total step : 79 \n",
      "error : 1.054267, accuarcy : 0.754377\n",
      "total step : 80 \n",
      "error : 1.051444, accuarcy : 0.754377\n",
      "total step : 81 \n",
      "error : 1.048660, accuarcy : 0.755378\n",
      "total step : 82 \n",
      "error : 1.045912, accuarcy : 0.754877\n",
      "total step : 83 \n",
      "error : 1.043199, accuarcy : 0.754377\n",
      "total step : 84 \n",
      "error : 1.040520, accuarcy : 0.754877\n",
      "total step : 85 \n",
      "error : 1.037875, accuarcy : 0.755878\n",
      "total step : 86 \n",
      "error : 1.035262, accuarcy : 0.755378\n",
      "total step : 87 \n",
      "error : 1.032679, accuarcy : 0.755878\n",
      "total step : 88 \n",
      "error : 1.030127, accuarcy : 0.755878\n",
      "total step : 89 \n",
      "error : 1.027603, accuarcy : 0.756878\n",
      "total step : 90 \n",
      "error : 1.025108, accuarcy : 0.757879\n",
      "total step : 91 \n",
      "error : 1.022639, accuarcy : 0.758379\n",
      "total step : 92 \n",
      "error : 1.020197, accuarcy : 0.758379\n",
      "total step : 93 \n",
      "error : 1.017781, accuarcy : 0.758879\n",
      "total step : 94 \n",
      "error : 1.015389, accuarcy : 0.759880\n",
      "total step : 95 \n",
      "error : 1.013021, accuarcy : 0.759880\n",
      "total step : 96 \n",
      "error : 1.010676, accuarcy : 0.760380\n",
      "total step : 97 \n",
      "error : 1.008354, accuarcy : 0.762381\n",
      "total step : 98 \n",
      "error : 1.006053, accuarcy : 0.762881\n",
      "total step : 99 \n",
      "error : 1.003774, accuarcy : 0.763382\n",
      "total step : 100 \n",
      "error : 1.001516, accuarcy : 0.763382\n",
      "total step : 101 \n",
      "error : 0.999277, accuarcy : 0.763882\n",
      "total step : 102 \n",
      "error : 0.997058, accuarcy : 0.763882\n",
      "total step : 103 \n",
      "error : 0.994858, accuarcy : 0.763882\n",
      "total step : 104 \n",
      "error : 0.992676, accuarcy : 0.764382\n",
      "total step : 105 \n",
      "error : 0.990512, accuarcy : 0.764882\n",
      "total step : 106 \n",
      "error : 0.988366, accuarcy : 0.765383\n",
      "total step : 107 \n",
      "error : 0.986236, accuarcy : 0.766883\n",
      "total step : 108 \n",
      "error : 0.984123, accuarcy : 0.767884\n",
      "total step : 109 \n",
      "error : 0.982027, accuarcy : 0.767884\n",
      "total step : 110 \n",
      "error : 0.979946, accuarcy : 0.767884\n",
      "total step : 111 \n",
      "error : 0.977880, accuarcy : 0.767884\n",
      "total step : 112 \n",
      "error : 0.975829, accuarcy : 0.769385\n",
      "total step : 113 \n",
      "error : 0.973793, accuarcy : 0.770385\n",
      "total step : 114 \n",
      "error : 0.971772, accuarcy : 0.771386\n",
      "total step : 115 \n",
      "error : 0.969764, accuarcy : 0.771386\n",
      "total step : 116 \n",
      "error : 0.967769, accuarcy : 0.772386\n",
      "total step : 117 \n",
      "error : 0.965788, accuarcy : 0.772886\n",
      "total step : 118 \n",
      "error : 0.963821, accuarcy : 0.772886\n",
      "total step : 119 \n",
      "error : 0.961866, accuarcy : 0.772886\n",
      "total step : 120 \n",
      "error : 0.959923, accuarcy : 0.772886\n",
      "total step : 121 \n",
      "error : 0.957992, accuarcy : 0.773387\n",
      "total step : 122 \n",
      "error : 0.956074, accuarcy : 0.775388\n",
      "total step : 123 \n",
      "error : 0.954167, accuarcy : 0.775388\n",
      "total step : 124 \n",
      "error : 0.952272, accuarcy : 0.775388\n",
      "total step : 125 \n",
      "error : 0.950388, accuarcy : 0.775888\n",
      "total step : 126 \n",
      "error : 0.948516, accuarcy : 0.775888\n",
      "total step : 127 \n",
      "error : 0.946654, accuarcy : 0.775888\n",
      "total step : 128 \n",
      "error : 0.944802, accuarcy : 0.776388\n",
      "total step : 129 \n",
      "error : 0.942962, accuarcy : 0.776388\n",
      "total step : 130 \n",
      "error : 0.941131, accuarcy : 0.776388\n",
      "total step : 131 \n",
      "error : 0.939311, accuarcy : 0.776888\n",
      "total step : 132 \n",
      "error : 0.937500, accuarcy : 0.777389\n",
      "total step : 133 \n",
      "error : 0.935700, accuarcy : 0.777389\n",
      "total step : 134 \n",
      "error : 0.933909, accuarcy : 0.777889\n",
      "total step : 135 \n",
      "error : 0.932127, accuarcy : 0.778389\n",
      "total step : 136 \n",
      "error : 0.930355, accuarcy : 0.778889\n",
      "total step : 137 \n",
      "error : 0.928592, accuarcy : 0.778889\n",
      "total step : 138 \n",
      "error : 0.926838, accuarcy : 0.779890\n",
      "total step : 139 \n",
      "error : 0.925093, accuarcy : 0.779890\n",
      "total step : 140 \n",
      "error : 0.923357, accuarcy : 0.780890\n",
      "total step : 141 \n",
      "error : 0.921630, accuarcy : 0.780890\n",
      "total step : 142 \n",
      "error : 0.919911, accuarcy : 0.780890\n",
      "total step : 143 \n",
      "error : 0.918200, accuarcy : 0.780890\n",
      "total step : 144 \n",
      "error : 0.916498, accuarcy : 0.781391\n",
      "total step : 145 \n",
      "error : 0.914804, accuarcy : 0.781891\n",
      "total step : 146 \n",
      "error : 0.913118, accuarcy : 0.782891\n",
      "total step : 147 \n",
      "error : 0.911440, accuarcy : 0.783392\n",
      "total step : 148 \n",
      "error : 0.909771, accuarcy : 0.783392\n",
      "total step : 149 \n",
      "error : 0.908108, accuarcy : 0.783892\n",
      "total step : 150 \n",
      "error : 0.906454, accuarcy : 0.784392\n",
      "total step : 151 \n",
      "error : 0.904807, accuarcy : 0.785393\n",
      "total step : 152 \n",
      "error : 0.903168, accuarcy : 0.785893\n",
      "total step : 153 \n",
      "error : 0.901536, accuarcy : 0.786393\n",
      "total step : 154 \n",
      "error : 0.899912, accuarcy : 0.786893\n",
      "total step : 155 \n",
      "error : 0.898295, accuarcy : 0.786893\n",
      "total step : 156 \n",
      "error : 0.896685, accuarcy : 0.787394\n",
      "total step : 157 \n",
      "error : 0.895082, accuarcy : 0.787894\n",
      "total step : 158 \n",
      "error : 0.893487, accuarcy : 0.788394\n",
      "total step : 159 \n",
      "error : 0.891898, accuarcy : 0.788394\n",
      "total step : 160 \n",
      "error : 0.890317, accuarcy : 0.787894\n",
      "total step : 161 \n",
      "error : 0.888742, accuarcy : 0.787894\n",
      "total step : 162 \n",
      "error : 0.887174, accuarcy : 0.787894\n",
      "total step : 163 \n",
      "error : 0.885613, accuarcy : 0.787394\n",
      "total step : 164 \n",
      "error : 0.884058, accuarcy : 0.787394\n",
      "total step : 165 \n",
      "error : 0.882510, accuarcy : 0.787894\n",
      "total step : 166 \n",
      "error : 0.880969, accuarcy : 0.787894\n",
      "total step : 167 \n",
      "error : 0.879434, accuarcy : 0.787894\n",
      "total step : 168 \n",
      "error : 0.877905, accuarcy : 0.788394\n",
      "total step : 169 \n",
      "error : 0.876383, accuarcy : 0.789395\n",
      "total step : 170 \n",
      "error : 0.874867, accuarcy : 0.790395\n",
      "total step : 171 \n",
      "error : 0.873358, accuarcy : 0.791396\n",
      "total step : 172 \n",
      "error : 0.871855, accuarcy : 0.791896\n",
      "total step : 173 \n",
      "error : 0.870357, accuarcy : 0.792396\n",
      "total step : 174 \n",
      "error : 0.868867, accuarcy : 0.792896\n",
      "total step : 175 \n",
      "error : 0.867382, accuarcy : 0.793397\n",
      "total step : 176 \n",
      "error : 0.865903, accuarcy : 0.793897\n",
      "total step : 177 \n",
      "error : 0.864430, accuarcy : 0.793897\n",
      "total step : 178 \n",
      "error : 0.862963, accuarcy : 0.793897\n",
      "total step : 179 \n",
      "error : 0.861502, accuarcy : 0.794397\n",
      "total step : 180 \n",
      "error : 0.860047, accuarcy : 0.795398\n",
      "total step : 181 \n",
      "error : 0.858597, accuarcy : 0.795898\n",
      "total step : 182 \n",
      "error : 0.857154, accuarcy : 0.796898\n",
      "total step : 183 \n",
      "error : 0.855716, accuarcy : 0.797899\n",
      "total step : 184 \n",
      "error : 0.854284, accuarcy : 0.798399\n",
      "total step : 185 \n",
      "error : 0.852857, accuarcy : 0.798399\n",
      "total step : 186 \n",
      "error : 0.851436, accuarcy : 0.798899\n",
      "total step : 187 \n",
      "error : 0.850021, accuarcy : 0.799400\n",
      "total step : 188 \n",
      "error : 0.848611, accuarcy : 0.799400\n",
      "total step : 189 \n",
      "error : 0.847207, accuarcy : 0.799400\n",
      "total step : 190 \n",
      "error : 0.845808, accuarcy : 0.799400\n",
      "total step : 191 \n",
      "error : 0.844414, accuarcy : 0.801401\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for number 1\n",
      "total step : 1 \n",
      "error : 4.249613, accuarcy : 0.334667\n",
      "total step : 2 \n",
      "error : 4.215963, accuarcy : 0.336168\n",
      "total step : 3 \n",
      "error : 4.182497, accuarcy : 0.337669\n",
      "total step : 4 \n",
      "error : 4.149219, accuarcy : 0.339170\n",
      "total step : 5 \n",
      "error : 4.116133, accuarcy : 0.341671\n",
      "total step : 6 \n",
      "error : 4.083242, accuarcy : 0.341671\n",
      "total step : 7 \n",
      "error : 4.050548, accuarcy : 0.341171\n",
      "total step : 8 \n",
      "error : 4.018054, accuarcy : 0.341171\n",
      "total step : 9 \n",
      "error : 3.985762, accuarcy : 0.345673\n",
      "total step : 10 \n",
      "error : 3.953675, accuarcy : 0.345173\n",
      "total step : 11 \n",
      "error : 3.921794, accuarcy : 0.346173\n",
      "total step : 12 \n",
      "error : 3.890121, accuarcy : 0.345673\n",
      "total step : 13 \n",
      "error : 3.858656, accuarcy : 0.346673\n",
      "total step : 14 \n",
      "error : 3.827402, accuarcy : 0.347174\n",
      "total step : 15 \n",
      "error : 3.796358, accuarcy : 0.347674\n",
      "total step : 16 \n",
      "error : 3.765527, accuarcy : 0.349175\n",
      "total step : 17 \n",
      "error : 3.734907, accuarcy : 0.350675\n",
      "total step : 18 \n",
      "error : 3.704499, accuarcy : 0.351176\n",
      "total step : 19 \n",
      "error : 3.674304, accuarcy : 0.352676\n",
      "total step : 20 \n",
      "error : 3.644321, accuarcy : 0.353677\n",
      "total step : 21 \n",
      "error : 3.614550, accuarcy : 0.355178\n",
      "total step : 22 \n",
      "error : 3.584990, accuarcy : 0.357179\n",
      "total step : 23 \n",
      "error : 3.555641, accuarcy : 0.357679\n",
      "total step : 24 \n",
      "error : 3.526501, accuarcy : 0.363182\n",
      "total step : 25 \n",
      "error : 3.497571, accuarcy : 0.365683\n",
      "total step : 26 \n",
      "error : 3.468849, accuarcy : 0.367684\n",
      "total step : 27 \n",
      "error : 3.440334, accuarcy : 0.369685\n",
      "total step : 28 \n",
      "error : 3.412025, accuarcy : 0.370685\n",
      "total step : 29 \n",
      "error : 3.383921, accuarcy : 0.373187\n",
      "total step : 30 \n",
      "error : 3.356020, accuarcy : 0.375188\n",
      "total step : 31 \n",
      "error : 3.328323, accuarcy : 0.374687\n",
      "total step : 32 \n",
      "error : 3.300829, accuarcy : 0.376188\n",
      "total step : 33 \n",
      "error : 3.273535, accuarcy : 0.376688\n",
      "total step : 34 \n",
      "error : 3.246443, accuarcy : 0.376188\n",
      "total step : 35 \n",
      "error : 3.219550, accuarcy : 0.377189\n",
      "total step : 36 \n",
      "error : 3.192857, accuarcy : 0.381191\n",
      "total step : 37 \n",
      "error : 3.166364, accuarcy : 0.381191\n",
      "total step : 38 \n",
      "error : 3.140070, accuarcy : 0.381191\n",
      "total step : 39 \n",
      "error : 3.113975, accuarcy : 0.383192\n",
      "total step : 40 \n",
      "error : 3.088079, accuarcy : 0.383192\n",
      "total step : 41 \n",
      "error : 3.062381, accuarcy : 0.387694\n",
      "total step : 42 \n",
      "error : 3.036883, accuarcy : 0.389195\n",
      "total step : 43 \n",
      "error : 3.011584, accuarcy : 0.390695\n",
      "total step : 44 \n",
      "error : 2.986484, accuarcy : 0.390695\n",
      "total step : 45 \n",
      "error : 2.961584, accuarcy : 0.391196\n",
      "total step : 46 \n",
      "error : 2.936883, accuarcy : 0.392696\n",
      "total step : 47 \n",
      "error : 2.912381, accuarcy : 0.393197\n",
      "total step : 48 \n",
      "error : 2.888079, accuarcy : 0.394197\n",
      "total step : 49 \n",
      "error : 2.863978, accuarcy : 0.395698\n",
      "total step : 50 \n",
      "error : 2.840076, accuarcy : 0.397199\n",
      "total step : 51 \n",
      "error : 2.816373, accuarcy : 0.398699\n",
      "total step : 52 \n",
      "error : 2.792871, accuarcy : 0.402701\n",
      "total step : 53 \n",
      "error : 2.769569, accuarcy : 0.404202\n",
      "total step : 54 \n",
      "error : 2.746467, accuarcy : 0.405703\n",
      "total step : 55 \n",
      "error : 2.723565, accuarcy : 0.407704\n",
      "total step : 56 \n",
      "error : 2.700862, accuarcy : 0.410205\n",
      "total step : 57 \n",
      "error : 2.678360, accuarcy : 0.411206\n",
      "total step : 58 \n",
      "error : 2.656057, accuarcy : 0.412206\n",
      "total step : 59 \n",
      "error : 2.633953, accuarcy : 0.414207\n",
      "total step : 60 \n",
      "error : 2.612049, accuarcy : 0.416708\n",
      "total step : 61 \n",
      "error : 2.590344, accuarcy : 0.419210\n",
      "total step : 62 \n",
      "error : 2.568838, accuarcy : 0.421711\n",
      "total step : 63 \n",
      "error : 2.547532, accuarcy : 0.424212\n",
      "total step : 64 \n",
      "error : 2.526423, accuarcy : 0.427714\n",
      "total step : 65 \n",
      "error : 2.505513, accuarcy : 0.428714\n",
      "total step : 66 \n",
      "error : 2.484800, accuarcy : 0.429715\n",
      "total step : 67 \n",
      "error : 2.464285, accuarcy : 0.430215\n",
      "total step : 68 \n",
      "error : 2.443967, accuarcy : 0.431716\n",
      "total step : 69 \n",
      "error : 2.423846, accuarcy : 0.433717\n",
      "total step : 70 \n",
      "error : 2.403920, accuarcy : 0.435218\n",
      "total step : 71 \n",
      "error : 2.384190, accuarcy : 0.437219\n",
      "total step : 72 \n",
      "error : 2.364654, accuarcy : 0.440220\n",
      "total step : 73 \n",
      "error : 2.345312, accuarcy : 0.443222\n",
      "total step : 74 \n",
      "error : 2.326164, accuarcy : 0.444722\n",
      "total step : 75 \n",
      "error : 2.307207, accuarcy : 0.448224\n",
      "total step : 76 \n",
      "error : 2.288442, accuarcy : 0.448724\n",
      "total step : 77 \n",
      "error : 2.269868, accuarcy : 0.451726\n",
      "total step : 78 \n",
      "error : 2.251483, accuarcy : 0.454727\n",
      "total step : 79 \n",
      "error : 2.233286, accuarcy : 0.458229\n",
      "total step : 80 \n",
      "error : 2.215277, accuarcy : 0.459230\n",
      "total step : 81 \n",
      "error : 2.197455, accuarcy : 0.461231\n",
      "total step : 82 \n",
      "error : 2.179817, accuarcy : 0.464232\n",
      "total step : 83 \n",
      "error : 2.162363, accuarcy : 0.466233\n",
      "total step : 84 \n",
      "error : 2.145092, accuarcy : 0.466733\n",
      "total step : 85 \n",
      "error : 2.128002, accuarcy : 0.470735\n",
      "total step : 86 \n",
      "error : 2.111092, accuarcy : 0.475238\n",
      "total step : 87 \n",
      "error : 2.094361, accuarcy : 0.478239\n",
      "total step : 88 \n",
      "error : 2.077808, accuarcy : 0.479240\n",
      "total step : 89 \n",
      "error : 2.061430, accuarcy : 0.480240\n",
      "total step : 90 \n",
      "error : 2.045227, accuarcy : 0.481741\n",
      "total step : 91 \n",
      "error : 2.029197, accuarcy : 0.485243\n",
      "total step : 92 \n",
      "error : 2.013339, accuarcy : 0.487744\n",
      "total step : 93 \n",
      "error : 1.997651, accuarcy : 0.490745\n",
      "total step : 94 \n",
      "error : 1.982131, accuarcy : 0.493247\n",
      "total step : 95 \n",
      "error : 1.966780, accuarcy : 0.497249\n",
      "total step : 96 \n",
      "error : 1.951593, accuarcy : 0.500750\n",
      "total step : 97 \n",
      "error : 1.936572, accuarcy : 0.502751\n",
      "total step : 98 \n",
      "error : 1.921713, accuarcy : 0.506253\n",
      "total step : 99 \n",
      "error : 1.907015, accuarcy : 0.508254\n",
      "total step : 100 \n",
      "error : 1.892477, accuarcy : 0.508754\n",
      "total step : 101 \n",
      "error : 1.878098, accuarcy : 0.510755\n",
      "total step : 102 \n",
      "error : 1.863875, accuarcy : 0.513757\n",
      "total step : 103 \n",
      "error : 1.849808, accuarcy : 0.514257\n",
      "total step : 104 \n",
      "error : 1.835895, accuarcy : 0.517759\n",
      "total step : 105 \n",
      "error : 1.822134, accuarcy : 0.519260\n",
      "total step : 106 \n",
      "error : 1.808523, accuarcy : 0.521761\n",
      "total step : 107 \n",
      "error : 1.795062, accuarcy : 0.523262\n",
      "total step : 108 \n",
      "error : 1.781749, accuarcy : 0.525763\n",
      "total step : 109 \n",
      "error : 1.768583, accuarcy : 0.528264\n",
      "total step : 110 \n",
      "error : 1.755561, accuarcy : 0.531266\n",
      "total step : 111 \n",
      "error : 1.742683, accuarcy : 0.533267\n",
      "total step : 112 \n",
      "error : 1.729947, accuarcy : 0.534267\n",
      "total step : 113 \n",
      "error : 1.717352, accuarcy : 0.537769\n",
      "total step : 114 \n",
      "error : 1.704896, accuarcy : 0.539770\n",
      "total step : 115 \n",
      "error : 1.692578, accuarcy : 0.540270\n",
      "total step : 116 \n",
      "error : 1.680397, accuarcy : 0.542771\n",
      "total step : 117 \n",
      "error : 1.668351, accuarcy : 0.544772\n",
      "total step : 118 \n",
      "error : 1.656438, accuarcy : 0.546273\n",
      "total step : 119 \n",
      "error : 1.644658, accuarcy : 0.547774\n",
      "total step : 120 \n",
      "error : 1.633009, accuarcy : 0.550775\n",
      "total step : 121 \n",
      "error : 1.621490, accuarcy : 0.553777\n",
      "total step : 122 \n",
      "error : 1.610100, accuarcy : 0.555778\n",
      "total step : 123 \n",
      "error : 1.598837, accuarcy : 0.557279\n",
      "total step : 124 \n",
      "error : 1.587700, accuarcy : 0.559780\n",
      "total step : 125 \n",
      "error : 1.576687, accuarcy : 0.562281\n",
      "total step : 126 \n",
      "error : 1.565798, accuarcy : 0.563782\n",
      "total step : 127 \n",
      "error : 1.555031, accuarcy : 0.565283\n",
      "total step : 128 \n",
      "error : 1.544385, accuarcy : 0.567284\n",
      "total step : 129 \n",
      "error : 1.533858, accuarcy : 0.570285\n",
      "total step : 130 \n",
      "error : 1.523450, accuarcy : 0.574287\n",
      "total step : 131 \n",
      "error : 1.513159, accuarcy : 0.576288\n",
      "total step : 132 \n",
      "error : 1.502984, accuarcy : 0.576788\n",
      "total step : 133 \n",
      "error : 1.492924, accuarcy : 0.578789\n",
      "total step : 134 \n",
      "error : 1.482977, accuarcy : 0.580290\n",
      "total step : 135 \n",
      "error : 1.473143, accuarcy : 0.582291\n",
      "total step : 136 \n",
      "error : 1.463419, accuarcy : 0.582791\n",
      "total step : 137 \n",
      "error : 1.453806, accuarcy : 0.585293\n",
      "total step : 138 \n",
      "error : 1.444301, accuarcy : 0.588794\n",
      "total step : 139 \n",
      "error : 1.434904, accuarcy : 0.590295\n",
      "total step : 140 \n",
      "error : 1.425613, accuarcy : 0.593297\n",
      "total step : 141 \n",
      "error : 1.416428, accuarcy : 0.594297\n",
      "total step : 142 \n",
      "error : 1.407346, accuarcy : 0.595798\n",
      "total step : 143 \n",
      "error : 1.398367, accuarcy : 0.597299\n",
      "total step : 144 \n",
      "error : 1.389490, accuarcy : 0.598799\n",
      "total step : 145 \n",
      "error : 1.380713, accuarcy : 0.601801\n",
      "total step : 146 \n",
      "error : 1.372036, accuarcy : 0.606303\n",
      "total step : 147 \n",
      "error : 1.363456, accuarcy : 0.608304\n",
      "total step : 148 \n",
      "error : 1.354974, accuarcy : 0.610305\n",
      "total step : 149 \n",
      "error : 1.346588, accuarcy : 0.610805\n",
      "total step : 150 \n",
      "error : 1.338297, accuarcy : 0.612306\n",
      "total step : 151 \n",
      "error : 1.330099, accuarcy : 0.615808\n",
      "total step : 152 \n",
      "error : 1.321994, accuarcy : 0.617809\n",
      "total step : 153 \n",
      "error : 1.313980, accuarcy : 0.619310\n",
      "total step : 154 \n",
      "error : 1.306056, accuarcy : 0.620810\n",
      "total step : 155 \n",
      "error : 1.298222, accuarcy : 0.623812\n",
      "total step : 156 \n",
      "error : 1.290476, accuarcy : 0.625313\n",
      "total step : 157 \n",
      "error : 1.282817, accuarcy : 0.626313\n",
      "total step : 158 \n",
      "error : 1.275244, accuarcy : 0.626813\n",
      "total step : 159 \n",
      "error : 1.267755, accuarcy : 0.627814\n",
      "total step : 160 \n",
      "error : 1.260351, accuarcy : 0.629815\n",
      "total step : 161 \n",
      "error : 1.253029, accuarcy : 0.631816\n",
      "total step : 162 \n",
      "error : 1.245789, accuarcy : 0.632316\n",
      "total step : 163 \n",
      "error : 1.238630, accuarcy : 0.634817\n",
      "total step : 164 \n",
      "error : 1.231550, accuarcy : 0.635318\n",
      "total step : 165 \n",
      "error : 1.224549, accuarcy : 0.636818\n",
      "total step : 166 \n",
      "error : 1.217626, accuarcy : 0.637819\n",
      "total step : 167 \n",
      "error : 1.210779, accuarcy : 0.640320\n",
      "total step : 168 \n",
      "error : 1.204008, accuarcy : 0.641821\n",
      "total step : 169 \n",
      "error : 1.197312, accuarcy : 0.643322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 170 \n",
      "error : 1.190689, accuarcy : 0.645823\n",
      "total step : 171 \n",
      "error : 1.184139, accuarcy : 0.646823\n",
      "total step : 172 \n",
      "error : 1.177661, accuarcy : 0.649825\n",
      "total step : 173 \n",
      "error : 1.171253, accuarcy : 0.650325\n",
      "total step : 174 \n",
      "error : 1.164915, accuarcy : 0.651326\n",
      "total step : 175 \n",
      "error : 1.158647, accuarcy : 0.653327\n",
      "total step : 176 \n",
      "error : 1.152446, accuarcy : 0.654827\n",
      "total step : 177 \n",
      "error : 1.146312, accuarcy : 0.655828\n",
      "total step : 178 \n",
      "error : 1.140245, accuarcy : 0.659330\n",
      "total step : 179 \n",
      "error : 1.134243, accuarcy : 0.659330\n",
      "total step : 180 \n",
      "error : 1.128306, accuarcy : 0.661331\n",
      "total step : 181 \n",
      "error : 1.122432, accuarcy : 0.662831\n",
      "total step : 182 \n",
      "error : 1.116621, accuarcy : 0.663832\n",
      "total step : 183 \n",
      "error : 1.110872, accuarcy : 0.666333\n",
      "total step : 184 \n",
      "error : 1.105184, accuarcy : 0.669335\n",
      "total step : 185 \n",
      "error : 1.099556, accuarcy : 0.670835\n",
      "total step : 186 \n",
      "error : 1.093988, accuarcy : 0.671836\n",
      "total step : 187 \n",
      "error : 1.088478, accuarcy : 0.672336\n",
      "total step : 188 \n",
      "error : 1.083027, accuarcy : 0.675338\n",
      "total step : 189 \n",
      "error : 1.077632, accuarcy : 0.676338\n",
      "total step : 190 \n",
      "error : 1.072294, accuarcy : 0.679840\n",
      "total step : 191 \n",
      "error : 1.067011, accuarcy : 0.682341\n",
      "total step : 192 \n",
      "error : 1.061784, accuarcy : 0.683342\n",
      "total step : 193 \n",
      "error : 1.056610, accuarcy : 0.684342\n",
      "total step : 194 \n",
      "error : 1.051490, accuarcy : 0.686343\n",
      "total step : 195 \n",
      "error : 1.046423, accuarcy : 0.686843\n",
      "total step : 196 \n",
      "error : 1.041407, accuarcy : 0.687844\n",
      "total step : 197 \n",
      "error : 1.036443, accuarcy : 0.689345\n",
      "total step : 198 \n",
      "error : 1.031530, accuarcy : 0.690345\n",
      "total step : 199 \n",
      "error : 1.026666, accuarcy : 0.691346\n",
      "total step : 200 \n",
      "error : 1.021852, accuarcy : 0.692346\n",
      "total step : 201 \n",
      "error : 1.017086, accuarcy : 0.693847\n",
      "total step : 202 \n",
      "error : 1.012368, accuarcy : 0.694847\n",
      "total step : 203 \n",
      "error : 1.007698, accuarcy : 0.695848\n",
      "total step : 204 \n",
      "error : 1.003074, accuarcy : 0.696848\n",
      "total step : 205 \n",
      "error : 0.998496, accuarcy : 0.698349\n",
      "total step : 206 \n",
      "error : 0.993964, accuarcy : 0.698349\n",
      "total step : 207 \n",
      "error : 0.989477, accuarcy : 0.698849\n",
      "total step : 208 \n",
      "error : 0.985034, accuarcy : 0.700850\n",
      "total step : 209 \n",
      "error : 0.980635, accuarcy : 0.702851\n",
      "total step : 210 \n",
      "error : 0.976278, accuarcy : 0.704352\n",
      "total step : 211 \n",
      "error : 0.971965, accuarcy : 0.706853\n",
      "total step : 212 \n",
      "error : 0.967693, accuarcy : 0.707854\n",
      "total step : 213 \n",
      "error : 0.963463, accuarcy : 0.708354\n",
      "total step : 214 \n",
      "error : 0.959273, accuarcy : 0.710355\n",
      "total step : 215 \n",
      "error : 0.955124, accuarcy : 0.710855\n",
      "total step : 216 \n",
      "error : 0.951015, accuarcy : 0.711356\n",
      "total step : 217 \n",
      "error : 0.946945, accuarcy : 0.711856\n",
      "total step : 218 \n",
      "error : 0.942914, accuarcy : 0.712856\n",
      "total step : 219 \n",
      "error : 0.938921, accuarcy : 0.714857\n",
      "total step : 220 \n",
      "error : 0.934966, accuarcy : 0.717859\n",
      "total step : 221 \n",
      "error : 0.931048, accuarcy : 0.719360\n",
      "total step : 222 \n",
      "error : 0.927167, accuarcy : 0.720360\n",
      "total step : 223 \n",
      "error : 0.923322, accuarcy : 0.721861\n",
      "total step : 224 \n",
      "error : 0.919514, accuarcy : 0.722861\n",
      "total step : 225 \n",
      "error : 0.915740, accuarcy : 0.723362\n",
      "total step : 226 \n",
      "error : 0.912002, accuarcy : 0.724862\n",
      "total step : 227 \n",
      "error : 0.908298, accuarcy : 0.725363\n",
      "total step : 228 \n",
      "error : 0.904628, accuarcy : 0.726363\n",
      "total step : 229 \n",
      "error : 0.900992, accuarcy : 0.727364\n",
      "total step : 230 \n",
      "error : 0.897389, accuarcy : 0.728364\n",
      "total step : 231 \n",
      "error : 0.893819, accuarcy : 0.729365\n",
      "total step : 232 \n",
      "error : 0.890281, accuarcy : 0.730365\n",
      "total step : 233 \n",
      "error : 0.886776, accuarcy : 0.732366\n",
      "total step : 234 \n",
      "error : 0.883301, accuarcy : 0.733867\n",
      "total step : 235 \n",
      "error : 0.879858, accuarcy : 0.734867\n",
      "total step : 236 \n",
      "error : 0.876446, accuarcy : 0.736368\n",
      "total step : 237 \n",
      "error : 0.873064, accuarcy : 0.738369\n",
      "total step : 238 \n",
      "error : 0.869712, accuarcy : 0.738869\n",
      "total step : 239 \n",
      "error : 0.866390, accuarcy : 0.738869\n",
      "total step : 240 \n",
      "error : 0.863097, accuarcy : 0.740370\n",
      "total step : 241 \n",
      "error : 0.859833, accuarcy : 0.741371\n",
      "total step : 242 \n",
      "error : 0.856598, accuarcy : 0.741871\n",
      "total step : 243 \n",
      "error : 0.853391, accuarcy : 0.742871\n",
      "total step : 244 \n",
      "error : 0.850212, accuarcy : 0.744372\n",
      "total step : 245 \n",
      "error : 0.847060, accuarcy : 0.745373\n",
      "total step : 246 \n",
      "error : 0.843935, accuarcy : 0.746373\n",
      "total step : 247 \n",
      "error : 0.840838, accuarcy : 0.746373\n",
      "total step : 248 \n",
      "error : 0.837767, accuarcy : 0.747374\n",
      "total step : 249 \n",
      "error : 0.834722, accuarcy : 0.747374\n",
      "total step : 250 \n",
      "error : 0.831703, accuarcy : 0.749375\n",
      "total step : 251 \n",
      "error : 0.828709, accuarcy : 0.749875\n",
      "total step : 252 \n",
      "error : 0.825741, accuarcy : 0.750375\n",
      "total step : 253 \n",
      "error : 0.822798, accuarcy : 0.750875\n",
      "total step : 254 \n",
      "error : 0.819880, accuarcy : 0.750875\n",
      "total step : 255 \n",
      "error : 0.816986, accuarcy : 0.751376\n",
      "total step : 256 \n",
      "error : 0.814117, accuarcy : 0.751876\n",
      "total step : 257 \n",
      "error : 0.811271, accuarcy : 0.752376\n",
      "total step : 258 \n",
      "error : 0.808448, accuarcy : 0.752876\n",
      "total step : 259 \n",
      "error : 0.805649, accuarcy : 0.753877\n",
      "total step : 260 \n",
      "error : 0.802873, accuarcy : 0.754877\n",
      "total step : 261 \n",
      "error : 0.800120, accuarcy : 0.755378\n",
      "total step : 262 \n",
      "error : 0.797389, accuarcy : 0.755878\n",
      "total step : 263 \n",
      "error : 0.794681, accuarcy : 0.755878\n",
      "total step : 264 \n",
      "error : 0.791994, accuarcy : 0.755878\n",
      "total step : 265 \n",
      "error : 0.789329, accuarcy : 0.755878\n",
      "total step : 266 \n",
      "error : 0.786686, accuarcy : 0.755878\n",
      "total step : 267 \n",
      "error : 0.784064, accuarcy : 0.756878\n",
      "total step : 268 \n",
      "error : 0.781463, accuarcy : 0.757379\n",
      "total step : 269 \n",
      "error : 0.778882, accuarcy : 0.757879\n",
      "total step : 270 \n",
      "error : 0.776322, accuarcy : 0.757879\n",
      "total step : 271 \n",
      "error : 0.773782, accuarcy : 0.757879\n",
      "total step : 272 \n",
      "error : 0.771263, accuarcy : 0.757879\n",
      "total step : 273 \n",
      "error : 0.768763, accuarcy : 0.760880\n",
      "total step : 274 \n",
      "error : 0.766283, accuarcy : 0.761381\n",
      "total step : 275 \n",
      "error : 0.763822, accuarcy : 0.762881\n",
      "total step : 276 \n",
      "error : 0.761380, accuarcy : 0.764382\n",
      "total step : 277 \n",
      "error : 0.758958, accuarcy : 0.765883\n",
      "total step : 278 \n",
      "error : 0.756554, accuarcy : 0.766383\n",
      "total step : 279 \n",
      "error : 0.754168, accuarcy : 0.767884\n",
      "total step : 280 \n",
      "error : 0.751801, accuarcy : 0.768884\n",
      "total step : 281 \n",
      "error : 0.749452, accuarcy : 0.769885\n",
      "total step : 282 \n",
      "error : 0.747121, accuarcy : 0.770385\n",
      "total step : 283 \n",
      "error : 0.744808, accuarcy : 0.771886\n",
      "total step : 284 \n",
      "error : 0.742512, accuarcy : 0.773387\n",
      "total step : 285 \n",
      "error : 0.740233, accuarcy : 0.774387\n",
      "total step : 286 \n",
      "error : 0.737972, accuarcy : 0.774887\n",
      "total step : 287 \n",
      "error : 0.735727, accuarcy : 0.774887\n",
      "total step : 288 \n",
      "error : 0.733500, accuarcy : 0.775888\n",
      "total step : 289 \n",
      "error : 0.731289, accuarcy : 0.776388\n",
      "total step : 290 \n",
      "error : 0.729094, accuarcy : 0.777389\n",
      "total step : 291 \n",
      "error : 0.726916, accuarcy : 0.777389\n",
      "total step : 292 \n",
      "error : 0.724754, accuarcy : 0.777889\n",
      "total step : 293 \n",
      "error : 0.722607, accuarcy : 0.777889\n",
      "total step : 294 \n",
      "error : 0.720477, accuarcy : 0.777889\n",
      "total step : 295 \n",
      "error : 0.718362, accuarcy : 0.778889\n",
      "total step : 296 \n",
      "error : 0.716262, accuarcy : 0.779390\n",
      "total step : 297 \n",
      "error : 0.714177, accuarcy : 0.779890\n",
      "total step : 298 \n",
      "error : 0.712108, accuarcy : 0.779890\n",
      "total step : 299 \n",
      "error : 0.710053, accuarcy : 0.781891\n",
      "total step : 300 \n",
      "error : 0.708014, accuarcy : 0.782891\n",
      "total step : 301 \n",
      "error : 0.705989, accuarcy : 0.783392\n",
      "total step : 302 \n",
      "error : 0.703978, accuarcy : 0.783392\n",
      "total step : 303 \n",
      "error : 0.701982, accuarcy : 0.783392\n",
      "total step : 304 \n",
      "error : 0.700000, accuarcy : 0.783392\n",
      "total step : 305 \n",
      "error : 0.698031, accuarcy : 0.784392\n",
      "total step : 306 \n",
      "error : 0.696077, accuarcy : 0.784392\n",
      "total step : 307 \n",
      "error : 0.694136, accuarcy : 0.785393\n",
      "total step : 308 \n",
      "error : 0.692209, accuarcy : 0.786393\n",
      "total step : 309 \n",
      "error : 0.690296, accuarcy : 0.787394\n",
      "total step : 310 \n",
      "error : 0.688396, accuarcy : 0.788394\n",
      "total step : 311 \n",
      "error : 0.686509, accuarcy : 0.788894\n",
      "total step : 312 \n",
      "error : 0.684635, accuarcy : 0.789395\n",
      "total step : 313 \n",
      "error : 0.682774, accuarcy : 0.789895\n",
      "total step : 314 \n",
      "error : 0.680925, accuarcy : 0.789895\n",
      "total step : 315 \n",
      "error : 0.679090, accuarcy : 0.789895\n",
      "total step : 316 \n",
      "error : 0.677266, accuarcy : 0.789895\n",
      "total step : 317 \n",
      "error : 0.675456, accuarcy : 0.790395\n",
      "total step : 318 \n",
      "error : 0.673657, accuarcy : 0.790395\n",
      "total step : 319 \n",
      "error : 0.671871, accuarcy : 0.791896\n",
      "total step : 320 \n",
      "error : 0.670097, accuarcy : 0.792896\n",
      "total step : 321 \n",
      "error : 0.668334, accuarcy : 0.792896\n",
      "total step : 322 \n",
      "error : 0.666584, accuarcy : 0.793897\n",
      "total step : 323 \n",
      "error : 0.664845, accuarcy : 0.794897\n",
      "total step : 324 \n",
      "error : 0.663118, accuarcy : 0.794897\n",
      "total step : 325 \n",
      "error : 0.661402, accuarcy : 0.794897\n",
      "total step : 326 \n",
      "error : 0.659697, accuarcy : 0.796398\n",
      "total step : 327 \n",
      "error : 0.658004, accuarcy : 0.796898\n",
      "total step : 328 \n",
      "error : 0.656322, accuarcy : 0.797399\n",
      "total step : 329 \n",
      "error : 0.654651, accuarcy : 0.797899\n",
      "total step : 330 \n",
      "error : 0.652991, accuarcy : 0.798399\n",
      "total step : 331 \n",
      "error : 0.651342, accuarcy : 0.798899\n",
      "total step : 332 \n",
      "error : 0.649703, accuarcy : 0.799400\n",
      "total step : 333 \n",
      "error : 0.648075, accuarcy : 0.799400\n",
      "total step : 334 \n",
      "error : 0.646458, accuarcy : 0.800900\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for number 2\n",
      "total step : 1 \n",
      "error : 4.403248, accuarcy : 0.472736\n",
      "total step : 2 \n",
      "error : 4.335695, accuarcy : 0.473237\n",
      "total step : 3 \n",
      "error : 4.268992, accuarcy : 0.472236\n",
      "total step : 4 \n",
      "error : 4.203208, accuarcy : 0.472736\n",
      "total step : 5 \n",
      "error : 4.138409, accuarcy : 0.472236\n",
      "total step : 6 \n",
      "error : 4.074655, accuarcy : 0.474237\n",
      "total step : 7 \n",
      "error : 4.012002, accuarcy : 0.480240\n",
      "total step : 8 \n",
      "error : 3.950500, accuarcy : 0.477239\n",
      "total step : 9 \n",
      "error : 3.890193, accuarcy : 0.480740\n",
      "total step : 10 \n",
      "error : 3.831120, accuarcy : 0.482741\n",
      "total step : 11 \n",
      "error : 3.773313, accuarcy : 0.481241\n",
      "total step : 12 \n",
      "error : 3.716801, accuarcy : 0.481741\n",
      "total step : 13 \n",
      "error : 3.661605, accuarcy : 0.479240\n",
      "total step : 14 \n",
      "error : 3.607745, accuarcy : 0.478239\n",
      "total step : 15 \n",
      "error : 3.555234, accuarcy : 0.481241\n",
      "total step : 16 \n",
      "error : 3.504079, accuarcy : 0.482241\n",
      "total step : 17 \n",
      "error : 3.454284, accuarcy : 0.485743\n",
      "total step : 18 \n",
      "error : 3.405847, accuarcy : 0.485243\n",
      "total step : 19 \n",
      "error : 3.358762, accuarcy : 0.483242\n",
      "total step : 20 \n",
      "error : 3.313021, accuarcy : 0.484242\n",
      "total step : 21 \n",
      "error : 3.268609, accuarcy : 0.486743\n",
      "total step : 22 \n",
      "error : 3.225512, accuarcy : 0.490245\n",
      "total step : 23 \n",
      "error : 3.183713, accuarcy : 0.491246\n",
      "total step : 24 \n",
      "error : 3.143191, accuarcy : 0.494747\n",
      "total step : 25 \n",
      "error : 3.103928, accuarcy : 0.494747\n",
      "total step : 26 \n",
      "error : 3.065900, accuarcy : 0.494247\n",
      "total step : 27 \n",
      "error : 3.029085, accuarcy : 0.496748\n",
      "total step : 28 \n",
      "error : 2.993458, accuarcy : 0.499250\n",
      "total step : 29 \n",
      "error : 2.958993, accuarcy : 0.502751\n",
      "total step : 30 \n",
      "error : 2.925663, accuarcy : 0.504752\n",
      "total step : 31 \n",
      "error : 2.893441, accuarcy : 0.508754\n",
      "total step : 32 \n",
      "error : 2.862296, accuarcy : 0.509255\n",
      "total step : 33 \n",
      "error : 2.832199, accuarcy : 0.508254\n",
      "total step : 34 \n",
      "error : 2.803119, accuarcy : 0.510755\n",
      "total step : 35 \n",
      "error : 2.775025, accuarcy : 0.514757\n",
      "total step : 36 \n",
      "error : 2.747886, accuarcy : 0.518259\n",
      "total step : 37 \n",
      "error : 2.721671, accuarcy : 0.519260\n",
      "total step : 38 \n",
      "error : 2.696349, accuarcy : 0.516258\n",
      "total step : 39 \n",
      "error : 2.671888, accuarcy : 0.518259\n",
      "total step : 40 \n",
      "error : 2.648259, accuarcy : 0.518759\n",
      "total step : 41 \n",
      "error : 2.625432, accuarcy : 0.520760\n",
      "total step : 42 \n",
      "error : 2.603379, accuarcy : 0.521761\n",
      "total step : 43 \n",
      "error : 2.582070, accuarcy : 0.523762\n",
      "total step : 44 \n",
      "error : 2.561479, accuarcy : 0.528264\n",
      "total step : 45 \n",
      "error : 2.541578, accuarcy : 0.529765\n",
      "total step : 46 \n",
      "error : 2.522342, accuarcy : 0.530765\n",
      "total step : 47 \n",
      "error : 2.503746, accuarcy : 0.529765\n",
      "total step : 48 \n",
      "error : 2.485765, accuarcy : 0.530265\n",
      "total step : 49 \n",
      "error : 2.468376, accuarcy : 0.530265\n",
      "total step : 50 \n",
      "error : 2.451555, accuarcy : 0.535268\n",
      "total step : 51 \n",
      "error : 2.435280, accuarcy : 0.538769\n",
      "total step : 52 \n",
      "error : 2.419528, accuarcy : 0.538769\n",
      "total step : 53 \n",
      "error : 2.404280, accuarcy : 0.539270\n",
      "total step : 54 \n",
      "error : 2.389514, accuarcy : 0.540270\n",
      "total step : 55 \n",
      "error : 2.375209, accuarcy : 0.541771\n",
      "total step : 56 \n",
      "error : 2.361347, accuarcy : 0.544772\n",
      "total step : 57 \n",
      "error : 2.347908, accuarcy : 0.547274\n",
      "total step : 58 \n",
      "error : 2.334874, accuarcy : 0.550775\n",
      "total step : 59 \n",
      "error : 2.322227, accuarcy : 0.550275\n",
      "total step : 60 \n",
      "error : 2.309950, accuarcy : 0.551776\n",
      "total step : 61 \n",
      "error : 2.298026, accuarcy : 0.554777\n",
      "total step : 62 \n",
      "error : 2.286440, accuarcy : 0.554777\n",
      "total step : 63 \n",
      "error : 2.275176, accuarcy : 0.555278\n",
      "total step : 64 \n",
      "error : 2.264219, accuarcy : 0.553777\n",
      "total step : 65 \n",
      "error : 2.253556, accuarcy : 0.554777\n",
      "total step : 66 \n",
      "error : 2.243173, accuarcy : 0.557279\n",
      "total step : 67 \n",
      "error : 2.233057, accuarcy : 0.559780\n",
      "total step : 68 \n",
      "error : 2.223196, accuarcy : 0.560780\n",
      "total step : 69 \n",
      "error : 2.213578, accuarcy : 0.562781\n",
      "total step : 70 \n",
      "error : 2.204192, accuarcy : 0.563282\n",
      "total step : 71 \n",
      "error : 2.195027, accuarcy : 0.563282\n",
      "total step : 72 \n",
      "error : 2.186074, accuarcy : 0.563782\n",
      "total step : 73 \n",
      "error : 2.177323, accuarcy : 0.564282\n",
      "total step : 74 \n",
      "error : 2.168765, accuarcy : 0.566283\n",
      "total step : 75 \n",
      "error : 2.160391, accuarcy : 0.567784\n",
      "total step : 76 \n",
      "error : 2.152193, accuarcy : 0.569785\n",
      "total step : 77 \n",
      "error : 2.144163, accuarcy : 0.571786\n",
      "total step : 78 \n",
      "error : 2.136293, accuarcy : 0.573787\n",
      "total step : 79 \n",
      "error : 2.128577, accuarcy : 0.575288\n",
      "total step : 80 \n",
      "error : 2.121009, accuarcy : 0.575788\n",
      "total step : 81 \n",
      "error : 2.113580, accuarcy : 0.574787\n",
      "total step : 82 \n",
      "error : 2.106287, accuarcy : 0.576288\n",
      "total step : 83 \n",
      "error : 2.099122, accuarcy : 0.577289\n",
      "total step : 84 \n",
      "error : 2.092081, accuarcy : 0.577289\n",
      "total step : 85 \n",
      "error : 2.085158, accuarcy : 0.577289\n",
      "total step : 86 \n",
      "error : 2.078348, accuarcy : 0.578289\n",
      "total step : 87 \n",
      "error : 2.071648, accuarcy : 0.577789\n",
      "total step : 88 \n",
      "error : 2.065051, accuarcy : 0.579790\n",
      "total step : 89 \n",
      "error : 2.058554, accuarcy : 0.580790\n",
      "total step : 90 \n",
      "error : 2.052153, accuarcy : 0.581791\n",
      "total step : 91 \n",
      "error : 2.045844, accuarcy : 0.581791\n",
      "total step : 92 \n",
      "error : 2.039624, accuarcy : 0.582791\n",
      "total step : 93 \n",
      "error : 2.033489, accuarcy : 0.582791\n",
      "total step : 94 \n",
      "error : 2.027435, accuarcy : 0.583792\n",
      "total step : 95 \n",
      "error : 2.021460, accuarcy : 0.585793\n",
      "total step : 96 \n",
      "error : 2.015561, accuarcy : 0.585793\n",
      "total step : 97 \n",
      "error : 2.009734, accuarcy : 0.587294\n",
      "total step : 98 \n",
      "error : 2.003978, accuarcy : 0.587294\n",
      "total step : 99 \n",
      "error : 1.998288, accuarcy : 0.588294\n",
      "total step : 100 \n",
      "error : 1.992664, accuarcy : 0.589295\n",
      "total step : 101 \n",
      "error : 1.987102, accuarcy : 0.589795\n",
      "total step : 102 \n",
      "error : 1.981601, accuarcy : 0.590295\n",
      "total step : 103 \n",
      "error : 1.976157, accuarcy : 0.590295\n",
      "total step : 104 \n",
      "error : 1.970770, accuarcy : 0.591296\n",
      "total step : 105 \n",
      "error : 1.965437, accuarcy : 0.592796\n",
      "total step : 106 \n",
      "error : 1.960157, accuarcy : 0.592796\n",
      "total step : 107 \n",
      "error : 1.954926, accuarcy : 0.593297\n",
      "total step : 108 \n",
      "error : 1.949745, accuarcy : 0.594797\n",
      "total step : 109 \n",
      "error : 1.944611, accuarcy : 0.595798\n",
      "total step : 110 \n",
      "error : 1.939523, accuarcy : 0.595798\n",
      "total step : 111 \n",
      "error : 1.934479, accuarcy : 0.596298\n",
      "total step : 112 \n",
      "error : 1.929478, accuarcy : 0.597299\n",
      "total step : 113 \n",
      "error : 1.924518, accuarcy : 0.597799\n",
      "total step : 114 \n",
      "error : 1.919598, accuarcy : 0.598299\n",
      "total step : 115 \n",
      "error : 1.914718, accuarcy : 0.599300\n",
      "total step : 116 \n",
      "error : 1.909875, accuarcy : 0.600300\n",
      "total step : 117 \n",
      "error : 1.905069, accuarcy : 0.600800\n",
      "total step : 118 \n",
      "error : 1.900298, accuarcy : 0.601301\n",
      "total step : 119 \n",
      "error : 1.895562, accuarcy : 0.602801\n",
      "total step : 120 \n",
      "error : 1.890860, accuarcy : 0.603802\n",
      "total step : 121 \n",
      "error : 1.886191, accuarcy : 0.603802\n",
      "total step : 122 \n",
      "error : 1.881553, accuarcy : 0.605303\n",
      "total step : 123 \n",
      "error : 1.876946, accuarcy : 0.605803\n",
      "total step : 124 \n",
      "error : 1.872369, accuarcy : 0.606303\n",
      "total step : 125 \n",
      "error : 1.867822, accuarcy : 0.606803\n",
      "total step : 126 \n",
      "error : 1.863303, accuarcy : 0.606803\n",
      "total step : 127 \n",
      "error : 1.858812, accuarcy : 0.607304\n",
      "total step : 128 \n",
      "error : 1.854349, accuarcy : 0.607304\n",
      "total step : 129 \n",
      "error : 1.849912, accuarcy : 0.609305\n",
      "total step : 130 \n",
      "error : 1.845501, accuarcy : 0.609805\n",
      "total step : 131 \n",
      "error : 1.841115, accuarcy : 0.610805\n",
      "total step : 132 \n",
      "error : 1.836754, accuarcy : 0.613307\n",
      "total step : 133 \n",
      "error : 1.832417, accuarcy : 0.614307\n",
      "total step : 134 \n",
      "error : 1.828104, accuarcy : 0.615308\n",
      "total step : 135 \n",
      "error : 1.823814, accuarcy : 0.615808\n",
      "total step : 136 \n",
      "error : 1.819547, accuarcy : 0.615808\n",
      "total step : 137 \n",
      "error : 1.815303, accuarcy : 0.615808\n",
      "total step : 138 \n",
      "error : 1.811080, accuarcy : 0.616808\n",
      "total step : 139 \n",
      "error : 1.806878, accuarcy : 0.617309\n",
      "total step : 140 \n",
      "error : 1.802698, accuarcy : 0.618309\n",
      "total step : 141 \n",
      "error : 1.798538, accuarcy : 0.619310\n",
      "total step : 142 \n",
      "error : 1.794398, accuarcy : 0.620310\n",
      "total step : 143 \n",
      "error : 1.790278, accuarcy : 0.620310\n",
      "total step : 144 \n",
      "error : 1.786178, accuarcy : 0.620310\n",
      "total step : 145 \n",
      "error : 1.782097, accuarcy : 0.621811\n",
      "total step : 146 \n",
      "error : 1.778035, accuarcy : 0.621811\n",
      "total step : 147 \n",
      "error : 1.773992, accuarcy : 0.622811\n",
      "total step : 148 \n",
      "error : 1.769967, accuarcy : 0.623312\n",
      "total step : 149 \n",
      "error : 1.765960, accuarcy : 0.623812\n",
      "total step : 150 \n",
      "error : 1.761971, accuarcy : 0.623812\n",
      "total step : 151 \n",
      "error : 1.757999, accuarcy : 0.624812\n",
      "total step : 152 \n",
      "error : 1.754045, accuarcy : 0.626313\n",
      "total step : 153 \n",
      "error : 1.750107, accuarcy : 0.626813\n",
      "total step : 154 \n",
      "error : 1.746187, accuarcy : 0.626813\n",
      "total step : 155 \n",
      "error : 1.742283, accuarcy : 0.627314\n",
      "total step : 156 \n",
      "error : 1.738396, accuarcy : 0.628314\n",
      "total step : 157 \n",
      "error : 1.734524, accuarcy : 0.629315\n",
      "total step : 158 \n",
      "error : 1.730669, accuarcy : 0.629315\n",
      "total step : 159 \n",
      "error : 1.726830, accuarcy : 0.630815\n",
      "total step : 160 \n",
      "error : 1.723006, accuarcy : 0.632316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 161 \n",
      "error : 1.719198, accuarcy : 0.633817\n",
      "total step : 162 \n",
      "error : 1.715405, accuarcy : 0.635818\n",
      "total step : 163 \n",
      "error : 1.711627, accuarcy : 0.635818\n",
      "total step : 164 \n",
      "error : 1.707864, accuarcy : 0.636318\n",
      "total step : 165 \n",
      "error : 1.704116, accuarcy : 0.636818\n",
      "total step : 166 \n",
      "error : 1.700382, accuarcy : 0.636818\n",
      "total step : 167 \n",
      "error : 1.696663, accuarcy : 0.637819\n",
      "total step : 168 \n",
      "error : 1.692959, accuarcy : 0.639320\n",
      "total step : 169 \n",
      "error : 1.689268, accuarcy : 0.639820\n",
      "total step : 170 \n",
      "error : 1.685592, accuarcy : 0.640820\n",
      "total step : 171 \n",
      "error : 1.681930, accuarcy : 0.641321\n",
      "total step : 172 \n",
      "error : 1.678282, accuarcy : 0.641321\n",
      "total step : 173 \n",
      "error : 1.674648, accuarcy : 0.642321\n",
      "total step : 174 \n",
      "error : 1.671027, accuarcy : 0.643322\n",
      "total step : 175 \n",
      "error : 1.667420, accuarcy : 0.643322\n",
      "total step : 176 \n",
      "error : 1.663826, accuarcy : 0.643322\n",
      "total step : 177 \n",
      "error : 1.660246, accuarcy : 0.643322\n",
      "total step : 178 \n",
      "error : 1.656679, accuarcy : 0.643822\n",
      "total step : 179 \n",
      "error : 1.653125, accuarcy : 0.644322\n",
      "total step : 180 \n",
      "error : 1.649584, accuarcy : 0.646323\n",
      "total step : 181 \n",
      "error : 1.646056, accuarcy : 0.647324\n",
      "total step : 182 \n",
      "error : 1.642541, accuarcy : 0.647824\n",
      "total step : 183 \n",
      "error : 1.639039, accuarcy : 0.647824\n",
      "total step : 184 \n",
      "error : 1.635550, accuarcy : 0.648324\n",
      "total step : 185 \n",
      "error : 1.632073, accuarcy : 0.648324\n",
      "total step : 186 \n",
      "error : 1.628609, accuarcy : 0.648324\n",
      "total step : 187 \n",
      "error : 1.625157, accuarcy : 0.648824\n",
      "total step : 188 \n",
      "error : 1.621718, accuarcy : 0.649325\n",
      "total step : 189 \n",
      "error : 1.618291, accuarcy : 0.649325\n",
      "total step : 190 \n",
      "error : 1.614876, accuarcy : 0.649325\n",
      "total step : 191 \n",
      "error : 1.611474, accuarcy : 0.649825\n",
      "total step : 192 \n",
      "error : 1.608083, accuarcy : 0.650325\n",
      "total step : 193 \n",
      "error : 1.604705, accuarcy : 0.651826\n",
      "total step : 194 \n",
      "error : 1.601339, accuarcy : 0.652826\n",
      "total step : 195 \n",
      "error : 1.597984, accuarcy : 0.654327\n",
      "total step : 196 \n",
      "error : 1.594642, accuarcy : 0.654327\n",
      "total step : 197 \n",
      "error : 1.591311, accuarcy : 0.654327\n",
      "total step : 198 \n",
      "error : 1.587992, accuarcy : 0.654327\n",
      "total step : 199 \n",
      "error : 1.584685, accuarcy : 0.654827\n",
      "total step : 200 \n",
      "error : 1.581390, accuarcy : 0.655328\n",
      "total step : 201 \n",
      "error : 1.578105, accuarcy : 0.655328\n",
      "total step : 202 \n",
      "error : 1.574833, accuarcy : 0.656328\n",
      "total step : 203 \n",
      "error : 1.571572, accuarcy : 0.656328\n",
      "total step : 204 \n",
      "error : 1.568322, accuarcy : 0.656328\n",
      "total step : 205 \n",
      "error : 1.565084, accuarcy : 0.656828\n",
      "total step : 206 \n",
      "error : 1.561857, accuarcy : 0.657329\n",
      "total step : 207 \n",
      "error : 1.558641, accuarcy : 0.658329\n",
      "total step : 208 \n",
      "error : 1.555437, accuarcy : 0.658329\n",
      "total step : 209 \n",
      "error : 1.552244, accuarcy : 0.658329\n",
      "total step : 210 \n",
      "error : 1.549062, accuarcy : 0.658329\n",
      "total step : 211 \n",
      "error : 1.545890, accuarcy : 0.658329\n",
      "total step : 212 \n",
      "error : 1.542730, accuarcy : 0.658829\n",
      "total step : 213 \n",
      "error : 1.539581, accuarcy : 0.659330\n",
      "total step : 214 \n",
      "error : 1.536443, accuarcy : 0.659830\n",
      "total step : 215 \n",
      "error : 1.533316, accuarcy : 0.659830\n",
      "total step : 216 \n",
      "error : 1.530199, accuarcy : 0.660830\n",
      "total step : 217 \n",
      "error : 1.527094, accuarcy : 0.660330\n",
      "total step : 218 \n",
      "error : 1.523999, accuarcy : 0.661331\n",
      "total step : 219 \n",
      "error : 1.520915, accuarcy : 0.661831\n",
      "total step : 220 \n",
      "error : 1.517841, accuarcy : 0.662331\n",
      "total step : 221 \n",
      "error : 1.514778, accuarcy : 0.662331\n",
      "total step : 222 \n",
      "error : 1.511726, accuarcy : 0.662331\n",
      "total step : 223 \n",
      "error : 1.508684, accuarcy : 0.663832\n",
      "total step : 224 \n",
      "error : 1.505653, accuarcy : 0.663832\n",
      "total step : 225 \n",
      "error : 1.502632, accuarcy : 0.663832\n",
      "total step : 226 \n",
      "error : 1.499622, accuarcy : 0.663832\n",
      "total step : 227 \n",
      "error : 1.496622, accuarcy : 0.664332\n",
      "total step : 228 \n",
      "error : 1.493632, accuarcy : 0.664832\n",
      "total step : 229 \n",
      "error : 1.490653, accuarcy : 0.666333\n",
      "total step : 230 \n",
      "error : 1.487684, accuarcy : 0.666833\n",
      "total step : 231 \n",
      "error : 1.484725, accuarcy : 0.667334\n",
      "total step : 232 \n",
      "error : 1.481776, accuarcy : 0.667334\n",
      "total step : 233 \n",
      "error : 1.478838, accuarcy : 0.668334\n",
      "total step : 234 \n",
      "error : 1.475910, accuarcy : 0.668334\n",
      "total step : 235 \n",
      "error : 1.472992, accuarcy : 0.668334\n",
      "total step : 236 \n",
      "error : 1.470083, accuarcy : 0.668334\n",
      "total step : 237 \n",
      "error : 1.467185, accuarcy : 0.668834\n",
      "total step : 238 \n",
      "error : 1.464297, accuarcy : 0.668334\n",
      "total step : 239 \n",
      "error : 1.461419, accuarcy : 0.668834\n",
      "total step : 240 \n",
      "error : 1.458551, accuarcy : 0.669335\n",
      "total step : 241 \n",
      "error : 1.455692, accuarcy : 0.670335\n",
      "total step : 242 \n",
      "error : 1.452844, accuarcy : 0.670335\n",
      "total step : 243 \n",
      "error : 1.450005, accuarcy : 0.671336\n",
      "total step : 244 \n",
      "error : 1.447176, accuarcy : 0.671836\n",
      "total step : 245 \n",
      "error : 1.444357, accuarcy : 0.672336\n",
      "total step : 246 \n",
      "error : 1.441547, accuarcy : 0.672336\n",
      "total step : 247 \n",
      "error : 1.438748, accuarcy : 0.672836\n",
      "total step : 248 \n",
      "error : 1.435957, accuarcy : 0.673837\n",
      "total step : 249 \n",
      "error : 1.433177, accuarcy : 0.674837\n",
      "total step : 250 \n",
      "error : 1.430406, accuarcy : 0.674837\n",
      "total step : 251 \n",
      "error : 1.427644, accuarcy : 0.675338\n",
      "total step : 252 \n",
      "error : 1.424892, accuarcy : 0.675338\n",
      "total step : 253 \n",
      "error : 1.422150, accuarcy : 0.676838\n",
      "total step : 254 \n",
      "error : 1.419417, accuarcy : 0.677339\n",
      "total step : 255 \n",
      "error : 1.416693, accuarcy : 0.677339\n",
      "total step : 256 \n",
      "error : 1.413979, accuarcy : 0.677339\n",
      "total step : 257 \n",
      "error : 1.411274, accuarcy : 0.677339\n",
      "total step : 258 \n",
      "error : 1.408578, accuarcy : 0.677339\n",
      "total step : 259 \n",
      "error : 1.405892, accuarcy : 0.678839\n",
      "total step : 260 \n",
      "error : 1.403214, accuarcy : 0.679340\n",
      "total step : 261 \n",
      "error : 1.400546, accuarcy : 0.679840\n",
      "total step : 262 \n",
      "error : 1.397888, accuarcy : 0.680340\n",
      "total step : 263 \n",
      "error : 1.395238, accuarcy : 0.680340\n",
      "total step : 264 \n",
      "error : 1.392597, accuarcy : 0.681341\n",
      "total step : 265 \n",
      "error : 1.389966, accuarcy : 0.683342\n",
      "total step : 266 \n",
      "error : 1.387344, accuarcy : 0.683342\n",
      "total step : 267 \n",
      "error : 1.384730, accuarcy : 0.683842\n",
      "total step : 268 \n",
      "error : 1.382126, accuarcy : 0.684842\n",
      "total step : 269 \n",
      "error : 1.379530, accuarcy : 0.684842\n",
      "total step : 270 \n",
      "error : 1.376944, accuarcy : 0.684842\n",
      "total step : 271 \n",
      "error : 1.374366, accuarcy : 0.685343\n",
      "total step : 272 \n",
      "error : 1.371797, accuarcy : 0.685843\n",
      "total step : 273 \n",
      "error : 1.369237, accuarcy : 0.685843\n",
      "total step : 274 \n",
      "error : 1.366686, accuarcy : 0.686343\n",
      "total step : 275 \n",
      "error : 1.364143, accuarcy : 0.686843\n",
      "total step : 276 \n",
      "error : 1.361609, accuarcy : 0.686843\n",
      "total step : 277 \n",
      "error : 1.359084, accuarcy : 0.687344\n",
      "total step : 278 \n",
      "error : 1.356568, accuarcy : 0.687844\n",
      "total step : 279 \n",
      "error : 1.354060, accuarcy : 0.688844\n",
      "total step : 280 \n",
      "error : 1.351561, accuarcy : 0.689345\n",
      "total step : 281 \n",
      "error : 1.349070, accuarcy : 0.689845\n",
      "total step : 282 \n",
      "error : 1.346588, accuarcy : 0.689845\n",
      "total step : 283 \n",
      "error : 1.344114, accuarcy : 0.691346\n",
      "total step : 284 \n",
      "error : 1.341649, accuarcy : 0.691846\n",
      "total step : 285 \n",
      "error : 1.339192, accuarcy : 0.692346\n",
      "total step : 286 \n",
      "error : 1.336744, accuarcy : 0.691846\n",
      "total step : 287 \n",
      "error : 1.334304, accuarcy : 0.691846\n",
      "total step : 288 \n",
      "error : 1.331872, accuarcy : 0.691846\n",
      "total step : 289 \n",
      "error : 1.329449, accuarcy : 0.691846\n",
      "total step : 290 \n",
      "error : 1.327034, accuarcy : 0.692346\n",
      "total step : 291 \n",
      "error : 1.324627, accuarcy : 0.692346\n",
      "total step : 292 \n",
      "error : 1.322229, accuarcy : 0.692846\n",
      "total step : 293 \n",
      "error : 1.319838, accuarcy : 0.693847\n",
      "total step : 294 \n",
      "error : 1.317456, accuarcy : 0.693847\n",
      "total step : 295 \n",
      "error : 1.315082, accuarcy : 0.695348\n",
      "total step : 296 \n",
      "error : 1.312716, accuarcy : 0.695848\n",
      "total step : 297 \n",
      "error : 1.310358, accuarcy : 0.696348\n",
      "total step : 298 \n",
      "error : 1.308008, accuarcy : 0.696848\n",
      "total step : 299 \n",
      "error : 1.305666, accuarcy : 0.697349\n",
      "total step : 300 \n",
      "error : 1.303331, accuarcy : 0.697349\n",
      "total step : 301 \n",
      "error : 1.301005, accuarcy : 0.697849\n",
      "total step : 302 \n",
      "error : 1.298687, accuarcy : 0.697849\n",
      "total step : 303 \n",
      "error : 1.296377, accuarcy : 0.697849\n",
      "total step : 304 \n",
      "error : 1.294074, accuarcy : 0.698349\n",
      "total step : 305 \n",
      "error : 1.291779, accuarcy : 0.698349\n",
      "total step : 306 \n",
      "error : 1.289492, accuarcy : 0.698849\n",
      "total step : 307 \n",
      "error : 1.287213, accuarcy : 0.699350\n",
      "total step : 308 \n",
      "error : 1.284942, accuarcy : 0.699350\n",
      "total step : 309 \n",
      "error : 1.282678, accuarcy : 0.699850\n",
      "total step : 310 \n",
      "error : 1.280422, accuarcy : 0.699850\n",
      "total step : 311 \n",
      "error : 1.278173, accuarcy : 0.699850\n",
      "total step : 312 \n",
      "error : 1.275932, accuarcy : 0.700850\n",
      "total step : 313 \n",
      "error : 1.273699, accuarcy : 0.701851\n",
      "total step : 314 \n",
      "error : 1.271473, accuarcy : 0.701851\n",
      "total step : 315 \n",
      "error : 1.269255, accuarcy : 0.702851\n",
      "total step : 316 \n",
      "error : 1.267044, accuarcy : 0.704352\n",
      "total step : 317 \n",
      "error : 1.264840, accuarcy : 0.704852\n",
      "total step : 318 \n",
      "error : 1.262644, accuarcy : 0.704852\n",
      "total step : 319 \n",
      "error : 1.260455, accuarcy : 0.704852\n",
      "total step : 320 \n",
      "error : 1.258274, accuarcy : 0.704852\n",
      "total step : 321 \n",
      "error : 1.256100, accuarcy : 0.705353\n",
      "total step : 322 \n",
      "error : 1.253933, accuarcy : 0.705853\n",
      "total step : 323 \n",
      "error : 1.251774, accuarcy : 0.705853\n",
      "total step : 324 \n",
      "error : 1.249622, accuarcy : 0.705853\n",
      "total step : 325 \n",
      "error : 1.247477, accuarcy : 0.706353\n",
      "total step : 326 \n",
      "error : 1.245339, accuarcy : 0.707354\n",
      "total step : 327 \n",
      "error : 1.243208, accuarcy : 0.707354\n",
      "total step : 328 \n",
      "error : 1.241084, accuarcy : 0.707354\n",
      "total step : 329 \n",
      "error : 1.238968, accuarcy : 0.707854\n",
      "total step : 330 \n",
      "error : 1.236858, accuarcy : 0.707854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 331 \n",
      "error : 1.234756, accuarcy : 0.708354\n",
      "total step : 332 \n",
      "error : 1.232660, accuarcy : 0.709855\n",
      "total step : 333 \n",
      "error : 1.230572, accuarcy : 0.709855\n",
      "total step : 334 \n",
      "error : 1.228490, accuarcy : 0.710855\n",
      "total step : 335 \n",
      "error : 1.226416, accuarcy : 0.710855\n",
      "total step : 336 \n",
      "error : 1.224348, accuarcy : 0.710855\n",
      "total step : 337 \n",
      "error : 1.222287, accuarcy : 0.710855\n",
      "total step : 338 \n",
      "error : 1.220233, accuarcy : 0.710855\n",
      "total step : 339 \n",
      "error : 1.218186, accuarcy : 0.710855\n",
      "total step : 340 \n",
      "error : 1.216145, accuarcy : 0.710855\n",
      "total step : 341 \n",
      "error : 1.214112, accuarcy : 0.710855\n",
      "total step : 342 \n",
      "error : 1.212085, accuarcy : 0.710855\n",
      "total step : 343 \n",
      "error : 1.210064, accuarcy : 0.710855\n",
      "total step : 344 \n",
      "error : 1.208051, accuarcy : 0.711856\n",
      "total step : 345 \n",
      "error : 1.206044, accuarcy : 0.711856\n",
      "total step : 346 \n",
      "error : 1.204043, accuarcy : 0.711856\n",
      "total step : 347 \n",
      "error : 1.202049, accuarcy : 0.711856\n",
      "total step : 348 \n",
      "error : 1.200062, accuarcy : 0.711856\n",
      "total step : 349 \n",
      "error : 1.198081, accuarcy : 0.711856\n",
      "total step : 350 \n",
      "error : 1.196107, accuarcy : 0.712356\n",
      "total step : 351 \n",
      "error : 1.194139, accuarcy : 0.712856\n",
      "total step : 352 \n",
      "error : 1.192178, accuarcy : 0.713357\n",
      "total step : 353 \n",
      "error : 1.190223, accuarcy : 0.713357\n",
      "total step : 354 \n",
      "error : 1.188275, accuarcy : 0.713857\n",
      "total step : 355 \n",
      "error : 1.186333, accuarcy : 0.713857\n",
      "total step : 356 \n",
      "error : 1.184397, accuarcy : 0.714857\n",
      "total step : 357 \n",
      "error : 1.182467, accuarcy : 0.714857\n",
      "total step : 358 \n",
      "error : 1.180544, accuarcy : 0.715358\n",
      "total step : 359 \n",
      "error : 1.178627, accuarcy : 0.715358\n",
      "total step : 360 \n",
      "error : 1.176717, accuarcy : 0.715358\n",
      "total step : 361 \n",
      "error : 1.174812, accuarcy : 0.715858\n",
      "total step : 362 \n",
      "error : 1.172914, accuarcy : 0.715858\n",
      "total step : 363 \n",
      "error : 1.171022, accuarcy : 0.716358\n",
      "total step : 364 \n",
      "error : 1.169136, accuarcy : 0.716358\n",
      "total step : 365 \n",
      "error : 1.167256, accuarcy : 0.717359\n",
      "total step : 366 \n",
      "error : 1.165382, accuarcy : 0.717359\n",
      "total step : 367 \n",
      "error : 1.163515, accuarcy : 0.717359\n",
      "total step : 368 \n",
      "error : 1.161653, accuarcy : 0.717359\n",
      "total step : 369 \n",
      "error : 1.159798, accuarcy : 0.718359\n",
      "total step : 370 \n",
      "error : 1.157948, accuarcy : 0.719360\n",
      "total step : 371 \n",
      "error : 1.156104, accuarcy : 0.719360\n",
      "total step : 372 \n",
      "error : 1.154267, accuarcy : 0.720360\n",
      "total step : 373 \n",
      "error : 1.152435, accuarcy : 0.720360\n",
      "total step : 374 \n",
      "error : 1.150609, accuarcy : 0.720360\n",
      "total step : 375 \n",
      "error : 1.148789, accuarcy : 0.720360\n",
      "total step : 376 \n",
      "error : 1.146975, accuarcy : 0.720360\n",
      "total step : 377 \n",
      "error : 1.145167, accuarcy : 0.720360\n",
      "total step : 378 \n",
      "error : 1.143364, accuarcy : 0.720860\n",
      "total step : 379 \n",
      "error : 1.141568, accuarcy : 0.721361\n",
      "total step : 380 \n",
      "error : 1.139777, accuarcy : 0.721361\n",
      "total step : 381 \n",
      "error : 1.137992, accuarcy : 0.721361\n",
      "total step : 382 \n",
      "error : 1.136212, accuarcy : 0.721361\n",
      "total step : 383 \n",
      "error : 1.134438, accuarcy : 0.722361\n",
      "total step : 384 \n",
      "error : 1.132670, accuarcy : 0.722361\n",
      "total step : 385 \n",
      "error : 1.130908, accuarcy : 0.722861\n",
      "total step : 386 \n",
      "error : 1.129151, accuarcy : 0.723862\n",
      "total step : 387 \n",
      "error : 1.127400, accuarcy : 0.723862\n",
      "total step : 388 \n",
      "error : 1.125654, accuarcy : 0.723862\n",
      "total step : 389 \n",
      "error : 1.123914, accuarcy : 0.724362\n",
      "total step : 390 \n",
      "error : 1.122180, accuarcy : 0.724362\n",
      "total step : 391 \n",
      "error : 1.120451, accuarcy : 0.724862\n",
      "total step : 392 \n",
      "error : 1.118727, accuarcy : 0.724862\n",
      "total step : 393 \n",
      "error : 1.117009, accuarcy : 0.725363\n",
      "total step : 394 \n",
      "error : 1.115297, accuarcy : 0.726363\n",
      "total step : 395 \n",
      "error : 1.113590, accuarcy : 0.727364\n",
      "total step : 396 \n",
      "error : 1.111888, accuarcy : 0.727364\n",
      "total step : 397 \n",
      "error : 1.110192, accuarcy : 0.727864\n",
      "total step : 398 \n",
      "error : 1.108501, accuarcy : 0.727864\n",
      "total step : 399 \n",
      "error : 1.106815, accuarcy : 0.727864\n",
      "total step : 400 \n",
      "error : 1.105135, accuarcy : 0.727864\n",
      "total step : 401 \n",
      "error : 1.103460, accuarcy : 0.728364\n",
      "total step : 402 \n",
      "error : 1.101791, accuarcy : 0.728364\n",
      "total step : 403 \n",
      "error : 1.100126, accuarcy : 0.728364\n",
      "total step : 404 \n",
      "error : 1.098467, accuarcy : 0.729365\n",
      "total step : 405 \n",
      "error : 1.096813, accuarcy : 0.729365\n",
      "total step : 406 \n",
      "error : 1.095165, accuarcy : 0.729365\n",
      "total step : 407 \n",
      "error : 1.093521, accuarcy : 0.729365\n",
      "total step : 408 \n",
      "error : 1.091883, accuarcy : 0.729865\n",
      "total step : 409 \n",
      "error : 1.090250, accuarcy : 0.730365\n",
      "total step : 410 \n",
      "error : 1.088622, accuarcy : 0.730365\n",
      "total step : 411 \n",
      "error : 1.086999, accuarcy : 0.730365\n",
      "total step : 412 \n",
      "error : 1.085381, accuarcy : 0.730365\n",
      "total step : 413 \n",
      "error : 1.083769, accuarcy : 0.731366\n",
      "total step : 414 \n",
      "error : 1.082161, accuarcy : 0.731366\n",
      "total step : 415 \n",
      "error : 1.080559, accuarcy : 0.731366\n",
      "total step : 416 \n",
      "error : 1.078961, accuarcy : 0.731866\n",
      "total step : 417 \n",
      "error : 1.077368, accuarcy : 0.731866\n",
      "total step : 418 \n",
      "error : 1.075781, accuarcy : 0.732866\n",
      "total step : 419 \n",
      "error : 1.074198, accuarcy : 0.733367\n",
      "total step : 420 \n",
      "error : 1.072621, accuarcy : 0.734367\n",
      "total step : 421 \n",
      "error : 1.071048, accuarcy : 0.734867\n",
      "total step : 422 \n",
      "error : 1.069480, accuarcy : 0.734867\n",
      "total step : 423 \n",
      "error : 1.067917, accuarcy : 0.734867\n",
      "total step : 424 \n",
      "error : 1.066359, accuarcy : 0.735868\n",
      "total step : 425 \n",
      "error : 1.064806, accuarcy : 0.736368\n",
      "total step : 426 \n",
      "error : 1.063258, accuarcy : 0.737369\n",
      "total step : 427 \n",
      "error : 1.061714, accuarcy : 0.737369\n",
      "total step : 428 \n",
      "error : 1.060175, accuarcy : 0.737869\n",
      "total step : 429 \n",
      "error : 1.058641, accuarcy : 0.737369\n",
      "total step : 430 \n",
      "error : 1.057112, accuarcy : 0.737869\n",
      "total step : 431 \n",
      "error : 1.055588, accuarcy : 0.737869\n",
      "total step : 432 \n",
      "error : 1.054068, accuarcy : 0.737869\n",
      "total step : 433 \n",
      "error : 1.052553, accuarcy : 0.737869\n",
      "total step : 434 \n",
      "error : 1.051043, accuarcy : 0.738369\n",
      "total step : 435 \n",
      "error : 1.049537, accuarcy : 0.738869\n",
      "total step : 436 \n",
      "error : 1.048037, accuarcy : 0.739370\n",
      "total step : 437 \n",
      "error : 1.046540, accuarcy : 0.739370\n",
      "total step : 438 \n",
      "error : 1.045049, accuarcy : 0.739370\n",
      "total step : 439 \n",
      "error : 1.043562, accuarcy : 0.739370\n",
      "total step : 440 \n",
      "error : 1.042079, accuarcy : 0.740370\n",
      "total step : 441 \n",
      "error : 1.040601, accuarcy : 0.740370\n",
      "total step : 442 \n",
      "error : 1.039128, accuarcy : 0.740370\n",
      "total step : 443 \n",
      "error : 1.037659, accuarcy : 0.740870\n",
      "total step : 444 \n",
      "error : 1.036195, accuarcy : 0.740870\n",
      "total step : 445 \n",
      "error : 1.034736, accuarcy : 0.740870\n",
      "total step : 446 \n",
      "error : 1.033280, accuarcy : 0.741371\n",
      "total step : 447 \n",
      "error : 1.031830, accuarcy : 0.741371\n",
      "total step : 448 \n",
      "error : 1.030384, accuarcy : 0.741371\n",
      "total step : 449 \n",
      "error : 1.028942, accuarcy : 0.741871\n",
      "total step : 450 \n",
      "error : 1.027505, accuarcy : 0.741871\n",
      "total step : 451 \n",
      "error : 1.026072, accuarcy : 0.742371\n",
      "total step : 452 \n",
      "error : 1.024643, accuarcy : 0.742371\n",
      "total step : 453 \n",
      "error : 1.023219, accuarcy : 0.743372\n",
      "total step : 454 \n",
      "error : 1.021799, accuarcy : 0.743372\n",
      "total step : 455 \n",
      "error : 1.020384, accuarcy : 0.743372\n",
      "total step : 456 \n",
      "error : 1.018973, accuarcy : 0.743872\n",
      "total step : 457 \n",
      "error : 1.017566, accuarcy : 0.743872\n",
      "total step : 458 \n",
      "error : 1.016164, accuarcy : 0.743872\n",
      "total step : 459 \n",
      "error : 1.014766, accuarcy : 0.743872\n",
      "total step : 460 \n",
      "error : 1.013372, accuarcy : 0.743872\n",
      "total step : 461 \n",
      "error : 1.011982, accuarcy : 0.744372\n",
      "total step : 462 \n",
      "error : 1.010597, accuarcy : 0.744372\n",
      "total step : 463 \n",
      "error : 1.009216, accuarcy : 0.744872\n",
      "total step : 464 \n",
      "error : 1.007839, accuarcy : 0.745373\n",
      "total step : 465 \n",
      "error : 1.006467, accuarcy : 0.746373\n",
      "total step : 466 \n",
      "error : 1.005098, accuarcy : 0.746373\n",
      "total step : 467 \n",
      "error : 1.003734, accuarcy : 0.747374\n",
      "total step : 468 \n",
      "error : 1.002374, accuarcy : 0.747374\n",
      "total step : 469 \n",
      "error : 1.001018, accuarcy : 0.747374\n",
      "total step : 470 \n",
      "error : 0.999666, accuarcy : 0.747874\n",
      "total step : 471 \n",
      "error : 0.998318, accuarcy : 0.747874\n",
      "total step : 472 \n",
      "error : 0.996975, accuarcy : 0.747374\n",
      "total step : 473 \n",
      "error : 0.995635, accuarcy : 0.747874\n",
      "total step : 474 \n",
      "error : 0.994300, accuarcy : 0.747874\n",
      "total step : 475 \n",
      "error : 0.992968, accuarcy : 0.747874\n",
      "total step : 476 \n",
      "error : 0.991641, accuarcy : 0.749375\n",
      "total step : 477 \n",
      "error : 0.990318, accuarcy : 0.749375\n",
      "total step : 478 \n",
      "error : 0.988999, accuarcy : 0.749875\n",
      "total step : 479 \n",
      "error : 0.987683, accuarcy : 0.749875\n",
      "total step : 480 \n",
      "error : 0.986372, accuarcy : 0.750375\n",
      "total step : 481 \n",
      "error : 0.985065, accuarcy : 0.750375\n",
      "total step : 482 \n",
      "error : 0.983761, accuarcy : 0.750375\n",
      "total step : 483 \n",
      "error : 0.982462, accuarcy : 0.750375\n",
      "total step : 484 \n",
      "error : 0.981166, accuarcy : 0.750375\n",
      "total step : 485 \n",
      "error : 0.979875, accuarcy : 0.750375\n",
      "total step : 486 \n",
      "error : 0.978587, accuarcy : 0.749875\n",
      "total step : 487 \n",
      "error : 0.977303, accuarcy : 0.749875\n",
      "total step : 488 \n",
      "error : 0.976024, accuarcy : 0.749375\n",
      "total step : 489 \n",
      "error : 0.974748, accuarcy : 0.749375\n",
      "total step : 490 \n",
      "error : 0.973475, accuarcy : 0.749375\n",
      "total step : 491 \n",
      "error : 0.972207, accuarcy : 0.750375\n",
      "total step : 492 \n",
      "error : 0.970943, accuarcy : 0.750375\n",
      "total step : 493 \n",
      "error : 0.969682, accuarcy : 0.750375\n",
      "total step : 494 \n",
      "error : 0.968425, accuarcy : 0.750875\n",
      "total step : 495 \n",
      "error : 0.967172, accuarcy : 0.750875\n",
      "total step : 496 \n",
      "error : 0.965923, accuarcy : 0.750875\n",
      "total step : 497 \n",
      "error : 0.964677, accuarcy : 0.751376\n",
      "total step : 498 \n",
      "error : 0.963435, accuarcy : 0.751376\n",
      "total step : 499 \n",
      "error : 0.962197, accuarcy : 0.751376\n",
      "total step : 500 \n",
      "error : 0.960963, accuarcy : 0.751876\n",
      "total step : 501 \n",
      "error : 0.959732, accuarcy : 0.752376\n",
      "total step : 502 \n",
      "error : 0.958505, accuarcy : 0.752376\n",
      "total step : 503 \n",
      "error : 0.957282, accuarcy : 0.752876\n",
      "total step : 504 \n",
      "error : 0.956062, accuarcy : 0.752876\n",
      "total step : 505 \n",
      "error : 0.954846, accuarcy : 0.752876\n",
      "total step : 506 \n",
      "error : 0.953634, accuarcy : 0.752876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 507 \n",
      "error : 0.952425, accuarcy : 0.752876\n",
      "total step : 508 \n",
      "error : 0.951220, accuarcy : 0.754377\n",
      "total step : 509 \n",
      "error : 0.950019, accuarcy : 0.754377\n",
      "total step : 510 \n",
      "error : 0.948821, accuarcy : 0.754377\n",
      "total step : 511 \n",
      "error : 0.947627, accuarcy : 0.754377\n",
      "total step : 512 \n",
      "error : 0.946436, accuarcy : 0.754877\n",
      "total step : 513 \n",
      "error : 0.945249, accuarcy : 0.755378\n",
      "total step : 514 \n",
      "error : 0.944065, accuarcy : 0.755378\n",
      "total step : 515 \n",
      "error : 0.942885, accuarcy : 0.755878\n",
      "total step : 516 \n",
      "error : 0.941709, accuarcy : 0.755878\n",
      "total step : 517 \n",
      "error : 0.940536, accuarcy : 0.755878\n",
      "total step : 518 \n",
      "error : 0.939366, accuarcy : 0.755878\n",
      "total step : 519 \n",
      "error : 0.938200, accuarcy : 0.756378\n",
      "total step : 520 \n",
      "error : 0.937037, accuarcy : 0.756878\n",
      "total step : 521 \n",
      "error : 0.935878, accuarcy : 0.756878\n",
      "total step : 522 \n",
      "error : 0.934722, accuarcy : 0.756878\n",
      "total step : 523 \n",
      "error : 0.933570, accuarcy : 0.757379\n",
      "total step : 524 \n",
      "error : 0.932421, accuarcy : 0.757379\n",
      "total step : 525 \n",
      "error : 0.931276, accuarcy : 0.757379\n",
      "total step : 526 \n",
      "error : 0.930134, accuarcy : 0.757879\n",
      "total step : 527 \n",
      "error : 0.928995, accuarcy : 0.757879\n",
      "total step : 528 \n",
      "error : 0.927860, accuarcy : 0.757879\n",
      "total step : 529 \n",
      "error : 0.926728, accuarcy : 0.757879\n",
      "total step : 530 \n",
      "error : 0.925599, accuarcy : 0.757879\n",
      "total step : 531 \n",
      "error : 0.924474, accuarcy : 0.757879\n",
      "total step : 532 \n",
      "error : 0.923352, accuarcy : 0.757879\n",
      "total step : 533 \n",
      "error : 0.922234, accuarcy : 0.757879\n",
      "total step : 534 \n",
      "error : 0.921118, accuarcy : 0.757879\n",
      "total step : 535 \n",
      "error : 0.920007, accuarcy : 0.758379\n",
      "total step : 536 \n",
      "error : 0.918898, accuarcy : 0.758379\n",
      "total step : 537 \n",
      "error : 0.917792, accuarcy : 0.758379\n",
      "total step : 538 \n",
      "error : 0.916690, accuarcy : 0.758379\n",
      "total step : 539 \n",
      "error : 0.915591, accuarcy : 0.758379\n",
      "total step : 540 \n",
      "error : 0.914496, accuarcy : 0.758379\n",
      "total step : 541 \n",
      "error : 0.913403, accuarcy : 0.758379\n",
      "total step : 542 \n",
      "error : 0.912314, accuarcy : 0.759380\n",
      "total step : 543 \n",
      "error : 0.911228, accuarcy : 0.759380\n",
      "total step : 544 \n",
      "error : 0.910145, accuarcy : 0.759380\n",
      "total step : 545 \n",
      "error : 0.909066, accuarcy : 0.759880\n",
      "total step : 546 \n",
      "error : 0.907989, accuarcy : 0.759880\n",
      "total step : 547 \n",
      "error : 0.906916, accuarcy : 0.759380\n",
      "total step : 548 \n",
      "error : 0.905846, accuarcy : 0.759880\n",
      "total step : 549 \n",
      "error : 0.904779, accuarcy : 0.759880\n",
      "total step : 550 \n",
      "error : 0.903715, accuarcy : 0.760380\n",
      "total step : 551 \n",
      "error : 0.902654, accuarcy : 0.760380\n",
      "total step : 552 \n",
      "error : 0.901596, accuarcy : 0.760380\n",
      "total step : 553 \n",
      "error : 0.900542, accuarcy : 0.760380\n",
      "total step : 554 \n",
      "error : 0.899490, accuarcy : 0.760380\n",
      "total step : 555 \n",
      "error : 0.898442, accuarcy : 0.760380\n",
      "total step : 556 \n",
      "error : 0.897397, accuarcy : 0.760380\n",
      "total step : 557 \n",
      "error : 0.896354, accuarcy : 0.760380\n",
      "total step : 558 \n",
      "error : 0.895315, accuarcy : 0.760380\n",
      "total step : 559 \n",
      "error : 0.894279, accuarcy : 0.760880\n",
      "total step : 560 \n",
      "error : 0.893246, accuarcy : 0.761381\n",
      "total step : 561 \n",
      "error : 0.892216, accuarcy : 0.761381\n",
      "total step : 562 \n",
      "error : 0.891188, accuarcy : 0.761381\n",
      "total step : 563 \n",
      "error : 0.890164, accuarcy : 0.761381\n",
      "total step : 564 \n",
      "error : 0.889143, accuarcy : 0.761381\n",
      "total step : 565 \n",
      "error : 0.888125, accuarcy : 0.761881\n",
      "total step : 566 \n",
      "error : 0.887110, accuarcy : 0.761881\n",
      "total step : 567 \n",
      "error : 0.886097, accuarcy : 0.761881\n",
      "total step : 568 \n",
      "error : 0.885088, accuarcy : 0.761881\n",
      "total step : 569 \n",
      "error : 0.884081, accuarcy : 0.761881\n",
      "total step : 570 \n",
      "error : 0.883078, accuarcy : 0.762381\n",
      "total step : 571 \n",
      "error : 0.882077, accuarcy : 0.763382\n",
      "total step : 572 \n",
      "error : 0.881079, accuarcy : 0.763882\n",
      "total step : 573 \n",
      "error : 0.880085, accuarcy : 0.764882\n",
      "total step : 574 \n",
      "error : 0.879093, accuarcy : 0.764882\n",
      "total step : 575 \n",
      "error : 0.878104, accuarcy : 0.764882\n",
      "total step : 576 \n",
      "error : 0.877117, accuarcy : 0.764882\n",
      "total step : 577 \n",
      "error : 0.876134, accuarcy : 0.765383\n",
      "total step : 578 \n",
      "error : 0.875153, accuarcy : 0.765383\n",
      "total step : 579 \n",
      "error : 0.874176, accuarcy : 0.765383\n",
      "total step : 580 \n",
      "error : 0.873201, accuarcy : 0.765383\n",
      "total step : 581 \n",
      "error : 0.872229, accuarcy : 0.764882\n",
      "total step : 582 \n",
      "error : 0.871259, accuarcy : 0.764882\n",
      "total step : 583 \n",
      "error : 0.870293, accuarcy : 0.764882\n",
      "total step : 584 \n",
      "error : 0.869329, accuarcy : 0.764882\n",
      "total step : 585 \n",
      "error : 0.868368, accuarcy : 0.765383\n",
      "total step : 586 \n",
      "error : 0.867410, accuarcy : 0.765383\n",
      "total step : 587 \n",
      "error : 0.866455, accuarcy : 0.765383\n",
      "total step : 588 \n",
      "error : 0.865502, accuarcy : 0.765883\n",
      "total step : 589 \n",
      "error : 0.864552, accuarcy : 0.765883\n",
      "total step : 590 \n",
      "error : 0.863605, accuarcy : 0.765883\n",
      "total step : 591 \n",
      "error : 0.862660, accuarcy : 0.766883\n",
      "total step : 592 \n",
      "error : 0.861718, accuarcy : 0.767884\n",
      "total step : 593 \n",
      "error : 0.860779, accuarcy : 0.768384\n",
      "total step : 594 \n",
      "error : 0.859843, accuarcy : 0.768884\n",
      "total step : 595 \n",
      "error : 0.858909, accuarcy : 0.769885\n",
      "total step : 596 \n",
      "error : 0.857978, accuarcy : 0.770385\n",
      "total step : 597 \n",
      "error : 0.857050, accuarcy : 0.770385\n",
      "total step : 598 \n",
      "error : 0.856124, accuarcy : 0.770385\n",
      "total step : 599 \n",
      "error : 0.855201, accuarcy : 0.770385\n",
      "total step : 600 \n",
      "error : 0.854280, accuarcy : 0.770385\n",
      "total step : 601 \n",
      "error : 0.853362, accuarcy : 0.770385\n",
      "total step : 602 \n",
      "error : 0.852447, accuarcy : 0.770885\n",
      "total step : 603 \n",
      "error : 0.851534, accuarcy : 0.770385\n",
      "total step : 604 \n",
      "error : 0.850624, accuarcy : 0.770385\n",
      "total step : 605 \n",
      "error : 0.849716, accuarcy : 0.770885\n",
      "total step : 606 \n",
      "error : 0.848811, accuarcy : 0.771386\n",
      "total step : 607 \n",
      "error : 0.847909, accuarcy : 0.771886\n",
      "total step : 608 \n",
      "error : 0.847009, accuarcy : 0.772386\n",
      "total step : 609 \n",
      "error : 0.846112, accuarcy : 0.772886\n",
      "total step : 610 \n",
      "error : 0.845217, accuarcy : 0.773387\n",
      "total step : 611 \n",
      "error : 0.844325, accuarcy : 0.773387\n",
      "total step : 612 \n",
      "error : 0.843435, accuarcy : 0.773387\n",
      "total step : 613 \n",
      "error : 0.842548, accuarcy : 0.773387\n",
      "total step : 614 \n",
      "error : 0.841663, accuarcy : 0.773887\n",
      "total step : 615 \n",
      "error : 0.840781, accuarcy : 0.774387\n",
      "total step : 616 \n",
      "error : 0.839901, accuarcy : 0.774887\n",
      "total step : 617 \n",
      "error : 0.839024, accuarcy : 0.774887\n",
      "total step : 618 \n",
      "error : 0.838149, accuarcy : 0.774887\n",
      "total step : 619 \n",
      "error : 0.837277, accuarcy : 0.774887\n",
      "total step : 620 \n",
      "error : 0.836407, accuarcy : 0.774887\n",
      "total step : 621 \n",
      "error : 0.835539, accuarcy : 0.774887\n",
      "total step : 622 \n",
      "error : 0.834674, accuarcy : 0.774887\n",
      "total step : 623 \n",
      "error : 0.833811, accuarcy : 0.774887\n",
      "total step : 624 \n",
      "error : 0.832951, accuarcy : 0.774887\n",
      "total step : 625 \n",
      "error : 0.832093, accuarcy : 0.774887\n",
      "total step : 626 \n",
      "error : 0.831238, accuarcy : 0.774887\n",
      "total step : 627 \n",
      "error : 0.830385, accuarcy : 0.775388\n",
      "total step : 628 \n",
      "error : 0.829534, accuarcy : 0.775388\n",
      "total step : 629 \n",
      "error : 0.828686, accuarcy : 0.775388\n",
      "total step : 630 \n",
      "error : 0.827840, accuarcy : 0.775888\n",
      "total step : 631 \n",
      "error : 0.826996, accuarcy : 0.775888\n",
      "total step : 632 \n",
      "error : 0.826155, accuarcy : 0.775888\n",
      "total step : 633 \n",
      "error : 0.825316, accuarcy : 0.775888\n",
      "total step : 634 \n",
      "error : 0.824480, accuarcy : 0.775888\n",
      "total step : 635 \n",
      "error : 0.823645, accuarcy : 0.775888\n",
      "total step : 636 \n",
      "error : 0.822813, accuarcy : 0.775888\n",
      "total step : 637 \n",
      "error : 0.821984, accuarcy : 0.777389\n",
      "total step : 638 \n",
      "error : 0.821156, accuarcy : 0.777889\n",
      "total step : 639 \n",
      "error : 0.820331, accuarcy : 0.777889\n",
      "total step : 640 \n",
      "error : 0.819509, accuarcy : 0.778389\n",
      "total step : 641 \n",
      "error : 0.818688, accuarcy : 0.778889\n",
      "total step : 642 \n",
      "error : 0.817870, accuarcy : 0.778889\n",
      "total step : 643 \n",
      "error : 0.817054, accuarcy : 0.778889\n",
      "total step : 644 \n",
      "error : 0.816240, accuarcy : 0.778889\n",
      "total step : 645 \n",
      "error : 0.815428, accuarcy : 0.778889\n",
      "total step : 646 \n",
      "error : 0.814619, accuarcy : 0.778889\n",
      "total step : 647 \n",
      "error : 0.813812, accuarcy : 0.778389\n",
      "total step : 648 \n",
      "error : 0.813007, accuarcy : 0.778389\n",
      "total step : 649 \n",
      "error : 0.812204, accuarcy : 0.778389\n",
      "total step : 650 \n",
      "error : 0.811404, accuarcy : 0.778889\n",
      "total step : 651 \n",
      "error : 0.810605, accuarcy : 0.779890\n",
      "total step : 652 \n",
      "error : 0.809809, accuarcy : 0.780390\n",
      "total step : 653 \n",
      "error : 0.809015, accuarcy : 0.780390\n",
      "total step : 654 \n",
      "error : 0.808223, accuarcy : 0.780390\n",
      "total step : 655 \n",
      "error : 0.807434, accuarcy : 0.780390\n",
      "total step : 656 \n",
      "error : 0.806646, accuarcy : 0.780890\n",
      "total step : 657 \n",
      "error : 0.805861, accuarcy : 0.781391\n",
      "total step : 658 \n",
      "error : 0.805078, accuarcy : 0.781391\n",
      "total step : 659 \n",
      "error : 0.804296, accuarcy : 0.781391\n",
      "total step : 660 \n",
      "error : 0.803517, accuarcy : 0.781391\n",
      "total step : 661 \n",
      "error : 0.802740, accuarcy : 0.781391\n",
      "total step : 662 \n",
      "error : 0.801966, accuarcy : 0.781391\n",
      "total step : 663 \n",
      "error : 0.801193, accuarcy : 0.782391\n",
      "total step : 664 \n",
      "error : 0.800422, accuarcy : 0.782391\n",
      "total step : 665 \n",
      "error : 0.799654, accuarcy : 0.782891\n",
      "total step : 666 \n",
      "error : 0.798887, accuarcy : 0.782891\n",
      "total step : 667 \n",
      "error : 0.798123, accuarcy : 0.782891\n",
      "total step : 668 \n",
      "error : 0.797360, accuarcy : 0.782891\n",
      "total step : 669 \n",
      "error : 0.796600, accuarcy : 0.783892\n",
      "total step : 670 \n",
      "error : 0.795842, accuarcy : 0.783892\n",
      "total step : 671 \n",
      "error : 0.795085, accuarcy : 0.784892\n",
      "total step : 672 \n",
      "error : 0.794331, accuarcy : 0.784892\n",
      "total step : 673 \n",
      "error : 0.793579, accuarcy : 0.784892\n",
      "total step : 674 \n",
      "error : 0.792829, accuarcy : 0.784892\n",
      "total step : 675 \n",
      "error : 0.792080, accuarcy : 0.784892\n",
      "total step : 676 \n",
      "error : 0.791334, accuarcy : 0.784892\n",
      "total step : 677 \n",
      "error : 0.790590, accuarcy : 0.784892\n",
      "total step : 678 \n",
      "error : 0.789848, accuarcy : 0.784892\n",
      "total step : 679 \n",
      "error : 0.789107, accuarcy : 0.784892\n",
      "total step : 680 \n",
      "error : 0.788369, accuarcy : 0.785393\n",
      "total step : 681 \n",
      "error : 0.787633, accuarcy : 0.785393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 682 \n",
      "error : 0.786898, accuarcy : 0.785393\n",
      "total step : 683 \n",
      "error : 0.786166, accuarcy : 0.785393\n",
      "total step : 684 \n",
      "error : 0.785435, accuarcy : 0.785393\n",
      "total step : 685 \n",
      "error : 0.784706, accuarcy : 0.785893\n",
      "total step : 686 \n",
      "error : 0.783980, accuarcy : 0.785893\n",
      "total step : 687 \n",
      "error : 0.783255, accuarcy : 0.785893\n",
      "total step : 688 \n",
      "error : 0.782532, accuarcy : 0.786393\n",
      "total step : 689 \n",
      "error : 0.781811, accuarcy : 0.786393\n",
      "total step : 690 \n",
      "error : 0.781092, accuarcy : 0.786393\n",
      "total step : 691 \n",
      "error : 0.780375, accuarcy : 0.786893\n",
      "total step : 692 \n",
      "error : 0.779659, accuarcy : 0.786893\n",
      "total step : 693 \n",
      "error : 0.778946, accuarcy : 0.787394\n",
      "total step : 694 \n",
      "error : 0.778234, accuarcy : 0.787394\n",
      "total step : 695 \n",
      "error : 0.777524, accuarcy : 0.787394\n",
      "total step : 696 \n",
      "error : 0.776816, accuarcy : 0.787394\n",
      "total step : 697 \n",
      "error : 0.776110, accuarcy : 0.787894\n",
      "total step : 698 \n",
      "error : 0.775406, accuarcy : 0.788394\n",
      "total step : 699 \n",
      "error : 0.774704, accuarcy : 0.788394\n",
      "total step : 700 \n",
      "error : 0.774003, accuarcy : 0.788394\n",
      "total step : 701 \n",
      "error : 0.773305, accuarcy : 0.788394\n",
      "total step : 702 \n",
      "error : 0.772608, accuarcy : 0.788394\n",
      "total step : 703 \n",
      "error : 0.771912, accuarcy : 0.787894\n",
      "total step : 704 \n",
      "error : 0.771219, accuarcy : 0.788394\n",
      "total step : 705 \n",
      "error : 0.770528, accuarcy : 0.788394\n",
      "total step : 706 \n",
      "error : 0.769838, accuarcy : 0.788394\n",
      "total step : 707 \n",
      "error : 0.769150, accuarcy : 0.788394\n",
      "total step : 708 \n",
      "error : 0.768464, accuarcy : 0.788394\n",
      "total step : 709 \n",
      "error : 0.767779, accuarcy : 0.788394\n",
      "total step : 710 \n",
      "error : 0.767096, accuarcy : 0.789395\n",
      "total step : 711 \n",
      "error : 0.766416, accuarcy : 0.789395\n",
      "total step : 712 \n",
      "error : 0.765736, accuarcy : 0.789395\n",
      "total step : 713 \n",
      "error : 0.765059, accuarcy : 0.788894\n",
      "total step : 714 \n",
      "error : 0.764383, accuarcy : 0.788894\n",
      "total step : 715 \n",
      "error : 0.763709, accuarcy : 0.789395\n",
      "total step : 716 \n",
      "error : 0.763037, accuarcy : 0.789395\n",
      "total step : 717 \n",
      "error : 0.762366, accuarcy : 0.789395\n",
      "total step : 718 \n",
      "error : 0.761697, accuarcy : 0.789395\n",
      "total step : 719 \n",
      "error : 0.761030, accuarcy : 0.789395\n",
      "total step : 720 \n",
      "error : 0.760365, accuarcy : 0.789395\n",
      "total step : 721 \n",
      "error : 0.759701, accuarcy : 0.789395\n",
      "total step : 722 \n",
      "error : 0.759039, accuarcy : 0.789895\n",
      "total step : 723 \n",
      "error : 0.758379, accuarcy : 0.789895\n",
      "total step : 724 \n",
      "error : 0.757720, accuarcy : 0.789895\n",
      "total step : 725 \n",
      "error : 0.757063, accuarcy : 0.789895\n",
      "total step : 726 \n",
      "error : 0.756407, accuarcy : 0.789395\n",
      "total step : 727 \n",
      "error : 0.755754, accuarcy : 0.790895\n",
      "total step : 728 \n",
      "error : 0.755101, accuarcy : 0.790895\n",
      "total step : 729 \n",
      "error : 0.754451, accuarcy : 0.790895\n",
      "total step : 730 \n",
      "error : 0.753802, accuarcy : 0.791396\n",
      "total step : 731 \n",
      "error : 0.753155, accuarcy : 0.791396\n",
      "total step : 732 \n",
      "error : 0.752509, accuarcy : 0.791396\n",
      "total step : 733 \n",
      "error : 0.751865, accuarcy : 0.792396\n",
      "total step : 734 \n",
      "error : 0.751223, accuarcy : 0.792896\n",
      "total step : 735 \n",
      "error : 0.750582, accuarcy : 0.793897\n",
      "total step : 736 \n",
      "error : 0.749943, accuarcy : 0.793397\n",
      "total step : 737 \n",
      "error : 0.749306, accuarcy : 0.793397\n",
      "total step : 738 \n",
      "error : 0.748670, accuarcy : 0.793397\n",
      "total step : 739 \n",
      "error : 0.748035, accuarcy : 0.793397\n",
      "total step : 740 \n",
      "error : 0.747402, accuarcy : 0.793397\n",
      "total step : 741 \n",
      "error : 0.746771, accuarcy : 0.793397\n",
      "total step : 742 \n",
      "error : 0.746141, accuarcy : 0.793397\n",
      "total step : 743 \n",
      "error : 0.745513, accuarcy : 0.793397\n",
      "total step : 744 \n",
      "error : 0.744887, accuarcy : 0.793897\n",
      "total step : 745 \n",
      "error : 0.744262, accuarcy : 0.793897\n",
      "total step : 746 \n",
      "error : 0.743638, accuarcy : 0.793897\n",
      "total step : 747 \n",
      "error : 0.743016, accuarcy : 0.794397\n",
      "total step : 748 \n",
      "error : 0.742396, accuarcy : 0.794897\n",
      "total step : 749 \n",
      "error : 0.741777, accuarcy : 0.795398\n",
      "total step : 750 \n",
      "error : 0.741160, accuarcy : 0.795398\n",
      "total step : 751 \n",
      "error : 0.740544, accuarcy : 0.795398\n",
      "total step : 752 \n",
      "error : 0.739930, accuarcy : 0.795898\n",
      "total step : 753 \n",
      "error : 0.739317, accuarcy : 0.795898\n",
      "total step : 754 \n",
      "error : 0.738705, accuarcy : 0.796398\n",
      "total step : 755 \n",
      "error : 0.738096, accuarcy : 0.796398\n",
      "total step : 756 \n",
      "error : 0.737487, accuarcy : 0.796398\n",
      "total step : 757 \n",
      "error : 0.736880, accuarcy : 0.796398\n",
      "total step : 758 \n",
      "error : 0.736275, accuarcy : 0.797399\n",
      "total step : 759 \n",
      "error : 0.735671, accuarcy : 0.797399\n",
      "total step : 760 \n",
      "error : 0.735069, accuarcy : 0.797399\n",
      "total step : 761 \n",
      "error : 0.734468, accuarcy : 0.797899\n",
      "total step : 762 \n",
      "error : 0.733868, accuarcy : 0.797899\n",
      "total step : 763 \n",
      "error : 0.733270, accuarcy : 0.798899\n",
      "total step : 764 \n",
      "error : 0.732674, accuarcy : 0.798899\n",
      "total step : 765 \n",
      "error : 0.732078, accuarcy : 0.798899\n",
      "total step : 766 \n",
      "error : 0.731485, accuarcy : 0.798899\n",
      "total step : 767 \n",
      "error : 0.730892, accuarcy : 0.798899\n",
      "total step : 768 \n",
      "error : 0.730302, accuarcy : 0.798899\n",
      "total step : 769 \n",
      "error : 0.729712, accuarcy : 0.798899\n",
      "total step : 770 \n",
      "error : 0.729124, accuarcy : 0.798899\n",
      "total step : 771 \n",
      "error : 0.728538, accuarcy : 0.799400\n",
      "total step : 772 \n",
      "error : 0.727952, accuarcy : 0.799900\n",
      "total step : 773 \n",
      "error : 0.727369, accuarcy : 0.799900\n",
      "total step : 774 \n",
      "error : 0.726786, accuarcy : 0.800400\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for number 3\n",
      "total step : 1 \n",
      "error : 3.980134, accuarcy : 0.407704\n",
      "total step : 2 \n",
      "error : 3.942180, accuarcy : 0.409705\n",
      "total step : 3 \n",
      "error : 3.904967, accuarcy : 0.410205\n",
      "total step : 4 \n",
      "error : 3.868498, accuarcy : 0.413707\n",
      "total step : 5 \n",
      "error : 3.832773, accuarcy : 0.415708\n",
      "total step : 6 \n",
      "error : 3.797793, accuarcy : 0.418709\n",
      "total step : 7 \n",
      "error : 3.763556, accuarcy : 0.418709\n",
      "total step : 8 \n",
      "error : 3.730060, accuarcy : 0.418209\n",
      "total step : 9 \n",
      "error : 3.697300, accuarcy : 0.420210\n",
      "total step : 10 \n",
      "error : 3.665270, accuarcy : 0.419710\n",
      "total step : 11 \n",
      "error : 3.633963, accuarcy : 0.423712\n",
      "total step : 12 \n",
      "error : 3.603371, accuarcy : 0.424212\n",
      "total step : 13 \n",
      "error : 3.573484, accuarcy : 0.427214\n",
      "total step : 14 \n",
      "error : 3.544291, accuarcy : 0.427714\n",
      "total step : 15 \n",
      "error : 3.515779, accuarcy : 0.429215\n",
      "total step : 16 \n",
      "error : 3.487937, accuarcy : 0.431216\n",
      "total step : 17 \n",
      "error : 3.460749, accuarcy : 0.434217\n",
      "total step : 18 \n",
      "error : 3.434202, accuarcy : 0.433717\n",
      "total step : 19 \n",
      "error : 3.408280, accuarcy : 0.436718\n",
      "total step : 20 \n",
      "error : 3.382968, accuarcy : 0.440220\n",
      "total step : 21 \n",
      "error : 3.358251, accuarcy : 0.440720\n",
      "total step : 22 \n",
      "error : 3.334111, accuarcy : 0.444222\n",
      "total step : 23 \n",
      "error : 3.310534, accuarcy : 0.443722\n",
      "total step : 24 \n",
      "error : 3.287502, accuarcy : 0.444222\n",
      "total step : 25 \n",
      "error : 3.265000, accuarcy : 0.444722\n",
      "total step : 26 \n",
      "error : 3.243011, accuarcy : 0.447224\n",
      "total step : 27 \n",
      "error : 3.221520, accuarcy : 0.450225\n",
      "total step : 28 \n",
      "error : 3.200511, accuarcy : 0.453227\n",
      "total step : 29 \n",
      "error : 3.179969, accuarcy : 0.454227\n",
      "total step : 30 \n",
      "error : 3.159877, accuarcy : 0.454727\n",
      "total step : 31 \n",
      "error : 3.140223, accuarcy : 0.456728\n",
      "total step : 32 \n",
      "error : 3.120990, accuarcy : 0.457229\n",
      "total step : 33 \n",
      "error : 3.102164, accuarcy : 0.457729\n",
      "total step : 34 \n",
      "error : 3.083733, accuarcy : 0.459730\n",
      "total step : 35 \n",
      "error : 3.065681, accuarcy : 0.460730\n",
      "total step : 36 \n",
      "error : 3.047998, accuarcy : 0.461731\n",
      "total step : 37 \n",
      "error : 3.030668, accuarcy : 0.463732\n",
      "total step : 38 \n",
      "error : 3.013682, accuarcy : 0.467234\n",
      "total step : 39 \n",
      "error : 2.997025, accuarcy : 0.468234\n",
      "total step : 40 \n",
      "error : 2.980689, accuarcy : 0.471736\n",
      "total step : 41 \n",
      "error : 2.964660, accuarcy : 0.472236\n",
      "total step : 42 \n",
      "error : 2.948929, accuarcy : 0.474737\n",
      "total step : 43 \n",
      "error : 2.933486, accuarcy : 0.476238\n",
      "total step : 44 \n",
      "error : 2.918320, accuarcy : 0.478739\n",
      "total step : 45 \n",
      "error : 2.903422, accuarcy : 0.480240\n",
      "total step : 46 \n",
      "error : 2.888784, accuarcy : 0.481241\n",
      "total step : 47 \n",
      "error : 2.874396, accuarcy : 0.484242\n",
      "total step : 48 \n",
      "error : 2.860251, accuarcy : 0.486743\n",
      "total step : 49 \n",
      "error : 2.846339, accuarcy : 0.487244\n",
      "total step : 50 \n",
      "error : 2.832654, accuarcy : 0.487744\n",
      "total step : 51 \n",
      "error : 2.819187, accuarcy : 0.488244\n",
      "total step : 52 \n",
      "error : 2.805933, accuarcy : 0.490245\n",
      "total step : 53 \n",
      "error : 2.792883, accuarcy : 0.491246\n",
      "total step : 54 \n",
      "error : 2.780032, accuarcy : 0.492246\n",
      "total step : 55 \n",
      "error : 2.767373, accuarcy : 0.494247\n",
      "total step : 56 \n",
      "error : 2.754900, accuarcy : 0.495248\n",
      "total step : 57 \n",
      "error : 2.742608, accuarcy : 0.495748\n",
      "total step : 58 \n",
      "error : 2.730490, accuarcy : 0.497249\n",
      "total step : 59 \n",
      "error : 2.718541, accuarcy : 0.497249\n",
      "total step : 60 \n",
      "error : 2.706757, accuarcy : 0.496748\n",
      "total step : 61 \n",
      "error : 2.695131, accuarcy : 0.498249\n",
      "total step : 62 \n",
      "error : 2.683661, accuarcy : 0.499750\n",
      "total step : 63 \n",
      "error : 2.672340, accuarcy : 0.499250\n",
      "total step : 64 \n",
      "error : 2.661165, accuarcy : 0.500750\n",
      "total step : 65 \n",
      "error : 2.650131, accuarcy : 0.500250\n",
      "total step : 66 \n",
      "error : 2.639234, accuarcy : 0.502751\n",
      "total step : 67 \n",
      "error : 2.628471, accuarcy : 0.504752\n",
      "total step : 68 \n",
      "error : 2.617837, accuarcy : 0.505753\n",
      "total step : 69 \n",
      "error : 2.607330, accuarcy : 0.507754\n",
      "total step : 70 \n",
      "error : 2.596945, accuarcy : 0.509755\n",
      "total step : 71 \n",
      "error : 2.586679, accuarcy : 0.509755\n",
      "total step : 72 \n",
      "error : 2.576529, accuarcy : 0.511256\n",
      "total step : 73 \n",
      "error : 2.566493, accuarcy : 0.512756\n",
      "total step : 74 \n",
      "error : 2.556566, accuarcy : 0.514757\n",
      "total step : 75 \n",
      "error : 2.546746, accuarcy : 0.515258\n",
      "total step : 76 \n",
      "error : 2.537031, accuarcy : 0.516258\n",
      "total step : 77 \n",
      "error : 2.527418, accuarcy : 0.516758\n",
      "total step : 78 \n",
      "error : 2.517903, accuarcy : 0.519260\n",
      "total step : 79 \n",
      "error : 2.508486, accuarcy : 0.519760\n",
      "total step : 80 \n",
      "error : 2.499163, accuarcy : 0.520760\n",
      "total step : 81 \n",
      "error : 2.489931, accuarcy : 0.524262\n",
      "total step : 82 \n",
      "error : 2.480790, accuarcy : 0.524762\n",
      "total step : 83 \n",
      "error : 2.471737, accuarcy : 0.525763\n",
      "total step : 84 \n",
      "error : 2.462769, accuarcy : 0.527764\n",
      "total step : 85 \n",
      "error : 2.453885, accuarcy : 0.527764\n",
      "total step : 86 \n",
      "error : 2.445083, accuarcy : 0.528264\n",
      "total step : 87 \n",
      "error : 2.436361, accuarcy : 0.530265\n",
      "total step : 88 \n",
      "error : 2.427717, accuarcy : 0.533267\n",
      "total step : 89 \n",
      "error : 2.419150, accuarcy : 0.533267\n",
      "total step : 90 \n",
      "error : 2.410658, accuarcy : 0.533767\n",
      "total step : 91 \n",
      "error : 2.402240, accuarcy : 0.534267\n",
      "total step : 92 \n",
      "error : 2.393893, accuarcy : 0.534767\n",
      "total step : 93 \n",
      "error : 2.385617, accuarcy : 0.535268\n",
      "total step : 94 \n",
      "error : 2.377410, accuarcy : 0.536768\n",
      "total step : 95 \n",
      "error : 2.369270, accuarcy : 0.539270\n",
      "total step : 96 \n",
      "error : 2.361198, accuarcy : 0.540270\n",
      "total step : 97 \n",
      "error : 2.353190, accuarcy : 0.542771\n",
      "total step : 98 \n",
      "error : 2.345246, accuarcy : 0.544272\n",
      "total step : 99 \n",
      "error : 2.337365, accuarcy : 0.545273\n",
      "total step : 100 \n",
      "error : 2.329545, accuarcy : 0.545773\n",
      "total step : 101 \n",
      "error : 2.321786, accuarcy : 0.547774\n",
      "total step : 102 \n",
      "error : 2.314087, accuarcy : 0.549275\n",
      "total step : 103 \n",
      "error : 2.306446, accuarcy : 0.551276\n",
      "total step : 104 \n",
      "error : 2.298862, accuarcy : 0.552776\n",
      "total step : 105 \n",
      "error : 2.291335, accuarcy : 0.554777\n",
      "total step : 106 \n",
      "error : 2.283863, accuarcy : 0.556278\n",
      "total step : 107 \n",
      "error : 2.276447, accuarcy : 0.558279\n",
      "total step : 108 \n",
      "error : 2.269084, accuarcy : 0.558779\n",
      "total step : 109 \n",
      "error : 2.261774, accuarcy : 0.559780\n",
      "total step : 110 \n",
      "error : 2.254516, accuarcy : 0.560780\n",
      "total step : 111 \n",
      "error : 2.247309, accuarcy : 0.561781\n",
      "total step : 112 \n",
      "error : 2.240154, accuarcy : 0.561781\n",
      "total step : 113 \n",
      "error : 2.233048, accuarcy : 0.563782\n",
      "total step : 114 \n",
      "error : 2.225991, accuarcy : 0.563782\n",
      "total step : 115 \n",
      "error : 2.218983, accuarcy : 0.563782\n",
      "total step : 116 \n",
      "error : 2.212023, accuarcy : 0.563782\n",
      "total step : 117 \n",
      "error : 2.205109, accuarcy : 0.565283\n",
      "total step : 118 \n",
      "error : 2.198243, accuarcy : 0.566283\n",
      "total step : 119 \n",
      "error : 2.191422, accuarcy : 0.567784\n",
      "total step : 120 \n",
      "error : 2.184647, accuarcy : 0.568784\n",
      "total step : 121 \n",
      "error : 2.177916, accuarcy : 0.568284\n",
      "total step : 122 \n",
      "error : 2.171230, accuarcy : 0.570285\n",
      "total step : 123 \n",
      "error : 2.164588, accuarcy : 0.570785\n",
      "total step : 124 \n",
      "error : 2.157988, accuarcy : 0.572286\n",
      "total step : 125 \n",
      "error : 2.151431, accuarcy : 0.572786\n",
      "total step : 126 \n",
      "error : 2.144917, accuarcy : 0.574287\n",
      "total step : 127 \n",
      "error : 2.138443, accuarcy : 0.576288\n",
      "total step : 128 \n",
      "error : 2.132012, accuarcy : 0.576788\n",
      "total step : 129 \n",
      "error : 2.125620, accuarcy : 0.577289\n",
      "total step : 130 \n",
      "error : 2.119269, accuarcy : 0.579290\n",
      "total step : 131 \n",
      "error : 2.112958, accuarcy : 0.580290\n",
      "total step : 132 \n",
      "error : 2.106686, accuarcy : 0.581291\n",
      "total step : 133 \n",
      "error : 2.100454, accuarcy : 0.582791\n",
      "total step : 134 \n",
      "error : 2.094259, accuarcy : 0.583792\n",
      "total step : 135 \n",
      "error : 2.088103, accuarcy : 0.583792\n",
      "total step : 136 \n",
      "error : 2.081985, accuarcy : 0.584292\n",
      "total step : 137 \n",
      "error : 2.075904, accuarcy : 0.585293\n",
      "total step : 138 \n",
      "error : 2.069860, accuarcy : 0.585293\n",
      "total step : 139 \n",
      "error : 2.063852, accuarcy : 0.586293\n",
      "total step : 140 \n",
      "error : 2.057881, accuarcy : 0.586793\n",
      "total step : 141 \n",
      "error : 2.051946, accuarcy : 0.586793\n",
      "total step : 142 \n",
      "error : 2.046046, accuarcy : 0.586793\n",
      "total step : 143 \n",
      "error : 2.040182, accuarcy : 0.587794\n",
      "total step : 144 \n",
      "error : 2.034353, accuarcy : 0.588294\n",
      "total step : 145 \n",
      "error : 2.028558, accuarcy : 0.588294\n",
      "total step : 146 \n",
      "error : 2.022797, accuarcy : 0.589295\n",
      "total step : 147 \n",
      "error : 2.017071, accuarcy : 0.590795\n",
      "total step : 148 \n",
      "error : 2.011378, accuarcy : 0.592296\n",
      "total step : 149 \n",
      "error : 2.005718, accuarcy : 0.593797\n",
      "total step : 150 \n",
      "error : 2.000092, accuarcy : 0.593297\n",
      "total step : 151 \n",
      "error : 1.994498, accuarcy : 0.595298\n",
      "total step : 152 \n",
      "error : 1.988937, accuarcy : 0.595298\n",
      "total step : 153 \n",
      "error : 1.983408, accuarcy : 0.596298\n",
      "total step : 154 \n",
      "error : 1.977911, accuarcy : 0.597299\n",
      "total step : 155 \n",
      "error : 1.972446, accuarcy : 0.597299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 156 \n",
      "error : 1.967012, accuarcy : 0.598299\n",
      "total step : 157 \n",
      "error : 1.961610, accuarcy : 0.599300\n",
      "total step : 158 \n",
      "error : 1.956238, accuarcy : 0.600800\n",
      "total step : 159 \n",
      "error : 1.950897, accuarcy : 0.600800\n",
      "total step : 160 \n",
      "error : 1.945586, accuarcy : 0.600800\n",
      "total step : 161 \n",
      "error : 1.940306, accuarcy : 0.600800\n",
      "total step : 162 \n",
      "error : 1.935055, accuarcy : 0.601301\n",
      "total step : 163 \n",
      "error : 1.929835, accuarcy : 0.602801\n",
      "total step : 164 \n",
      "error : 1.924643, accuarcy : 0.603302\n",
      "total step : 165 \n",
      "error : 1.919481, accuarcy : 0.603302\n",
      "total step : 166 \n",
      "error : 1.914348, accuarcy : 0.603802\n",
      "total step : 167 \n",
      "error : 1.909244, accuarcy : 0.604802\n",
      "total step : 168 \n",
      "error : 1.904168, accuarcy : 0.606303\n",
      "total step : 169 \n",
      "error : 1.899120, accuarcy : 0.607804\n",
      "total step : 170 \n",
      "error : 1.894101, accuarcy : 0.608804\n",
      "total step : 171 \n",
      "error : 1.889110, accuarcy : 0.609805\n",
      "total step : 172 \n",
      "error : 1.884146, accuarcy : 0.609305\n",
      "total step : 173 \n",
      "error : 1.879210, accuarcy : 0.609805\n",
      "total step : 174 \n",
      "error : 1.874301, accuarcy : 0.612306\n",
      "total step : 175 \n",
      "error : 1.869419, accuarcy : 0.611806\n",
      "total step : 176 \n",
      "error : 1.864564, accuarcy : 0.613307\n",
      "total step : 177 \n",
      "error : 1.859735, accuarcy : 0.614307\n",
      "total step : 178 \n",
      "error : 1.854934, accuarcy : 0.614307\n",
      "total step : 179 \n",
      "error : 1.850158, accuarcy : 0.614807\n",
      "total step : 180 \n",
      "error : 1.845409, accuarcy : 0.616808\n",
      "total step : 181 \n",
      "error : 1.840685, accuarcy : 0.617809\n",
      "total step : 182 \n",
      "error : 1.835987, accuarcy : 0.617809\n",
      "total step : 183 \n",
      "error : 1.831315, accuarcy : 0.618309\n",
      "total step : 184 \n",
      "error : 1.826668, accuarcy : 0.619810\n",
      "total step : 185 \n",
      "error : 1.822047, accuarcy : 0.620310\n",
      "total step : 186 \n",
      "error : 1.817450, accuarcy : 0.621311\n",
      "total step : 187 \n",
      "error : 1.812878, accuarcy : 0.621811\n",
      "total step : 188 \n",
      "error : 1.808331, accuarcy : 0.621311\n",
      "total step : 189 \n",
      "error : 1.803808, accuarcy : 0.621311\n",
      "total step : 190 \n",
      "error : 1.799310, accuarcy : 0.622311\n",
      "total step : 191 \n",
      "error : 1.794836, accuarcy : 0.622311\n",
      "total step : 192 \n",
      "error : 1.790385, accuarcy : 0.622811\n",
      "total step : 193 \n",
      "error : 1.785959, accuarcy : 0.623812\n",
      "total step : 194 \n",
      "error : 1.781556, accuarcy : 0.623812\n",
      "total step : 195 \n",
      "error : 1.777177, accuarcy : 0.623812\n",
      "total step : 196 \n",
      "error : 1.772821, accuarcy : 0.623812\n",
      "total step : 197 \n",
      "error : 1.768488, accuarcy : 0.623812\n",
      "total step : 198 \n",
      "error : 1.764179, accuarcy : 0.623812\n",
      "total step : 199 \n",
      "error : 1.759892, accuarcy : 0.624812\n",
      "total step : 200 \n",
      "error : 1.755628, accuarcy : 0.625313\n",
      "total step : 201 \n",
      "error : 1.751386, accuarcy : 0.625813\n",
      "total step : 202 \n",
      "error : 1.747167, accuarcy : 0.626313\n",
      "total step : 203 \n",
      "error : 1.742970, accuarcy : 0.627314\n",
      "total step : 204 \n",
      "error : 1.738795, accuarcy : 0.627814\n",
      "total step : 205 \n",
      "error : 1.734643, accuarcy : 0.628814\n",
      "total step : 206 \n",
      "error : 1.730512, accuarcy : 0.629815\n",
      "total step : 207 \n",
      "error : 1.726403, accuarcy : 0.630315\n",
      "total step : 208 \n",
      "error : 1.722315, accuarcy : 0.630315\n",
      "total step : 209 \n",
      "error : 1.718249, accuarcy : 0.630315\n",
      "total step : 210 \n",
      "error : 1.714204, accuarcy : 0.630815\n",
      "total step : 211 \n",
      "error : 1.710180, accuarcy : 0.631316\n",
      "total step : 212 \n",
      "error : 1.706177, accuarcy : 0.630315\n",
      "total step : 213 \n",
      "error : 1.702195, accuarcy : 0.632316\n",
      "total step : 214 \n",
      "error : 1.698234, accuarcy : 0.632316\n",
      "total step : 215 \n",
      "error : 1.694294, accuarcy : 0.632816\n",
      "total step : 216 \n",
      "error : 1.690374, accuarcy : 0.634317\n",
      "total step : 217 \n",
      "error : 1.686474, accuarcy : 0.634317\n",
      "total step : 218 \n",
      "error : 1.682595, accuarcy : 0.634317\n",
      "total step : 219 \n",
      "error : 1.678736, accuarcy : 0.634817\n",
      "total step : 220 \n",
      "error : 1.674897, accuarcy : 0.634817\n",
      "total step : 221 \n",
      "error : 1.671077, accuarcy : 0.635318\n",
      "total step : 222 \n",
      "error : 1.667278, accuarcy : 0.635318\n",
      "total step : 223 \n",
      "error : 1.663498, accuarcy : 0.636818\n",
      "total step : 224 \n",
      "error : 1.659737, accuarcy : 0.638319\n",
      "total step : 225 \n",
      "error : 1.655996, accuarcy : 0.639320\n",
      "total step : 226 \n",
      "error : 1.652274, accuarcy : 0.640820\n",
      "total step : 227 \n",
      "error : 1.648572, accuarcy : 0.640820\n",
      "total step : 228 \n",
      "error : 1.644888, accuarcy : 0.641321\n",
      "total step : 229 \n",
      "error : 1.641223, accuarcy : 0.641821\n",
      "total step : 230 \n",
      "error : 1.637577, accuarcy : 0.642321\n",
      "total step : 231 \n",
      "error : 1.633950, accuarcy : 0.642321\n",
      "total step : 232 \n",
      "error : 1.630342, accuarcy : 0.642821\n",
      "total step : 233 \n",
      "error : 1.626752, accuarcy : 0.642821\n",
      "total step : 234 \n",
      "error : 1.623180, accuarcy : 0.643322\n",
      "total step : 235 \n",
      "error : 1.619626, accuarcy : 0.643822\n",
      "total step : 236 \n",
      "error : 1.616091, accuarcy : 0.643822\n",
      "total step : 237 \n",
      "error : 1.612574, accuarcy : 0.645323\n",
      "total step : 238 \n",
      "error : 1.609074, accuarcy : 0.646823\n",
      "total step : 239 \n",
      "error : 1.605593, accuarcy : 0.647824\n",
      "total step : 240 \n",
      "error : 1.602129, accuarcy : 0.647824\n",
      "total step : 241 \n",
      "error : 1.598683, accuarcy : 0.647824\n",
      "total step : 242 \n",
      "error : 1.595254, accuarcy : 0.648324\n",
      "total step : 243 \n",
      "error : 1.591843, accuarcy : 0.648824\n",
      "total step : 244 \n",
      "error : 1.588449, accuarcy : 0.648824\n",
      "total step : 245 \n",
      "error : 1.585072, accuarcy : 0.649325\n",
      "total step : 246 \n",
      "error : 1.581713, accuarcy : 0.649825\n",
      "total step : 247 \n",
      "error : 1.578370, accuarcy : 0.650325\n",
      "total step : 248 \n",
      "error : 1.575045, accuarcy : 0.650325\n",
      "total step : 249 \n",
      "error : 1.571736, accuarcy : 0.652326\n",
      "total step : 250 \n",
      "error : 1.568444, accuarcy : 0.652826\n",
      "total step : 251 \n",
      "error : 1.565168, accuarcy : 0.653327\n",
      "total step : 252 \n",
      "error : 1.561909, accuarcy : 0.653827\n",
      "total step : 253 \n",
      "error : 1.558666, accuarcy : 0.653827\n",
      "total step : 254 \n",
      "error : 1.555440, accuarcy : 0.653827\n",
      "total step : 255 \n",
      "error : 1.552230, accuarcy : 0.655328\n",
      "total step : 256 \n",
      "error : 1.549036, accuarcy : 0.656828\n",
      "total step : 257 \n",
      "error : 1.545858, accuarcy : 0.657329\n",
      "total step : 258 \n",
      "error : 1.542696, accuarcy : 0.657329\n",
      "total step : 259 \n",
      "error : 1.539550, accuarcy : 0.657829\n",
      "total step : 260 \n",
      "error : 1.536420, accuarcy : 0.658829\n",
      "total step : 261 \n",
      "error : 1.533305, accuarcy : 0.659330\n",
      "total step : 262 \n",
      "error : 1.530206, accuarcy : 0.659330\n",
      "total step : 263 \n",
      "error : 1.527123, accuarcy : 0.659830\n",
      "total step : 264 \n",
      "error : 1.524054, accuarcy : 0.659830\n",
      "total step : 265 \n",
      "error : 1.521002, accuarcy : 0.660830\n",
      "total step : 266 \n",
      "error : 1.517964, accuarcy : 0.661331\n",
      "total step : 267 \n",
      "error : 1.514941, accuarcy : 0.661331\n",
      "total step : 268 \n",
      "error : 1.511934, accuarcy : 0.661831\n",
      "total step : 269 \n",
      "error : 1.508941, accuarcy : 0.662331\n",
      "total step : 270 \n",
      "error : 1.505964, accuarcy : 0.662331\n",
      "total step : 271 \n",
      "error : 1.503001, accuarcy : 0.662331\n",
      "total step : 272 \n",
      "error : 1.500052, accuarcy : 0.662831\n",
      "total step : 273 \n",
      "error : 1.497119, accuarcy : 0.663332\n",
      "total step : 274 \n",
      "error : 1.494200, accuarcy : 0.666333\n",
      "total step : 275 \n",
      "error : 1.491295, accuarcy : 0.666833\n",
      "total step : 276 \n",
      "error : 1.488405, accuarcy : 0.666833\n",
      "total step : 277 \n",
      "error : 1.485529, accuarcy : 0.667334\n",
      "total step : 278 \n",
      "error : 1.482667, accuarcy : 0.668834\n",
      "total step : 279 \n",
      "error : 1.479819, accuarcy : 0.669835\n",
      "total step : 280 \n",
      "error : 1.476985, accuarcy : 0.669835\n",
      "total step : 281 \n",
      "error : 1.474165, accuarcy : 0.670335\n",
      "total step : 282 \n",
      "error : 1.471359, accuarcy : 0.670335\n",
      "total step : 283 \n",
      "error : 1.468567, accuarcy : 0.670835\n",
      "total step : 284 \n",
      "error : 1.465788, accuarcy : 0.671836\n",
      "total step : 285 \n",
      "error : 1.463024, accuarcy : 0.672336\n",
      "total step : 286 \n",
      "error : 1.460272, accuarcy : 0.672836\n",
      "total step : 287 \n",
      "error : 1.457534, accuarcy : 0.673337\n",
      "total step : 288 \n",
      "error : 1.454810, accuarcy : 0.673837\n",
      "total step : 289 \n",
      "error : 1.452098, accuarcy : 0.674337\n",
      "total step : 290 \n",
      "error : 1.449400, accuarcy : 0.675338\n",
      "total step : 291 \n",
      "error : 1.446715, accuarcy : 0.675838\n",
      "total step : 292 \n",
      "error : 1.444043, accuarcy : 0.675838\n",
      "total step : 293 \n",
      "error : 1.441384, accuarcy : 0.675838\n",
      "total step : 294 \n",
      "error : 1.438738, accuarcy : 0.675838\n",
      "total step : 295 \n",
      "error : 1.436105, accuarcy : 0.675838\n",
      "total step : 296 \n",
      "error : 1.433484, accuarcy : 0.676338\n",
      "total step : 297 \n",
      "error : 1.430877, accuarcy : 0.676838\n",
      "total step : 298 \n",
      "error : 1.428281, accuarcy : 0.676838\n",
      "total step : 299 \n",
      "error : 1.425699, accuarcy : 0.677339\n",
      "total step : 300 \n",
      "error : 1.423129, accuarcy : 0.677339\n",
      "total step : 301 \n",
      "error : 1.420571, accuarcy : 0.677839\n",
      "total step : 302 \n",
      "error : 1.418025, accuarcy : 0.678839\n",
      "total step : 303 \n",
      "error : 1.415492, accuarcy : 0.678839\n",
      "total step : 304 \n",
      "error : 1.412971, accuarcy : 0.678839\n",
      "total step : 305 \n",
      "error : 1.410462, accuarcy : 0.680340\n",
      "total step : 306 \n",
      "error : 1.407965, accuarcy : 0.681341\n",
      "total step : 307 \n",
      "error : 1.405479, accuarcy : 0.681841\n",
      "total step : 308 \n",
      "error : 1.403006, accuarcy : 0.683842\n",
      "total step : 309 \n",
      "error : 1.400545, accuarcy : 0.684342\n",
      "total step : 310 \n",
      "error : 1.398095, accuarcy : 0.685343\n",
      "total step : 311 \n",
      "error : 1.395657, accuarcy : 0.685343\n",
      "total step : 312 \n",
      "error : 1.393231, accuarcy : 0.686343\n",
      "total step : 313 \n",
      "error : 1.390816, accuarcy : 0.686343\n",
      "total step : 314 \n",
      "error : 1.388412, accuarcy : 0.686343\n",
      "total step : 315 \n",
      "error : 1.386020, accuarcy : 0.685843\n",
      "total step : 316 \n",
      "error : 1.383640, accuarcy : 0.685843\n",
      "total step : 317 \n",
      "error : 1.381270, accuarcy : 0.685843\n",
      "total step : 318 \n",
      "error : 1.378912, accuarcy : 0.686843\n",
      "total step : 319 \n",
      "error : 1.376565, accuarcy : 0.686843\n",
      "total step : 320 \n",
      "error : 1.374229, accuarcy : 0.686843\n",
      "total step : 321 \n",
      "error : 1.371904, accuarcy : 0.687344\n",
      "total step : 322 \n",
      "error : 1.369590, accuarcy : 0.687844\n",
      "total step : 323 \n",
      "error : 1.367287, accuarcy : 0.688844\n",
      "total step : 324 \n",
      "error : 1.364994, accuarcy : 0.690345\n",
      "total step : 325 \n",
      "error : 1.362712, accuarcy : 0.689845\n",
      "total step : 326 \n",
      "error : 1.360441, accuarcy : 0.690845\n",
      "total step : 327 \n",
      "error : 1.358181, accuarcy : 0.691846\n",
      "total step : 328 \n",
      "error : 1.355931, accuarcy : 0.692346\n",
      "total step : 329 \n",
      "error : 1.353692, accuarcy : 0.692346\n",
      "total step : 330 \n",
      "error : 1.351463, accuarcy : 0.692346\n",
      "total step : 331 \n",
      "error : 1.349245, accuarcy : 0.692846\n",
      "total step : 332 \n",
      "error : 1.347037, accuarcy : 0.693847\n",
      "total step : 333 \n",
      "error : 1.344839, accuarcy : 0.693847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 334 \n",
      "error : 1.342651, accuarcy : 0.694847\n",
      "total step : 335 \n",
      "error : 1.340474, accuarcy : 0.694847\n",
      "total step : 336 \n",
      "error : 1.338306, accuarcy : 0.695348\n",
      "total step : 337 \n",
      "error : 1.336149, accuarcy : 0.695848\n",
      "total step : 338 \n",
      "error : 1.334001, accuarcy : 0.696348\n",
      "total step : 339 \n",
      "error : 1.331864, accuarcy : 0.696848\n",
      "total step : 340 \n",
      "error : 1.329736, accuarcy : 0.696848\n",
      "total step : 341 \n",
      "error : 1.327618, accuarcy : 0.696848\n",
      "total step : 342 \n",
      "error : 1.325510, accuarcy : 0.697849\n",
      "total step : 343 \n",
      "error : 1.323412, accuarcy : 0.698849\n",
      "total step : 344 \n",
      "error : 1.321323, accuarcy : 0.698849\n",
      "total step : 345 \n",
      "error : 1.319244, accuarcy : 0.699350\n",
      "total step : 346 \n",
      "error : 1.317174, accuarcy : 0.699850\n",
      "total step : 347 \n",
      "error : 1.315114, accuarcy : 0.700350\n",
      "total step : 348 \n",
      "error : 1.313063, accuarcy : 0.700350\n",
      "total step : 349 \n",
      "error : 1.311022, accuarcy : 0.699850\n",
      "total step : 350 \n",
      "error : 1.308990, accuarcy : 0.699850\n",
      "total step : 351 \n",
      "error : 1.306967, accuarcy : 0.700350\n",
      "total step : 352 \n",
      "error : 1.304954, accuarcy : 0.700850\n",
      "total step : 353 \n",
      "error : 1.302949, accuarcy : 0.701851\n",
      "total step : 354 \n",
      "error : 1.300954, accuarcy : 0.703352\n",
      "total step : 355 \n",
      "error : 1.298967, accuarcy : 0.703352\n",
      "total step : 356 \n",
      "error : 1.296990, accuarcy : 0.703852\n",
      "total step : 357 \n",
      "error : 1.295022, accuarcy : 0.703852\n",
      "total step : 358 \n",
      "error : 1.293062, accuarcy : 0.704852\n",
      "total step : 359 \n",
      "error : 1.291112, accuarcy : 0.704852\n",
      "total step : 360 \n",
      "error : 1.289170, accuarcy : 0.705353\n",
      "total step : 361 \n",
      "error : 1.287237, accuarcy : 0.705353\n",
      "total step : 362 \n",
      "error : 1.285313, accuarcy : 0.705353\n",
      "total step : 363 \n",
      "error : 1.283397, accuarcy : 0.705853\n",
      "total step : 364 \n",
      "error : 1.281490, accuarcy : 0.706353\n",
      "total step : 365 \n",
      "error : 1.279591, accuarcy : 0.707354\n",
      "total step : 366 \n",
      "error : 1.277701, accuarcy : 0.708354\n",
      "total step : 367 \n",
      "error : 1.275820, accuarcy : 0.708854\n",
      "total step : 368 \n",
      "error : 1.273946, accuarcy : 0.708854\n",
      "total step : 369 \n",
      "error : 1.272082, accuarcy : 0.708854\n",
      "total step : 370 \n",
      "error : 1.270225, accuarcy : 0.708854\n",
      "total step : 371 \n",
      "error : 1.268377, accuarcy : 0.709355\n",
      "total step : 372 \n",
      "error : 1.266537, accuarcy : 0.709355\n",
      "total step : 373 \n",
      "error : 1.264705, accuarcy : 0.709355\n",
      "total step : 374 \n",
      "error : 1.262882, accuarcy : 0.709355\n",
      "total step : 375 \n",
      "error : 1.261066, accuarcy : 0.709855\n",
      "total step : 376 \n",
      "error : 1.259259, accuarcy : 0.709355\n",
      "total step : 377 \n",
      "error : 1.257459, accuarcy : 0.709355\n",
      "total step : 378 \n",
      "error : 1.255667, accuarcy : 0.709855\n",
      "total step : 379 \n",
      "error : 1.253884, accuarcy : 0.709855\n",
      "total step : 380 \n",
      "error : 1.252108, accuarcy : 0.709855\n",
      "total step : 381 \n",
      "error : 1.250340, accuarcy : 0.709855\n",
      "total step : 382 \n",
      "error : 1.248580, accuarcy : 0.711356\n",
      "total step : 383 \n",
      "error : 1.246827, accuarcy : 0.711356\n",
      "total step : 384 \n",
      "error : 1.245082, accuarcy : 0.712356\n",
      "total step : 385 \n",
      "error : 1.243345, accuarcy : 0.712356\n",
      "total step : 386 \n",
      "error : 1.241616, accuarcy : 0.713357\n",
      "total step : 387 \n",
      "error : 1.239894, accuarcy : 0.713857\n",
      "total step : 388 \n",
      "error : 1.238179, accuarcy : 0.715358\n",
      "total step : 389 \n",
      "error : 1.236472, accuarcy : 0.715858\n",
      "total step : 390 \n",
      "error : 1.234773, accuarcy : 0.716358\n",
      "total step : 391 \n",
      "error : 1.233080, accuarcy : 0.716358\n",
      "total step : 392 \n",
      "error : 1.231396, accuarcy : 0.716358\n",
      "total step : 393 \n",
      "error : 1.229718, accuarcy : 0.716358\n",
      "total step : 394 \n",
      "error : 1.228048, accuarcy : 0.716858\n",
      "total step : 395 \n",
      "error : 1.226385, accuarcy : 0.716858\n",
      "total step : 396 \n",
      "error : 1.224729, accuarcy : 0.717359\n",
      "total step : 397 \n",
      "error : 1.223080, accuarcy : 0.717859\n",
      "total step : 398 \n",
      "error : 1.221439, accuarcy : 0.717859\n",
      "total step : 399 \n",
      "error : 1.219804, accuarcy : 0.717859\n",
      "total step : 400 \n",
      "error : 1.218176, accuarcy : 0.718359\n",
      "total step : 401 \n",
      "error : 1.216556, accuarcy : 0.718859\n",
      "total step : 402 \n",
      "error : 1.214942, accuarcy : 0.718859\n",
      "total step : 403 \n",
      "error : 1.213336, accuarcy : 0.718859\n",
      "total step : 404 \n",
      "error : 1.211736, accuarcy : 0.718859\n",
      "total step : 405 \n",
      "error : 1.210143, accuarcy : 0.719860\n",
      "total step : 406 \n",
      "error : 1.208556, accuarcy : 0.719860\n",
      "total step : 407 \n",
      "error : 1.206977, accuarcy : 0.720360\n",
      "total step : 408 \n",
      "error : 1.205404, accuarcy : 0.720860\n",
      "total step : 409 \n",
      "error : 1.203838, accuarcy : 0.720860\n",
      "total step : 410 \n",
      "error : 1.202279, accuarcy : 0.721361\n",
      "total step : 411 \n",
      "error : 1.200726, accuarcy : 0.721861\n",
      "total step : 412 \n",
      "error : 1.199179, accuarcy : 0.722361\n",
      "total step : 413 \n",
      "error : 1.197640, accuarcy : 0.722361\n",
      "total step : 414 \n",
      "error : 1.196106, accuarcy : 0.722861\n",
      "total step : 415 \n",
      "error : 1.194579, accuarcy : 0.722861\n",
      "total step : 416 \n",
      "error : 1.193059, accuarcy : 0.722861\n",
      "total step : 417 \n",
      "error : 1.191545, accuarcy : 0.723362\n",
      "total step : 418 \n",
      "error : 1.190037, accuarcy : 0.723362\n",
      "total step : 419 \n",
      "error : 1.188536, accuarcy : 0.723362\n",
      "total step : 420 \n",
      "error : 1.187040, accuarcy : 0.723862\n",
      "total step : 421 \n",
      "error : 1.185552, accuarcy : 0.724862\n",
      "total step : 422 \n",
      "error : 1.184069, accuarcy : 0.726863\n",
      "total step : 423 \n",
      "error : 1.182592, accuarcy : 0.727864\n",
      "total step : 424 \n",
      "error : 1.181122, accuarcy : 0.727864\n",
      "total step : 425 \n",
      "error : 1.179657, accuarcy : 0.728864\n",
      "total step : 426 \n",
      "error : 1.178199, accuarcy : 0.729365\n",
      "total step : 427 \n",
      "error : 1.176747, accuarcy : 0.729865\n",
      "total step : 428 \n",
      "error : 1.175300, accuarcy : 0.730365\n",
      "total step : 429 \n",
      "error : 1.173860, accuarcy : 0.730365\n",
      "total step : 430 \n",
      "error : 1.172425, accuarcy : 0.730865\n",
      "total step : 431 \n",
      "error : 1.170997, accuarcy : 0.731366\n",
      "total step : 432 \n",
      "error : 1.169574, accuarcy : 0.732366\n",
      "total step : 433 \n",
      "error : 1.168157, accuarcy : 0.732866\n",
      "total step : 434 \n",
      "error : 1.166746, accuarcy : 0.733367\n",
      "total step : 435 \n",
      "error : 1.165341, accuarcy : 0.733367\n",
      "total step : 436 \n",
      "error : 1.163941, accuarcy : 0.733367\n",
      "total step : 437 \n",
      "error : 1.162547, accuarcy : 0.733367\n",
      "total step : 438 \n",
      "error : 1.161158, accuarcy : 0.733867\n",
      "total step : 439 \n",
      "error : 1.159776, accuarcy : 0.733867\n",
      "total step : 440 \n",
      "error : 1.158399, accuarcy : 0.733867\n",
      "total step : 441 \n",
      "error : 1.157027, accuarcy : 0.734367\n",
      "total step : 442 \n",
      "error : 1.155661, accuarcy : 0.734367\n",
      "total step : 443 \n",
      "error : 1.154300, accuarcy : 0.734367\n",
      "total step : 444 \n",
      "error : 1.152945, accuarcy : 0.734867\n",
      "total step : 445 \n",
      "error : 1.151595, accuarcy : 0.735368\n",
      "total step : 446 \n",
      "error : 1.150251, accuarcy : 0.735868\n",
      "total step : 447 \n",
      "error : 1.148912, accuarcy : 0.736368\n",
      "total step : 448 \n",
      "error : 1.147579, accuarcy : 0.736368\n",
      "total step : 449 \n",
      "error : 1.146250, accuarcy : 0.736868\n",
      "total step : 450 \n",
      "error : 1.144927, accuarcy : 0.736868\n",
      "total step : 451 \n",
      "error : 1.143610, accuarcy : 0.736868\n",
      "total step : 452 \n",
      "error : 1.142297, accuarcy : 0.737369\n",
      "total step : 453 \n",
      "error : 1.140990, accuarcy : 0.737369\n",
      "total step : 454 \n",
      "error : 1.139687, accuarcy : 0.737369\n",
      "total step : 455 \n",
      "error : 1.138390, accuarcy : 0.738369\n",
      "total step : 456 \n",
      "error : 1.137098, accuarcy : 0.738869\n",
      "total step : 457 \n",
      "error : 1.135811, accuarcy : 0.739370\n",
      "total step : 458 \n",
      "error : 1.134529, accuarcy : 0.739370\n",
      "total step : 459 \n",
      "error : 1.133252, accuarcy : 0.739370\n",
      "total step : 460 \n",
      "error : 1.131981, accuarcy : 0.739870\n",
      "total step : 461 \n",
      "error : 1.130714, accuarcy : 0.740370\n",
      "total step : 462 \n",
      "error : 1.129452, accuarcy : 0.740370\n",
      "total step : 463 \n",
      "error : 1.128194, accuarcy : 0.740370\n",
      "total step : 464 \n",
      "error : 1.126942, accuarcy : 0.741371\n",
      "total step : 465 \n",
      "error : 1.125695, accuarcy : 0.741371\n",
      "total step : 466 \n",
      "error : 1.124452, accuarcy : 0.741871\n",
      "total step : 467 \n",
      "error : 1.123214, accuarcy : 0.741871\n",
      "total step : 468 \n",
      "error : 1.121981, accuarcy : 0.742371\n",
      "total step : 469 \n",
      "error : 1.120753, accuarcy : 0.742871\n",
      "total step : 470 \n",
      "error : 1.119529, accuarcy : 0.742871\n",
      "total step : 471 \n",
      "error : 1.118310, accuarcy : 0.742871\n",
      "total step : 472 \n",
      "error : 1.117096, accuarcy : 0.742871\n",
      "total step : 473 \n",
      "error : 1.115886, accuarcy : 0.743872\n",
      "total step : 474 \n",
      "error : 1.114681, accuarcy : 0.744872\n",
      "total step : 475 \n",
      "error : 1.113481, accuarcy : 0.745373\n",
      "total step : 476 \n",
      "error : 1.112285, accuarcy : 0.745373\n",
      "total step : 477 \n",
      "error : 1.111093, accuarcy : 0.745873\n",
      "total step : 478 \n",
      "error : 1.109906, accuarcy : 0.746373\n",
      "total step : 479 \n",
      "error : 1.108724, accuarcy : 0.746873\n",
      "total step : 480 \n",
      "error : 1.107546, accuarcy : 0.746873\n",
      "total step : 481 \n",
      "error : 1.106372, accuarcy : 0.746873\n",
      "total step : 482 \n",
      "error : 1.105203, accuarcy : 0.747874\n",
      "total step : 483 \n",
      "error : 1.104038, accuarcy : 0.748374\n",
      "total step : 484 \n",
      "error : 1.102878, accuarcy : 0.748374\n",
      "total step : 485 \n",
      "error : 1.101722, accuarcy : 0.748374\n",
      "total step : 486 \n",
      "error : 1.100570, accuarcy : 0.748374\n",
      "total step : 487 \n",
      "error : 1.099422, accuarcy : 0.748374\n",
      "total step : 488 \n",
      "error : 1.098279, accuarcy : 0.748374\n",
      "total step : 489 \n",
      "error : 1.097140, accuarcy : 0.748374\n",
      "total step : 490 \n",
      "error : 1.096005, accuarcy : 0.748874\n",
      "total step : 491 \n",
      "error : 1.094874, accuarcy : 0.748874\n",
      "total step : 492 \n",
      "error : 1.093747, accuarcy : 0.748874\n",
      "total step : 493 \n",
      "error : 1.092625, accuarcy : 0.748874\n",
      "total step : 494 \n",
      "error : 1.091507, accuarcy : 0.748874\n",
      "total step : 495 \n",
      "error : 1.090393, accuarcy : 0.749375\n",
      "total step : 496 \n",
      "error : 1.089282, accuarcy : 0.749875\n",
      "total step : 497 \n",
      "error : 1.088176, accuarcy : 0.750375\n",
      "total step : 498 \n",
      "error : 1.087074, accuarcy : 0.749875\n",
      "total step : 499 \n",
      "error : 1.085976, accuarcy : 0.749875\n",
      "total step : 500 \n",
      "error : 1.084882, accuarcy : 0.749875\n",
      "total step : 501 \n",
      "error : 1.083792, accuarcy : 0.750375\n",
      "total step : 502 \n",
      "error : 1.082706, accuarcy : 0.750875\n",
      "total step : 503 \n",
      "error : 1.081623, accuarcy : 0.750875\n",
      "total step : 504 \n",
      "error : 1.080545, accuarcy : 0.750875\n",
      "total step : 505 \n",
      "error : 1.079470, accuarcy : 0.751376\n",
      "total step : 506 \n",
      "error : 1.078400, accuarcy : 0.751376\n",
      "total step : 507 \n",
      "error : 1.077333, accuarcy : 0.751376\n",
      "total step : 508 \n",
      "error : 1.076270, accuarcy : 0.751376\n",
      "total step : 509 \n",
      "error : 1.075210, accuarcy : 0.751876\n",
      "total step : 510 \n",
      "error : 1.074155, accuarcy : 0.751876\n",
      "total step : 511 \n",
      "error : 1.073103, accuarcy : 0.752376\n",
      "total step : 512 \n",
      "error : 1.072055, accuarcy : 0.752876\n",
      "total step : 513 \n",
      "error : 1.071011, accuarcy : 0.752876\n",
      "total step : 514 \n",
      "error : 1.069970, accuarcy : 0.752876\n",
      "total step : 515 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error : 1.068933, accuarcy : 0.753877\n",
      "total step : 516 \n",
      "error : 1.067900, accuarcy : 0.753877\n",
      "total step : 517 \n",
      "error : 1.066870, accuarcy : 0.753877\n",
      "total step : 518 \n",
      "error : 1.065844, accuarcy : 0.753877\n",
      "total step : 519 \n",
      "error : 1.064821, accuarcy : 0.753877\n",
      "total step : 520 \n",
      "error : 1.063802, accuarcy : 0.754377\n",
      "total step : 521 \n",
      "error : 1.062787, accuarcy : 0.754377\n",
      "total step : 522 \n",
      "error : 1.061775, accuarcy : 0.753877\n",
      "total step : 523 \n",
      "error : 1.060767, accuarcy : 0.753377\n",
      "total step : 524 \n",
      "error : 1.059762, accuarcy : 0.753877\n",
      "total step : 525 \n",
      "error : 1.058760, accuarcy : 0.753877\n",
      "total step : 526 \n",
      "error : 1.057763, accuarcy : 0.753877\n",
      "total step : 527 \n",
      "error : 1.056768, accuarcy : 0.753877\n",
      "total step : 528 \n",
      "error : 1.055777, accuarcy : 0.753877\n",
      "total step : 529 \n",
      "error : 1.054789, accuarcy : 0.754377\n",
      "total step : 530 \n",
      "error : 1.053805, accuarcy : 0.754377\n",
      "total step : 531 \n",
      "error : 1.052824, accuarcy : 0.754377\n",
      "total step : 532 \n",
      "error : 1.051846, accuarcy : 0.754877\n",
      "total step : 533 \n",
      "error : 1.050872, accuarcy : 0.754877\n",
      "total step : 534 \n",
      "error : 1.049901, accuarcy : 0.754877\n",
      "total step : 535 \n",
      "error : 1.048934, accuarcy : 0.754877\n",
      "total step : 536 \n",
      "error : 1.047969, accuarcy : 0.755378\n",
      "total step : 537 \n",
      "error : 1.047008, accuarcy : 0.755878\n",
      "total step : 538 \n",
      "error : 1.046050, accuarcy : 0.755878\n",
      "total step : 539 \n",
      "error : 1.045096, accuarcy : 0.756378\n",
      "total step : 540 \n",
      "error : 1.044144, accuarcy : 0.756378\n",
      "total step : 541 \n",
      "error : 1.043196, accuarcy : 0.756378\n",
      "total step : 542 \n",
      "error : 1.042251, accuarcy : 0.756378\n",
      "total step : 543 \n",
      "error : 1.041309, accuarcy : 0.756378\n",
      "total step : 544 \n",
      "error : 1.040370, accuarcy : 0.756378\n",
      "total step : 545 \n",
      "error : 1.039435, accuarcy : 0.756378\n",
      "total step : 546 \n",
      "error : 1.038502, accuarcy : 0.756878\n",
      "total step : 547 \n",
      "error : 1.037573, accuarcy : 0.756878\n",
      "total step : 548 \n",
      "error : 1.036646, accuarcy : 0.756878\n",
      "total step : 549 \n",
      "error : 1.035723, accuarcy : 0.757379\n",
      "total step : 550 \n",
      "error : 1.034803, accuarcy : 0.757879\n",
      "total step : 551 \n",
      "error : 1.033886, accuarcy : 0.757879\n",
      "total step : 552 \n",
      "error : 1.032971, accuarcy : 0.757879\n",
      "total step : 553 \n",
      "error : 1.032060, accuarcy : 0.757879\n",
      "total step : 554 \n",
      "error : 1.031152, accuarcy : 0.757879\n",
      "total step : 555 \n",
      "error : 1.030247, accuarcy : 0.757379\n",
      "total step : 556 \n",
      "error : 1.029345, accuarcy : 0.757379\n",
      "total step : 557 \n",
      "error : 1.028445, accuarcy : 0.757379\n",
      "total step : 558 \n",
      "error : 1.027549, accuarcy : 0.757379\n",
      "total step : 559 \n",
      "error : 1.026655, accuarcy : 0.757379\n",
      "total step : 560 \n",
      "error : 1.025765, accuarcy : 0.757379\n",
      "total step : 561 \n",
      "error : 1.024877, accuarcy : 0.757879\n",
      "total step : 562 \n",
      "error : 1.023992, accuarcy : 0.758379\n",
      "total step : 563 \n",
      "error : 1.023110, accuarcy : 0.758379\n",
      "total step : 564 \n",
      "error : 1.022231, accuarcy : 0.758379\n",
      "total step : 565 \n",
      "error : 1.021355, accuarcy : 0.758379\n",
      "total step : 566 \n",
      "error : 1.020481, accuarcy : 0.758879\n",
      "total step : 567 \n",
      "error : 1.019611, accuarcy : 0.758879\n",
      "total step : 568 \n",
      "error : 1.018743, accuarcy : 0.759380\n",
      "total step : 569 \n",
      "error : 1.017878, accuarcy : 0.759380\n",
      "total step : 570 \n",
      "error : 1.017015, accuarcy : 0.759380\n",
      "total step : 571 \n",
      "error : 1.016156, accuarcy : 0.759380\n",
      "total step : 572 \n",
      "error : 1.015299, accuarcy : 0.759880\n",
      "total step : 573 \n",
      "error : 1.014444, accuarcy : 0.759880\n",
      "total step : 574 \n",
      "error : 1.013593, accuarcy : 0.759880\n",
      "total step : 575 \n",
      "error : 1.012744, accuarcy : 0.759880\n",
      "total step : 576 \n",
      "error : 1.011898, accuarcy : 0.759880\n",
      "total step : 577 \n",
      "error : 1.011054, accuarcy : 0.759880\n",
      "total step : 578 \n",
      "error : 1.010214, accuarcy : 0.760880\n",
      "total step : 579 \n",
      "error : 1.009375, accuarcy : 0.761881\n",
      "total step : 580 \n",
      "error : 1.008540, accuarcy : 0.761881\n",
      "total step : 581 \n",
      "error : 1.007707, accuarcy : 0.762381\n",
      "total step : 582 \n",
      "error : 1.006877, accuarcy : 0.762381\n",
      "total step : 583 \n",
      "error : 1.006049, accuarcy : 0.762381\n",
      "total step : 584 \n",
      "error : 1.005224, accuarcy : 0.762881\n",
      "total step : 585 \n",
      "error : 1.004401, accuarcy : 0.762881\n",
      "total step : 586 \n",
      "error : 1.003581, accuarcy : 0.762881\n",
      "total step : 587 \n",
      "error : 1.002763, accuarcy : 0.763382\n",
      "total step : 588 \n",
      "error : 1.001948, accuarcy : 0.763382\n",
      "total step : 589 \n",
      "error : 1.001136, accuarcy : 0.763382\n",
      "total step : 590 \n",
      "error : 1.000326, accuarcy : 0.763382\n",
      "total step : 591 \n",
      "error : 0.999518, accuarcy : 0.763382\n",
      "total step : 592 \n",
      "error : 0.998713, accuarcy : 0.762881\n",
      "total step : 593 \n",
      "error : 0.997910, accuarcy : 0.762881\n",
      "total step : 594 \n",
      "error : 0.997110, accuarcy : 0.763882\n",
      "total step : 595 \n",
      "error : 0.996313, accuarcy : 0.763882\n",
      "total step : 596 \n",
      "error : 0.995517, accuarcy : 0.763882\n",
      "total step : 597 \n",
      "error : 0.994724, accuarcy : 0.763382\n",
      "total step : 598 \n",
      "error : 0.993934, accuarcy : 0.763382\n",
      "total step : 599 \n",
      "error : 0.993146, accuarcy : 0.763382\n",
      "total step : 600 \n",
      "error : 0.992360, accuarcy : 0.763382\n",
      "total step : 601 \n",
      "error : 0.991577, accuarcy : 0.763882\n",
      "total step : 602 \n",
      "error : 0.990796, accuarcy : 0.763882\n",
      "total step : 603 \n",
      "error : 0.990017, accuarcy : 0.763882\n",
      "total step : 604 \n",
      "error : 0.989241, accuarcy : 0.763882\n",
      "total step : 605 \n",
      "error : 0.988467, accuarcy : 0.764882\n",
      "total step : 606 \n",
      "error : 0.987695, accuarcy : 0.765383\n",
      "total step : 607 \n",
      "error : 0.986926, accuarcy : 0.765383\n",
      "total step : 608 \n",
      "error : 0.986159, accuarcy : 0.765383\n",
      "total step : 609 \n",
      "error : 0.985394, accuarcy : 0.765383\n",
      "total step : 610 \n",
      "error : 0.984632, accuarcy : 0.765883\n",
      "total step : 611 \n",
      "error : 0.983872, accuarcy : 0.765883\n",
      "total step : 612 \n",
      "error : 0.983114, accuarcy : 0.765883\n",
      "total step : 613 \n",
      "error : 0.982358, accuarcy : 0.765883\n",
      "total step : 614 \n",
      "error : 0.981605, accuarcy : 0.765883\n",
      "total step : 615 \n",
      "error : 0.980853, accuarcy : 0.765883\n",
      "total step : 616 \n",
      "error : 0.980104, accuarcy : 0.765883\n",
      "total step : 617 \n",
      "error : 0.979357, accuarcy : 0.765883\n",
      "total step : 618 \n",
      "error : 0.978613, accuarcy : 0.765883\n",
      "total step : 619 \n",
      "error : 0.977870, accuarcy : 0.765883\n",
      "total step : 620 \n",
      "error : 0.977130, accuarcy : 0.766883\n",
      "total step : 621 \n",
      "error : 0.976392, accuarcy : 0.766883\n",
      "total step : 622 \n",
      "error : 0.975656, accuarcy : 0.766883\n",
      "total step : 623 \n",
      "error : 0.974922, accuarcy : 0.766883\n",
      "total step : 624 \n",
      "error : 0.974190, accuarcy : 0.767884\n",
      "total step : 625 \n",
      "error : 0.973461, accuarcy : 0.768384\n",
      "total step : 626 \n",
      "error : 0.972733, accuarcy : 0.768384\n",
      "total step : 627 \n",
      "error : 0.972008, accuarcy : 0.768884\n",
      "total step : 628 \n",
      "error : 0.971284, accuarcy : 0.768884\n",
      "total step : 629 \n",
      "error : 0.970563, accuarcy : 0.768884\n",
      "total step : 630 \n",
      "error : 0.969844, accuarcy : 0.768884\n",
      "total step : 631 \n",
      "error : 0.969127, accuarcy : 0.769385\n",
      "total step : 632 \n",
      "error : 0.968412, accuarcy : 0.769385\n",
      "total step : 633 \n",
      "error : 0.967699, accuarcy : 0.769885\n",
      "total step : 634 \n",
      "error : 0.966988, accuarcy : 0.769885\n",
      "total step : 635 \n",
      "error : 0.966279, accuarcy : 0.770885\n",
      "total step : 636 \n",
      "error : 0.965572, accuarcy : 0.770885\n",
      "total step : 637 \n",
      "error : 0.964867, accuarcy : 0.770885\n",
      "total step : 638 \n",
      "error : 0.964164, accuarcy : 0.771386\n",
      "total step : 639 \n",
      "error : 0.963463, accuarcy : 0.771386\n",
      "total step : 640 \n",
      "error : 0.962764, accuarcy : 0.772386\n",
      "total step : 641 \n",
      "error : 0.962067, accuarcy : 0.772386\n",
      "total step : 642 \n",
      "error : 0.961372, accuarcy : 0.772386\n",
      "total step : 643 \n",
      "error : 0.960679, accuarcy : 0.773387\n",
      "total step : 644 \n",
      "error : 0.959988, accuarcy : 0.773387\n",
      "total step : 645 \n",
      "error : 0.959299, accuarcy : 0.773887\n",
      "total step : 646 \n",
      "error : 0.958612, accuarcy : 0.773887\n",
      "total step : 647 \n",
      "error : 0.957926, accuarcy : 0.773887\n",
      "total step : 648 \n",
      "error : 0.957243, accuarcy : 0.774387\n",
      "total step : 649 \n",
      "error : 0.956562, accuarcy : 0.774387\n",
      "total step : 650 \n",
      "error : 0.955882, accuarcy : 0.774387\n",
      "total step : 651 \n",
      "error : 0.955204, accuarcy : 0.774387\n",
      "total step : 652 \n",
      "error : 0.954528, accuarcy : 0.774387\n",
      "total step : 653 \n",
      "error : 0.953854, accuarcy : 0.774387\n",
      "total step : 654 \n",
      "error : 0.953182, accuarcy : 0.774387\n",
      "total step : 655 \n",
      "error : 0.952512, accuarcy : 0.774387\n",
      "total step : 656 \n",
      "error : 0.951844, accuarcy : 0.774387\n",
      "total step : 657 \n",
      "error : 0.951177, accuarcy : 0.774387\n",
      "total step : 658 \n",
      "error : 0.950512, accuarcy : 0.774887\n",
      "total step : 659 \n",
      "error : 0.949849, accuarcy : 0.774887\n",
      "total step : 660 \n",
      "error : 0.949188, accuarcy : 0.774887\n",
      "total step : 661 \n",
      "error : 0.948529, accuarcy : 0.775388\n",
      "total step : 662 \n",
      "error : 0.947871, accuarcy : 0.775388\n",
      "total step : 663 \n",
      "error : 0.947216, accuarcy : 0.775388\n",
      "total step : 664 \n",
      "error : 0.946562, accuarcy : 0.775388\n",
      "total step : 665 \n",
      "error : 0.945910, accuarcy : 0.775388\n",
      "total step : 666 \n",
      "error : 0.945259, accuarcy : 0.775388\n",
      "total step : 667 \n",
      "error : 0.944610, accuarcy : 0.775388\n",
      "total step : 668 \n",
      "error : 0.943964, accuarcy : 0.775388\n",
      "total step : 669 \n",
      "error : 0.943318, accuarcy : 0.775388\n",
      "total step : 670 \n",
      "error : 0.942675, accuarcy : 0.775388\n",
      "total step : 671 \n",
      "error : 0.942033, accuarcy : 0.775888\n",
      "total step : 672 \n",
      "error : 0.941393, accuarcy : 0.776388\n",
      "total step : 673 \n",
      "error : 0.940755, accuarcy : 0.776388\n",
      "total step : 674 \n",
      "error : 0.940119, accuarcy : 0.776388\n",
      "total step : 675 \n",
      "error : 0.939484, accuarcy : 0.775888\n",
      "total step : 676 \n",
      "error : 0.938850, accuarcy : 0.775888\n",
      "total step : 677 \n",
      "error : 0.938219, accuarcy : 0.775888\n",
      "total step : 678 \n",
      "error : 0.937589, accuarcy : 0.775888\n",
      "total step : 679 \n",
      "error : 0.936961, accuarcy : 0.775888\n",
      "total step : 680 \n",
      "error : 0.936335, accuarcy : 0.775888\n",
      "total step : 681 \n",
      "error : 0.935710, accuarcy : 0.775888\n",
      "total step : 682 \n",
      "error : 0.935087, accuarcy : 0.776388\n",
      "total step : 683 \n",
      "error : 0.934465, accuarcy : 0.776388\n",
      "total step : 684 \n",
      "error : 0.933845, accuarcy : 0.776388\n",
      "total step : 685 \n",
      "error : 0.933227, accuarcy : 0.776388\n",
      "total step : 686 \n",
      "error : 0.932610, accuarcy : 0.776388\n",
      "total step : 687 \n",
      "error : 0.931995, accuarcy : 0.776388\n",
      "total step : 688 \n",
      "error : 0.931382, accuarcy : 0.776388\n",
      "total step : 689 \n",
      "error : 0.930770, accuarcy : 0.776388\n",
      "total step : 690 \n",
      "error : 0.930160, accuarcy : 0.776388\n",
      "total step : 691 \n",
      "error : 0.929551, accuarcy : 0.776388\n",
      "total step : 692 \n",
      "error : 0.928944, accuarcy : 0.776388\n",
      "total step : 693 \n",
      "error : 0.928338, accuarcy : 0.776388\n",
      "total step : 694 \n",
      "error : 0.927734, accuarcy : 0.777389\n",
      "total step : 695 \n",
      "error : 0.927132, accuarcy : 0.777389\n",
      "total step : 696 \n",
      "error : 0.926531, accuarcy : 0.777889\n",
      "total step : 697 \n",
      "error : 0.925932, accuarcy : 0.777889\n",
      "total step : 698 \n",
      "error : 0.925334, accuarcy : 0.777889\n",
      "total step : 699 \n",
      "error : 0.924738, accuarcy : 0.777889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 700 \n",
      "error : 0.924143, accuarcy : 0.777389\n",
      "total step : 701 \n",
      "error : 0.923550, accuarcy : 0.777889\n",
      "total step : 702 \n",
      "error : 0.922959, accuarcy : 0.777889\n",
      "total step : 703 \n",
      "error : 0.922368, accuarcy : 0.777889\n",
      "total step : 704 \n",
      "error : 0.921780, accuarcy : 0.777889\n",
      "total step : 705 \n",
      "error : 0.921193, accuarcy : 0.777889\n",
      "total step : 706 \n",
      "error : 0.920607, accuarcy : 0.778389\n",
      "total step : 707 \n",
      "error : 0.920023, accuarcy : 0.778389\n",
      "total step : 708 \n",
      "error : 0.919440, accuarcy : 0.778389\n",
      "total step : 709 \n",
      "error : 0.918859, accuarcy : 0.780390\n",
      "total step : 710 \n",
      "error : 0.918279, accuarcy : 0.780890\n",
      "total step : 711 \n",
      "error : 0.917701, accuarcy : 0.780890\n",
      "total step : 712 \n",
      "error : 0.917124, accuarcy : 0.781391\n",
      "total step : 713 \n",
      "error : 0.916548, accuarcy : 0.781391\n",
      "total step : 714 \n",
      "error : 0.915975, accuarcy : 0.781391\n",
      "total step : 715 \n",
      "error : 0.915402, accuarcy : 0.781391\n",
      "total step : 716 \n",
      "error : 0.914831, accuarcy : 0.782391\n",
      "total step : 717 \n",
      "error : 0.914261, accuarcy : 0.782391\n",
      "total step : 718 \n",
      "error : 0.913693, accuarcy : 0.782391\n",
      "total step : 719 \n",
      "error : 0.913126, accuarcy : 0.782391\n",
      "total step : 720 \n",
      "error : 0.912561, accuarcy : 0.782391\n",
      "total step : 721 \n",
      "error : 0.911997, accuarcy : 0.782891\n",
      "total step : 722 \n",
      "error : 0.911434, accuarcy : 0.783392\n",
      "total step : 723 \n",
      "error : 0.910873, accuarcy : 0.783392\n",
      "total step : 724 \n",
      "error : 0.910313, accuarcy : 0.783392\n",
      "total step : 725 \n",
      "error : 0.909754, accuarcy : 0.783392\n",
      "total step : 726 \n",
      "error : 0.909197, accuarcy : 0.783392\n",
      "total step : 727 \n",
      "error : 0.908641, accuarcy : 0.783392\n",
      "total step : 728 \n",
      "error : 0.908087, accuarcy : 0.783892\n",
      "total step : 729 \n",
      "error : 0.907534, accuarcy : 0.784392\n",
      "total step : 730 \n",
      "error : 0.906982, accuarcy : 0.784892\n",
      "total step : 731 \n",
      "error : 0.906432, accuarcy : 0.784892\n",
      "total step : 732 \n",
      "error : 0.905882, accuarcy : 0.784392\n",
      "total step : 733 \n",
      "error : 0.905335, accuarcy : 0.784392\n",
      "total step : 734 \n",
      "error : 0.904788, accuarcy : 0.784392\n",
      "total step : 735 \n",
      "error : 0.904243, accuarcy : 0.784892\n",
      "total step : 736 \n",
      "error : 0.903700, accuarcy : 0.784892\n",
      "total step : 737 \n",
      "error : 0.903157, accuarcy : 0.784892\n",
      "total step : 738 \n",
      "error : 0.902616, accuarcy : 0.784892\n",
      "total step : 739 \n",
      "error : 0.902076, accuarcy : 0.784892\n",
      "total step : 740 \n",
      "error : 0.901538, accuarcy : 0.785393\n",
      "total step : 741 \n",
      "error : 0.901000, accuarcy : 0.785393\n",
      "total step : 742 \n",
      "error : 0.900464, accuarcy : 0.785393\n",
      "total step : 743 \n",
      "error : 0.899930, accuarcy : 0.785393\n",
      "total step : 744 \n",
      "error : 0.899396, accuarcy : 0.785393\n",
      "total step : 745 \n",
      "error : 0.898864, accuarcy : 0.785393\n",
      "total step : 746 \n",
      "error : 0.898333, accuarcy : 0.785393\n",
      "total step : 747 \n",
      "error : 0.897803, accuarcy : 0.785393\n",
      "total step : 748 \n",
      "error : 0.897275, accuarcy : 0.785393\n",
      "total step : 749 \n",
      "error : 0.896748, accuarcy : 0.785393\n",
      "total step : 750 \n",
      "error : 0.896222, accuarcy : 0.785393\n",
      "total step : 751 \n",
      "error : 0.895697, accuarcy : 0.785393\n",
      "total step : 752 \n",
      "error : 0.895174, accuarcy : 0.785393\n",
      "total step : 753 \n",
      "error : 0.894652, accuarcy : 0.785393\n",
      "total step : 754 \n",
      "error : 0.894131, accuarcy : 0.785393\n",
      "total step : 755 \n",
      "error : 0.893611, accuarcy : 0.785893\n",
      "total step : 756 \n",
      "error : 0.893092, accuarcy : 0.785893\n",
      "total step : 757 \n",
      "error : 0.892575, accuarcy : 0.785893\n",
      "total step : 758 \n",
      "error : 0.892059, accuarcy : 0.785893\n",
      "total step : 759 \n",
      "error : 0.891544, accuarcy : 0.785893\n",
      "total step : 760 \n",
      "error : 0.891030, accuarcy : 0.785893\n",
      "total step : 761 \n",
      "error : 0.890518, accuarcy : 0.785893\n",
      "total step : 762 \n",
      "error : 0.890006, accuarcy : 0.785893\n",
      "total step : 763 \n",
      "error : 0.889496, accuarcy : 0.785893\n",
      "total step : 764 \n",
      "error : 0.888987, accuarcy : 0.785893\n",
      "total step : 765 \n",
      "error : 0.888479, accuarcy : 0.785893\n",
      "total step : 766 \n",
      "error : 0.887972, accuarcy : 0.785893\n",
      "total step : 767 \n",
      "error : 0.887467, accuarcy : 0.785893\n",
      "total step : 768 \n",
      "error : 0.886963, accuarcy : 0.785393\n",
      "total step : 769 \n",
      "error : 0.886459, accuarcy : 0.785393\n",
      "total step : 770 \n",
      "error : 0.885957, accuarcy : 0.785893\n",
      "total step : 771 \n",
      "error : 0.885456, accuarcy : 0.786393\n",
      "total step : 772 \n",
      "error : 0.884957, accuarcy : 0.786393\n",
      "total step : 773 \n",
      "error : 0.884458, accuarcy : 0.786393\n",
      "total step : 774 \n",
      "error : 0.883960, accuarcy : 0.786393\n",
      "total step : 775 \n",
      "error : 0.883464, accuarcy : 0.786393\n",
      "total step : 776 \n",
      "error : 0.882969, accuarcy : 0.786393\n",
      "total step : 777 \n",
      "error : 0.882475, accuarcy : 0.786393\n",
      "total step : 778 \n",
      "error : 0.881982, accuarcy : 0.786393\n",
      "total step : 779 \n",
      "error : 0.881490, accuarcy : 0.786893\n",
      "total step : 780 \n",
      "error : 0.880999, accuarcy : 0.787394\n",
      "total step : 781 \n",
      "error : 0.880509, accuarcy : 0.787394\n",
      "total step : 782 \n",
      "error : 0.880020, accuarcy : 0.787394\n",
      "total step : 783 \n",
      "error : 0.879533, accuarcy : 0.787394\n",
      "total step : 784 \n",
      "error : 0.879046, accuarcy : 0.787394\n",
      "total step : 785 \n",
      "error : 0.878561, accuarcy : 0.787394\n",
      "total step : 786 \n",
      "error : 0.878077, accuarcy : 0.787394\n",
      "total step : 787 \n",
      "error : 0.877593, accuarcy : 0.787394\n",
      "total step : 788 \n",
      "error : 0.877111, accuarcy : 0.787894\n",
      "total step : 789 \n",
      "error : 0.876630, accuarcy : 0.788394\n",
      "total step : 790 \n",
      "error : 0.876150, accuarcy : 0.788394\n",
      "total step : 791 \n",
      "error : 0.875671, accuarcy : 0.788394\n",
      "total step : 792 \n",
      "error : 0.875193, accuarcy : 0.788394\n",
      "total step : 793 \n",
      "error : 0.874716, accuarcy : 0.789895\n",
      "total step : 794 \n",
      "error : 0.874240, accuarcy : 0.789895\n",
      "total step : 795 \n",
      "error : 0.873766, accuarcy : 0.789895\n",
      "total step : 796 \n",
      "error : 0.873292, accuarcy : 0.789895\n",
      "total step : 797 \n",
      "error : 0.872819, accuarcy : 0.789895\n",
      "total step : 798 \n",
      "error : 0.872348, accuarcy : 0.789895\n",
      "total step : 799 \n",
      "error : 0.871877, accuarcy : 0.789895\n",
      "total step : 800 \n",
      "error : 0.871407, accuarcy : 0.789895\n",
      "total step : 801 \n",
      "error : 0.870939, accuarcy : 0.790395\n",
      "total step : 802 \n",
      "error : 0.870471, accuarcy : 0.790395\n",
      "total step : 803 \n",
      "error : 0.870004, accuarcy : 0.790895\n",
      "total step : 804 \n",
      "error : 0.869539, accuarcy : 0.790895\n",
      "total step : 805 \n",
      "error : 0.869074, accuarcy : 0.790895\n",
      "total step : 806 \n",
      "error : 0.868611, accuarcy : 0.791896\n",
      "total step : 807 \n",
      "error : 0.868148, accuarcy : 0.791896\n",
      "total step : 808 \n",
      "error : 0.867686, accuarcy : 0.792896\n",
      "total step : 809 \n",
      "error : 0.867226, accuarcy : 0.792896\n",
      "total step : 810 \n",
      "error : 0.866766, accuarcy : 0.792896\n",
      "total step : 811 \n",
      "error : 0.866307, accuarcy : 0.792896\n",
      "total step : 812 \n",
      "error : 0.865850, accuarcy : 0.792896\n",
      "total step : 813 \n",
      "error : 0.865393, accuarcy : 0.792896\n",
      "total step : 814 \n",
      "error : 0.864937, accuarcy : 0.792896\n",
      "total step : 815 \n",
      "error : 0.864482, accuarcy : 0.792896\n",
      "total step : 816 \n",
      "error : 0.864029, accuarcy : 0.793397\n",
      "total step : 817 \n",
      "error : 0.863576, accuarcy : 0.793397\n",
      "total step : 818 \n",
      "error : 0.863124, accuarcy : 0.793397\n",
      "total step : 819 \n",
      "error : 0.862673, accuarcy : 0.793897\n",
      "total step : 820 \n",
      "error : 0.862223, accuarcy : 0.793897\n",
      "total step : 821 \n",
      "error : 0.861774, accuarcy : 0.793897\n",
      "total step : 822 \n",
      "error : 0.861326, accuarcy : 0.794397\n",
      "total step : 823 \n",
      "error : 0.860879, accuarcy : 0.794397\n",
      "total step : 824 \n",
      "error : 0.860432, accuarcy : 0.794397\n",
      "total step : 825 \n",
      "error : 0.859987, accuarcy : 0.794397\n",
      "total step : 826 \n",
      "error : 0.859543, accuarcy : 0.794397\n",
      "total step : 827 \n",
      "error : 0.859099, accuarcy : 0.794397\n",
      "total step : 828 \n",
      "error : 0.858657, accuarcy : 0.794397\n",
      "total step : 829 \n",
      "error : 0.858215, accuarcy : 0.794397\n",
      "total step : 830 \n",
      "error : 0.857774, accuarcy : 0.794397\n",
      "total step : 831 \n",
      "error : 0.857334, accuarcy : 0.794397\n",
      "total step : 832 \n",
      "error : 0.856895, accuarcy : 0.794897\n",
      "total step : 833 \n",
      "error : 0.856457, accuarcy : 0.794897\n",
      "total step : 834 \n",
      "error : 0.856020, accuarcy : 0.794897\n",
      "total step : 835 \n",
      "error : 0.855584, accuarcy : 0.794897\n",
      "total step : 836 \n",
      "error : 0.855149, accuarcy : 0.794897\n",
      "total step : 837 \n",
      "error : 0.854714, accuarcy : 0.795398\n",
      "total step : 838 \n",
      "error : 0.854281, accuarcy : 0.795398\n",
      "total step : 839 \n",
      "error : 0.853848, accuarcy : 0.795398\n",
      "total step : 840 \n",
      "error : 0.853416, accuarcy : 0.795398\n",
      "total step : 841 \n",
      "error : 0.852985, accuarcy : 0.795398\n",
      "total step : 842 \n",
      "error : 0.852555, accuarcy : 0.795398\n",
      "total step : 843 \n",
      "error : 0.852126, accuarcy : 0.795398\n",
      "total step : 844 \n",
      "error : 0.851698, accuarcy : 0.795398\n",
      "total step : 845 \n",
      "error : 0.851270, accuarcy : 0.795398\n",
      "total step : 846 \n",
      "error : 0.850844, accuarcy : 0.795398\n",
      "total step : 847 \n",
      "error : 0.850418, accuarcy : 0.795898\n",
      "total step : 848 \n",
      "error : 0.849993, accuarcy : 0.796398\n",
      "total step : 849 \n",
      "error : 0.849569, accuarcy : 0.796398\n",
      "total step : 850 \n",
      "error : 0.849146, accuarcy : 0.796398\n",
      "total step : 851 \n",
      "error : 0.848724, accuarcy : 0.796398\n",
      "total step : 852 \n",
      "error : 0.848302, accuarcy : 0.797399\n",
      "total step : 853 \n",
      "error : 0.847882, accuarcy : 0.797399\n",
      "total step : 854 \n",
      "error : 0.847462, accuarcy : 0.797899\n",
      "total step : 855 \n",
      "error : 0.847043, accuarcy : 0.798399\n",
      "total step : 856 \n",
      "error : 0.846625, accuarcy : 0.798399\n",
      "total step : 857 \n",
      "error : 0.846208, accuarcy : 0.798399\n",
      "total step : 858 \n",
      "error : 0.845791, accuarcy : 0.798399\n",
      "total step : 859 \n",
      "error : 0.845375, accuarcy : 0.798899\n",
      "total step : 860 \n",
      "error : 0.844961, accuarcy : 0.798899\n",
      "total step : 861 \n",
      "error : 0.844547, accuarcy : 0.798899\n",
      "total step : 862 \n",
      "error : 0.844133, accuarcy : 0.798899\n",
      "total step : 863 \n",
      "error : 0.843721, accuarcy : 0.798899\n",
      "total step : 864 \n",
      "error : 0.843309, accuarcy : 0.798899\n",
      "total step : 865 \n",
      "error : 0.842899, accuarcy : 0.798899\n",
      "total step : 866 \n",
      "error : 0.842489, accuarcy : 0.798899\n",
      "total step : 867 \n",
      "error : 0.842079, accuarcy : 0.798899\n",
      "total step : 868 \n",
      "error : 0.841671, accuarcy : 0.799400\n",
      "total step : 869 \n",
      "error : 0.841263, accuarcy : 0.799400\n",
      "total step : 870 \n",
      "error : 0.840857, accuarcy : 0.799900\n",
      "total step : 871 \n",
      "error : 0.840451, accuarcy : 0.800400\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for number 4\n",
      "total step : 1 \n",
      "error : 5.234192, accuarcy : 0.516258\n",
      "total step : 2 \n",
      "error : 5.201286, accuarcy : 0.517759\n",
      "total step : 3 \n",
      "error : 5.168436, accuarcy : 0.519260\n",
      "total step : 4 \n",
      "error : 5.135658, accuarcy : 0.517759\n",
      "total step : 5 \n",
      "error : 5.102965, accuarcy : 0.517759\n",
      "total step : 6 \n",
      "error : 5.070373, accuarcy : 0.516258\n",
      "total step : 7 \n",
      "error : 5.037893, accuarcy : 0.514757\n",
      "total step : 8 \n",
      "error : 5.005538, accuarcy : 0.514257\n",
      "total step : 9 \n",
      "error : 4.973319, accuarcy : 0.513757\n",
      "total step : 10 \n",
      "error : 4.941246, accuarcy : 0.514257\n",
      "total step : 11 \n",
      "error : 4.909328, accuarcy : 0.515258\n",
      "total step : 12 \n",
      "error : 4.877573, accuarcy : 0.514757\n",
      "total step : 13 \n",
      "error : 4.845987, accuarcy : 0.512756\n",
      "total step : 14 \n",
      "error : 4.814577, accuarcy : 0.511756\n",
      "total step : 15 \n",
      "error : 4.783348, accuarcy : 0.510255\n",
      "total step : 16 \n",
      "error : 4.752305, accuarcy : 0.514757\n",
      "total step : 17 \n",
      "error : 4.721452, accuarcy : 0.515258\n",
      "total step : 18 \n",
      "error : 4.690792, accuarcy : 0.514757\n",
      "total step : 19 \n",
      "error : 4.660329, accuarcy : 0.514757\n",
      "total step : 20 \n",
      "error : 4.630064, accuarcy : 0.515258\n",
      "total step : 21 \n",
      "error : 4.600000, accuarcy : 0.515258\n",
      "total step : 22 \n",
      "error : 4.570140, accuarcy : 0.515258\n",
      "total step : 23 \n",
      "error : 4.540486, accuarcy : 0.516258\n",
      "total step : 24 \n",
      "error : 4.511040, accuarcy : 0.514257\n",
      "total step : 25 \n",
      "error : 4.481804, accuarcy : 0.512256\n",
      "total step : 26 \n",
      "error : 4.452782, accuarcy : 0.512756\n",
      "total step : 27 \n",
      "error : 4.423974, accuarcy : 0.511256\n",
      "total step : 28 \n",
      "error : 4.395383, accuarcy : 0.509255\n",
      "total step : 29 \n",
      "error : 4.367013, accuarcy : 0.508254\n",
      "total step : 30 \n",
      "error : 4.338864, accuarcy : 0.509755\n",
      "total step : 31 \n",
      "error : 4.310939, accuarcy : 0.509755\n",
      "total step : 32 \n",
      "error : 4.283241, accuarcy : 0.509255\n",
      "total step : 33 \n",
      "error : 4.255772, accuarcy : 0.507754\n",
      "total step : 34 \n",
      "error : 4.228534, accuarcy : 0.509255\n",
      "total step : 35 \n",
      "error : 4.201529, accuarcy : 0.509255\n",
      "total step : 36 \n",
      "error : 4.174760, accuarcy : 0.509755\n",
      "total step : 37 \n",
      "error : 4.148228, accuarcy : 0.510755\n",
      "total step : 38 \n",
      "error : 4.121936, accuarcy : 0.509255\n",
      "total step : 39 \n",
      "error : 4.095887, accuarcy : 0.507254\n",
      "total step : 40 \n",
      "error : 4.070081, accuarcy : 0.508254\n",
      "total step : 41 \n",
      "error : 4.044521, accuarcy : 0.508754\n",
      "total step : 42 \n",
      "error : 4.019209, accuarcy : 0.506753\n",
      "total step : 43 \n",
      "error : 3.994145, accuarcy : 0.506253\n",
      "total step : 44 \n",
      "error : 3.969330, accuarcy : 0.506253\n",
      "total step : 45 \n",
      "error : 3.944766, accuarcy : 0.506753\n",
      "total step : 46 \n",
      "error : 3.920452, accuarcy : 0.506753\n",
      "total step : 47 \n",
      "error : 3.896388, accuarcy : 0.508254\n",
      "total step : 48 \n",
      "error : 3.872573, accuarcy : 0.506753\n",
      "total step : 49 \n",
      "error : 3.849006, accuarcy : 0.505753\n",
      "total step : 50 \n",
      "error : 3.825686, accuarcy : 0.506753\n",
      "total step : 51 \n",
      "error : 3.802610, accuarcy : 0.505753\n",
      "total step : 52 \n",
      "error : 3.779777, accuarcy : 0.504752\n",
      "total step : 53 \n",
      "error : 3.757184, accuarcy : 0.506253\n",
      "total step : 54 \n",
      "error : 3.734829, accuarcy : 0.507254\n",
      "total step : 55 \n",
      "error : 3.712708, accuarcy : 0.507754\n",
      "total step : 56 \n",
      "error : 3.690819, accuarcy : 0.507754\n",
      "total step : 57 \n",
      "error : 3.669159, accuarcy : 0.506753\n",
      "total step : 58 \n",
      "error : 3.647723, accuarcy : 0.507254\n",
      "total step : 59 \n",
      "error : 3.626510, accuarcy : 0.506753\n",
      "total step : 60 \n",
      "error : 3.605516, accuarcy : 0.506753\n",
      "total step : 61 \n",
      "error : 3.584738, accuarcy : 0.505253\n",
      "total step : 62 \n",
      "error : 3.564173, accuarcy : 0.506253\n",
      "total step : 63 \n",
      "error : 3.543819, accuarcy : 0.506253\n",
      "total step : 64 \n",
      "error : 3.523672, accuarcy : 0.506753\n",
      "total step : 65 \n",
      "error : 3.503731, accuarcy : 0.507254\n",
      "total step : 66 \n",
      "error : 3.483994, accuarcy : 0.505753\n",
      "total step : 67 \n",
      "error : 3.464459, accuarcy : 0.505253\n",
      "total step : 68 \n",
      "error : 3.445123, accuarcy : 0.505753\n",
      "total step : 69 \n",
      "error : 3.425986, accuarcy : 0.505253\n",
      "total step : 70 \n",
      "error : 3.407046, accuarcy : 0.505253\n",
      "total step : 71 \n",
      "error : 3.388302, accuarcy : 0.503752\n",
      "total step : 72 \n",
      "error : 3.369753, accuarcy : 0.503752\n",
      "total step : 73 \n",
      "error : 3.351397, accuarcy : 0.505753\n",
      "total step : 74 \n",
      "error : 3.333234, accuarcy : 0.507754\n",
      "total step : 75 \n",
      "error : 3.315262, accuarcy : 0.507254\n",
      "total step : 76 \n",
      "error : 3.297481, accuarcy : 0.507754\n",
      "total step : 77 \n",
      "error : 3.279888, accuarcy : 0.508254\n",
      "total step : 78 \n",
      "error : 3.262483, accuarcy : 0.508754\n",
      "total step : 79 \n",
      "error : 3.245264, accuarcy : 0.510755\n",
      "total step : 80 \n",
      "error : 3.228230, accuarcy : 0.510755\n",
      "total step : 81 \n",
      "error : 3.211379, accuarcy : 0.512756\n",
      "total step : 82 \n",
      "error : 3.194710, accuarcy : 0.513757\n",
      "total step : 83 \n",
      "error : 3.178220, accuarcy : 0.514257\n",
      "total step : 84 \n",
      "error : 3.161908, accuarcy : 0.516258\n",
      "total step : 85 \n",
      "error : 3.145773, accuarcy : 0.516258\n",
      "total step : 86 \n",
      "error : 3.129811, accuarcy : 0.516258\n",
      "total step : 87 \n",
      "error : 3.114020, accuarcy : 0.515258\n",
      "total step : 88 \n",
      "error : 3.098399, accuarcy : 0.516258\n",
      "total step : 89 \n",
      "error : 3.082946, accuarcy : 0.515758\n",
      "total step : 90 \n",
      "error : 3.067657, accuarcy : 0.518759\n",
      "total step : 91 \n",
      "error : 3.052530, accuarcy : 0.518259\n",
      "total step : 92 \n",
      "error : 3.037564, accuarcy : 0.517759\n",
      "total step : 93 \n",
      "error : 3.022755, accuarcy : 0.518259\n",
      "total step : 94 \n",
      "error : 3.008101, accuarcy : 0.517759\n",
      "total step : 95 \n",
      "error : 2.993599, accuarcy : 0.517259\n",
      "total step : 96 \n",
      "error : 2.979247, accuarcy : 0.515758\n",
      "total step : 97 \n",
      "error : 2.965043, accuarcy : 0.515758\n",
      "total step : 98 \n",
      "error : 2.950983, accuarcy : 0.517259\n",
      "total step : 99 \n",
      "error : 2.937066, accuarcy : 0.518259\n",
      "total step : 100 \n",
      "error : 2.923289, accuarcy : 0.517759\n",
      "total step : 101 \n",
      "error : 2.909650, accuarcy : 0.518759\n",
      "total step : 102 \n",
      "error : 2.896145, accuarcy : 0.520260\n",
      "total step : 103 \n",
      "error : 2.882773, accuarcy : 0.522261\n",
      "total step : 104 \n",
      "error : 2.869532, accuarcy : 0.525763\n",
      "total step : 105 \n",
      "error : 2.856419, accuarcy : 0.526263\n",
      "total step : 106 \n",
      "error : 2.843431, accuarcy : 0.526263\n",
      "total step : 107 \n",
      "error : 2.830567, accuarcy : 0.526763\n",
      "total step : 108 \n",
      "error : 2.817825, accuarcy : 0.526763\n",
      "total step : 109 \n",
      "error : 2.805201, accuarcy : 0.528264\n",
      "total step : 110 \n",
      "error : 2.792696, accuarcy : 0.528264\n",
      "total step : 111 \n",
      "error : 2.780305, accuarcy : 0.529265\n",
      "total step : 112 \n",
      "error : 2.768028, accuarcy : 0.529265\n",
      "total step : 113 \n",
      "error : 2.755862, accuarcy : 0.530265\n",
      "total step : 114 \n",
      "error : 2.743805, accuarcy : 0.530265\n",
      "total step : 115 \n",
      "error : 2.731856, accuarcy : 0.530265\n",
      "total step : 116 \n",
      "error : 2.720013, accuarcy : 0.531766\n",
      "total step : 117 \n",
      "error : 2.708274, accuarcy : 0.533267\n",
      "total step : 118 \n",
      "error : 2.696637, accuarcy : 0.533767\n",
      "total step : 119 \n",
      "error : 2.685100, accuarcy : 0.535268\n",
      "total step : 120 \n",
      "error : 2.673662, accuarcy : 0.537269\n",
      "total step : 121 \n",
      "error : 2.662321, accuarcy : 0.537769\n",
      "total step : 122 \n",
      "error : 2.651075, accuarcy : 0.536268\n",
      "total step : 123 \n",
      "error : 2.639924, accuarcy : 0.536268\n",
      "total step : 124 \n",
      "error : 2.628864, accuarcy : 0.536768\n",
      "total step : 125 \n",
      "error : 2.617895, accuarcy : 0.537769\n",
      "total step : 126 \n",
      "error : 2.607015, accuarcy : 0.538269\n",
      "total step : 127 \n",
      "error : 2.596222, accuarcy : 0.538769\n",
      "total step : 128 \n",
      "error : 2.585515, accuarcy : 0.540270\n",
      "total step : 129 \n",
      "error : 2.574893, accuarcy : 0.541771\n",
      "total step : 130 \n",
      "error : 2.564354, accuarcy : 0.544272\n",
      "total step : 131 \n",
      "error : 2.553896, accuarcy : 0.545273\n",
      "total step : 132 \n",
      "error : 2.543519, accuarcy : 0.545273\n",
      "total step : 133 \n",
      "error : 2.533220, accuarcy : 0.545773\n",
      "total step : 134 \n",
      "error : 2.522998, accuarcy : 0.546773\n",
      "total step : 135 \n",
      "error : 2.512853, accuarcy : 0.547274\n",
      "total step : 136 \n",
      "error : 2.502782, accuarcy : 0.547774\n",
      "total step : 137 \n",
      "error : 2.492785, accuarcy : 0.549275\n",
      "total step : 138 \n",
      "error : 2.482860, accuarcy : 0.552276\n",
      "total step : 139 \n",
      "error : 2.473006, accuarcy : 0.552276\n",
      "total step : 140 \n",
      "error : 2.463221, accuarcy : 0.552276\n",
      "total step : 141 \n",
      "error : 2.453505, accuarcy : 0.551776\n",
      "total step : 142 \n",
      "error : 2.443857, accuarcy : 0.553277\n",
      "total step : 143 \n",
      "error : 2.434274, accuarcy : 0.553777\n",
      "total step : 144 \n",
      "error : 2.424757, accuarcy : 0.554277\n",
      "total step : 145 \n",
      "error : 2.415304, accuarcy : 0.555778\n",
      "total step : 146 \n",
      "error : 2.405913, accuarcy : 0.557279\n",
      "total step : 147 \n",
      "error : 2.396584, accuarcy : 0.558779\n",
      "total step : 148 \n",
      "error : 2.387317, accuarcy : 0.558779\n",
      "total step : 149 \n",
      "error : 2.378109, accuarcy : 0.558779\n",
      "total step : 150 \n",
      "error : 2.368960, accuarcy : 0.559280\n",
      "total step : 151 \n",
      "error : 2.359869, accuarcy : 0.559280\n",
      "total step : 152 \n",
      "error : 2.350835, accuarcy : 0.560780\n",
      "total step : 153 \n",
      "error : 2.341857, accuarcy : 0.560280\n",
      "total step : 154 \n",
      "error : 2.332934, accuarcy : 0.559780\n",
      "total step : 155 \n",
      "error : 2.324066, accuarcy : 0.561281\n",
      "total step : 156 \n",
      "error : 2.315252, accuarcy : 0.561781\n",
      "total step : 157 \n",
      "error : 2.306491, accuarcy : 0.563782\n",
      "total step : 158 \n",
      "error : 2.297781, accuarcy : 0.562781\n",
      "total step : 159 \n",
      "error : 2.289123, accuarcy : 0.564282\n",
      "total step : 160 \n",
      "error : 2.280516, accuarcy : 0.565783\n",
      "total step : 161 \n",
      "error : 2.271959, accuarcy : 0.565783\n",
      "total step : 162 \n",
      "error : 2.263451, accuarcy : 0.566783\n",
      "total step : 163 \n",
      "error : 2.254992, accuarcy : 0.567284\n",
      "total step : 164 \n",
      "error : 2.246581, accuarcy : 0.569285\n",
      "total step : 165 \n",
      "error : 2.238217, accuarcy : 0.569785\n",
      "total step : 166 \n",
      "error : 2.229900, accuarcy : 0.570285\n",
      "total step : 167 \n",
      "error : 2.221630, accuarcy : 0.570785\n",
      "total step : 168 \n",
      "error : 2.213405, accuarcy : 0.572286\n",
      "total step : 169 \n",
      "error : 2.205226, accuarcy : 0.573287\n",
      "total step : 170 \n",
      "error : 2.197091, accuarcy : 0.574287\n",
      "total step : 171 \n",
      "error : 2.189001, accuarcy : 0.575288\n",
      "total step : 172 \n",
      "error : 2.180955, accuarcy : 0.576288\n",
      "total step : 173 \n",
      "error : 2.172952, accuarcy : 0.576288\n",
      "total step : 174 \n",
      "error : 2.164992, accuarcy : 0.577289\n",
      "total step : 175 \n",
      "error : 2.157074, accuarcy : 0.578289\n",
      "total step : 176 \n",
      "error : 2.149199, accuarcy : 0.578289\n",
      "total step : 177 \n",
      "error : 2.141365, accuarcy : 0.579790\n",
      "total step : 178 \n",
      "error : 2.133573, accuarcy : 0.580790\n",
      "total step : 179 \n",
      "error : 2.125822, accuarcy : 0.580790\n",
      "total step : 180 \n",
      "error : 2.118112, accuarcy : 0.581291\n",
      "total step : 181 \n",
      "error : 2.110442, accuarcy : 0.582791\n",
      "total step : 182 \n",
      "error : 2.102812, accuarcy : 0.584292\n",
      "total step : 183 \n",
      "error : 2.095221, accuarcy : 0.584292\n",
      "total step : 184 \n",
      "error : 2.087670, accuarcy : 0.584792\n",
      "total step : 185 \n",
      "error : 2.080159, accuarcy : 0.585793\n",
      "total step : 186 \n",
      "error : 2.072686, accuarcy : 0.587294\n",
      "total step : 187 \n",
      "error : 2.065251, accuarcy : 0.588294\n",
      "total step : 188 \n",
      "error : 2.057855, accuarcy : 0.589795\n",
      "total step : 189 \n",
      "error : 2.050497, accuarcy : 0.589795\n",
      "total step : 190 \n",
      "error : 2.043177, accuarcy : 0.589795\n",
      "total step : 191 \n",
      "error : 2.035895, accuarcy : 0.591296\n",
      "total step : 192 \n",
      "error : 2.028650, accuarcy : 0.592296\n",
      "total step : 193 \n",
      "error : 2.021442, accuarcy : 0.592796\n",
      "total step : 194 \n",
      "error : 2.014270, accuarcy : 0.592296\n",
      "total step : 195 \n",
      "error : 2.007136, accuarcy : 0.593297\n",
      "total step : 196 \n",
      "error : 2.000038, accuarcy : 0.591796\n",
      "total step : 197 \n",
      "error : 1.992976, accuarcy : 0.592296\n",
      "total step : 198 \n",
      "error : 1.985950, accuarcy : 0.592796\n",
      "total step : 199 \n",
      "error : 1.978961, accuarcy : 0.592796\n",
      "total step : 200 \n",
      "error : 1.972007, accuarcy : 0.594297\n",
      "total step : 201 \n",
      "error : 1.965088, accuarcy : 0.594297\n",
      "total step : 202 \n",
      "error : 1.958205, accuarcy : 0.596298\n",
      "total step : 203 \n",
      "error : 1.951357, accuarcy : 0.596298\n",
      "total step : 204 \n",
      "error : 1.944544, accuarcy : 0.597299\n",
      "total step : 205 \n",
      "error : 1.937765, accuarcy : 0.597799\n",
      "total step : 206 \n",
      "error : 1.931022, accuarcy : 0.597299\n",
      "total step : 207 \n",
      "error : 1.924313, accuarcy : 0.598299\n",
      "total step : 208 \n",
      "error : 1.917638, accuarcy : 0.598299\n",
      "total step : 209 \n",
      "error : 1.910997, accuarcy : 0.598799\n",
      "total step : 210 \n",
      "error : 1.904391, accuarcy : 0.600300\n",
      "total step : 211 \n",
      "error : 1.897818, accuarcy : 0.601301\n",
      "total step : 212 \n",
      "error : 1.891279, accuarcy : 0.602301\n",
      "total step : 213 \n",
      "error : 1.884774, accuarcy : 0.603302\n",
      "total step : 214 \n",
      "error : 1.878302, accuarcy : 0.603802\n",
      "total step : 215 \n",
      "error : 1.871863, accuarcy : 0.603802\n",
      "total step : 216 \n",
      "error : 1.865458, accuarcy : 0.603802\n",
      "total step : 217 \n",
      "error : 1.859085, accuarcy : 0.604302\n",
      "total step : 218 \n",
      "error : 1.852746, accuarcy : 0.605303\n",
      "total step : 219 \n",
      "error : 1.846439, accuarcy : 0.606803\n",
      "total step : 220 \n",
      "error : 1.840164, accuarcy : 0.607804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 221 \n",
      "error : 1.833923, accuarcy : 0.607804\n",
      "total step : 222 \n",
      "error : 1.827713, accuarcy : 0.608304\n",
      "total step : 223 \n",
      "error : 1.821536, accuarcy : 0.608804\n",
      "total step : 224 \n",
      "error : 1.815391, accuarcy : 0.608804\n",
      "total step : 225 \n",
      "error : 1.809278, accuarcy : 0.609805\n",
      "total step : 226 \n",
      "error : 1.803196, accuarcy : 0.611806\n",
      "total step : 227 \n",
      "error : 1.797147, accuarcy : 0.613307\n",
      "total step : 228 \n",
      "error : 1.791129, accuarcy : 0.612806\n",
      "total step : 229 \n",
      "error : 1.785142, accuarcy : 0.613807\n",
      "total step : 230 \n",
      "error : 1.779187, accuarcy : 0.613807\n",
      "total step : 231 \n",
      "error : 1.773263, accuarcy : 0.613807\n",
      "total step : 232 \n",
      "error : 1.767370, accuarcy : 0.613807\n",
      "total step : 233 \n",
      "error : 1.761508, accuarcy : 0.615308\n",
      "total step : 234 \n",
      "error : 1.755677, accuarcy : 0.615308\n",
      "total step : 235 \n",
      "error : 1.749877, accuarcy : 0.615808\n",
      "total step : 236 \n",
      "error : 1.744107, accuarcy : 0.616808\n",
      "total step : 237 \n",
      "error : 1.738367, accuarcy : 0.617309\n",
      "total step : 238 \n",
      "error : 1.732659, accuarcy : 0.618309\n",
      "total step : 239 \n",
      "error : 1.726980, accuarcy : 0.619310\n",
      "total step : 240 \n",
      "error : 1.721331, accuarcy : 0.619810\n",
      "total step : 241 \n",
      "error : 1.715713, accuarcy : 0.620310\n",
      "total step : 242 \n",
      "error : 1.710124, accuarcy : 0.621811\n",
      "total step : 243 \n",
      "error : 1.704565, accuarcy : 0.622311\n",
      "total step : 244 \n",
      "error : 1.699035, accuarcy : 0.622311\n",
      "total step : 245 \n",
      "error : 1.693535, accuarcy : 0.622311\n",
      "total step : 246 \n",
      "error : 1.688065, accuarcy : 0.623312\n",
      "total step : 247 \n",
      "error : 1.682623, accuarcy : 0.623312\n",
      "total step : 248 \n",
      "error : 1.677211, accuarcy : 0.623312\n",
      "total step : 249 \n",
      "error : 1.671828, accuarcy : 0.624312\n",
      "total step : 250 \n",
      "error : 1.666474, accuarcy : 0.624812\n",
      "total step : 251 \n",
      "error : 1.661148, accuarcy : 0.624812\n",
      "total step : 252 \n",
      "error : 1.655851, accuarcy : 0.625813\n",
      "total step : 253 \n",
      "error : 1.650582, accuarcy : 0.628314\n",
      "total step : 254 \n",
      "error : 1.645342, accuarcy : 0.629315\n",
      "total step : 255 \n",
      "error : 1.640130, accuarcy : 0.630315\n",
      "total step : 256 \n",
      "error : 1.634947, accuarcy : 0.630815\n",
      "total step : 257 \n",
      "error : 1.629791, accuarcy : 0.631816\n",
      "total step : 258 \n",
      "error : 1.624663, accuarcy : 0.632816\n",
      "total step : 259 \n",
      "error : 1.619562, accuarcy : 0.633317\n",
      "total step : 260 \n",
      "error : 1.614490, accuarcy : 0.633317\n",
      "total step : 261 \n",
      "error : 1.609444, accuarcy : 0.634817\n",
      "total step : 262 \n",
      "error : 1.604426, accuarcy : 0.635818\n",
      "total step : 263 \n",
      "error : 1.599436, accuarcy : 0.635818\n",
      "total step : 264 \n",
      "error : 1.594472, accuarcy : 0.635818\n",
      "total step : 265 \n",
      "error : 1.589535, accuarcy : 0.635818\n",
      "total step : 266 \n",
      "error : 1.584625, accuarcy : 0.636318\n",
      "total step : 267 \n",
      "error : 1.579742, accuarcy : 0.637819\n",
      "total step : 268 \n",
      "error : 1.574885, accuarcy : 0.638319\n",
      "total step : 269 \n",
      "error : 1.570054, accuarcy : 0.639820\n",
      "total step : 270 \n",
      "error : 1.565250, accuarcy : 0.640820\n",
      "total step : 271 \n",
      "error : 1.560472, accuarcy : 0.642321\n",
      "total step : 272 \n",
      "error : 1.555719, accuarcy : 0.642821\n",
      "total step : 273 \n",
      "error : 1.550993, accuarcy : 0.643322\n",
      "total step : 274 \n",
      "error : 1.546292, accuarcy : 0.643822\n",
      "total step : 275 \n",
      "error : 1.541617, accuarcy : 0.644322\n",
      "total step : 276 \n",
      "error : 1.536967, accuarcy : 0.644822\n",
      "total step : 277 \n",
      "error : 1.532343, accuarcy : 0.645323\n",
      "total step : 278 \n",
      "error : 1.527744, accuarcy : 0.646823\n",
      "total step : 279 \n",
      "error : 1.523169, accuarcy : 0.647324\n",
      "total step : 280 \n",
      "error : 1.518620, accuarcy : 0.647824\n",
      "total step : 281 \n",
      "error : 1.514095, accuarcy : 0.648824\n",
      "total step : 282 \n",
      "error : 1.509595, accuarcy : 0.650325\n",
      "total step : 283 \n",
      "error : 1.505119, accuarcy : 0.651326\n",
      "total step : 284 \n",
      "error : 1.500667, accuarcy : 0.651326\n",
      "total step : 285 \n",
      "error : 1.496240, accuarcy : 0.651326\n",
      "total step : 286 \n",
      "error : 1.491836, accuarcy : 0.653327\n",
      "total step : 287 \n",
      "error : 1.487457, accuarcy : 0.652826\n",
      "total step : 288 \n",
      "error : 1.483101, accuarcy : 0.652326\n",
      "total step : 289 \n",
      "error : 1.478769, accuarcy : 0.652826\n",
      "total step : 290 \n",
      "error : 1.474460, accuarcy : 0.654827\n",
      "total step : 291 \n",
      "error : 1.470175, accuarcy : 0.656828\n",
      "total step : 292 \n",
      "error : 1.465912, accuarcy : 0.658329\n",
      "total step : 293 \n",
      "error : 1.461673, accuarcy : 0.659830\n",
      "total step : 294 \n",
      "error : 1.457457, accuarcy : 0.660330\n",
      "total step : 295 \n",
      "error : 1.453263, accuarcy : 0.660330\n",
      "total step : 296 \n",
      "error : 1.449092, accuarcy : 0.660330\n",
      "total step : 297 \n",
      "error : 1.444943, accuarcy : 0.660830\n",
      "total step : 298 \n",
      "error : 1.440817, accuarcy : 0.661831\n",
      "total step : 299 \n",
      "error : 1.436713, accuarcy : 0.661831\n",
      "total step : 300 \n",
      "error : 1.432630, accuarcy : 0.663332\n",
      "total step : 301 \n",
      "error : 1.428570, accuarcy : 0.663832\n",
      "total step : 302 \n",
      "error : 1.424532, accuarcy : 0.665333\n",
      "total step : 303 \n",
      "error : 1.420515, accuarcy : 0.666333\n",
      "total step : 304 \n",
      "error : 1.416519, accuarcy : 0.667334\n",
      "total step : 305 \n",
      "error : 1.412545, accuarcy : 0.667334\n",
      "total step : 306 \n",
      "error : 1.408592, accuarcy : 0.668334\n",
      "total step : 307 \n",
      "error : 1.404660, accuarcy : 0.669335\n",
      "total step : 308 \n",
      "error : 1.400749, accuarcy : 0.669335\n",
      "total step : 309 \n",
      "error : 1.396859, accuarcy : 0.670335\n",
      "total step : 310 \n",
      "error : 1.392989, accuarcy : 0.670835\n",
      "total step : 311 \n",
      "error : 1.389140, accuarcy : 0.672336\n",
      "total step : 312 \n",
      "error : 1.385312, accuarcy : 0.672836\n",
      "total step : 313 \n",
      "error : 1.381503, accuarcy : 0.673837\n",
      "total step : 314 \n",
      "error : 1.377715, accuarcy : 0.674837\n",
      "total step : 315 \n",
      "error : 1.373946, accuarcy : 0.675338\n",
      "total step : 316 \n",
      "error : 1.370198, accuarcy : 0.676838\n",
      "total step : 317 \n",
      "error : 1.366469, accuarcy : 0.676338\n",
      "total step : 318 \n",
      "error : 1.362759, accuarcy : 0.677339\n",
      "total step : 319 \n",
      "error : 1.359069, accuarcy : 0.676838\n",
      "total step : 320 \n",
      "error : 1.355399, accuarcy : 0.678339\n",
      "total step : 321 \n",
      "error : 1.351747, accuarcy : 0.679340\n",
      "total step : 322 \n",
      "error : 1.348115, accuarcy : 0.680340\n",
      "total step : 323 \n",
      "error : 1.344501, accuarcy : 0.680840\n",
      "total step : 324 \n",
      "error : 1.340907, accuarcy : 0.681341\n",
      "total step : 325 \n",
      "error : 1.337331, accuarcy : 0.681341\n",
      "total step : 326 \n",
      "error : 1.333773, accuarcy : 0.681341\n",
      "total step : 327 \n",
      "error : 1.330234, accuarcy : 0.682341\n",
      "total step : 328 \n",
      "error : 1.326713, accuarcy : 0.683842\n",
      "total step : 329 \n",
      "error : 1.323210, accuarcy : 0.685343\n",
      "total step : 330 \n",
      "error : 1.319725, accuarcy : 0.685843\n",
      "total step : 331 \n",
      "error : 1.316259, accuarcy : 0.686343\n",
      "total step : 332 \n",
      "error : 1.312810, accuarcy : 0.687344\n",
      "total step : 333 \n",
      "error : 1.309378, accuarcy : 0.686843\n",
      "total step : 334 \n",
      "error : 1.305964, accuarcy : 0.687344\n",
      "total step : 335 \n",
      "error : 1.302568, accuarcy : 0.687844\n",
      "total step : 336 \n",
      "error : 1.299189, accuarcy : 0.687844\n",
      "total step : 337 \n",
      "error : 1.295827, accuarcy : 0.689345\n",
      "total step : 338 \n",
      "error : 1.292482, accuarcy : 0.688844\n",
      "total step : 339 \n",
      "error : 1.289154, accuarcy : 0.689345\n",
      "total step : 340 \n",
      "error : 1.285842, accuarcy : 0.689845\n",
      "total step : 341 \n",
      "error : 1.282548, accuarcy : 0.690845\n",
      "total step : 342 \n",
      "error : 1.279270, accuarcy : 0.691346\n",
      "total step : 343 \n",
      "error : 1.276009, accuarcy : 0.690845\n",
      "total step : 344 \n",
      "error : 1.272764, accuarcy : 0.691846\n",
      "total step : 345 \n",
      "error : 1.269535, accuarcy : 0.691846\n",
      "total step : 346 \n",
      "error : 1.266322, accuarcy : 0.691846\n",
      "total step : 347 \n",
      "error : 1.263126, accuarcy : 0.692346\n",
      "total step : 348 \n",
      "error : 1.259945, accuarcy : 0.692846\n",
      "total step : 349 \n",
      "error : 1.256780, accuarcy : 0.693847\n",
      "total step : 350 \n",
      "error : 1.253631, accuarcy : 0.694347\n",
      "total step : 351 \n",
      "error : 1.250498, accuarcy : 0.694347\n",
      "total step : 352 \n",
      "error : 1.247380, accuarcy : 0.694847\n",
      "total step : 353 \n",
      "error : 1.244277, accuarcy : 0.694847\n",
      "total step : 354 \n",
      "error : 1.241190, accuarcy : 0.695848\n",
      "total step : 355 \n",
      "error : 1.238118, accuarcy : 0.695848\n",
      "total step : 356 \n",
      "error : 1.235061, accuarcy : 0.696348\n",
      "total step : 357 \n",
      "error : 1.232019, accuarcy : 0.696848\n",
      "total step : 358 \n",
      "error : 1.228992, accuarcy : 0.697849\n",
      "total step : 359 \n",
      "error : 1.225979, accuarcy : 0.698349\n",
      "total step : 360 \n",
      "error : 1.222982, accuarcy : 0.698349\n",
      "total step : 361 \n",
      "error : 1.219999, accuarcy : 0.698349\n",
      "total step : 362 \n",
      "error : 1.217030, accuarcy : 0.698349\n",
      "total step : 363 \n",
      "error : 1.214076, accuarcy : 0.698349\n",
      "total step : 364 \n",
      "error : 1.211136, accuarcy : 0.698849\n",
      "total step : 365 \n",
      "error : 1.208211, accuarcy : 0.699350\n",
      "total step : 366 \n",
      "error : 1.205299, accuarcy : 0.699350\n",
      "total step : 367 \n",
      "error : 1.202402, accuarcy : 0.699850\n",
      "total step : 368 \n",
      "error : 1.199519, accuarcy : 0.700850\n",
      "total step : 369 \n",
      "error : 1.196649, accuarcy : 0.700850\n",
      "total step : 370 \n",
      "error : 1.193793, accuarcy : 0.701351\n",
      "total step : 371 \n",
      "error : 1.190951, accuarcy : 0.701851\n",
      "total step : 372 \n",
      "error : 1.188123, accuarcy : 0.702851\n",
      "total step : 373 \n",
      "error : 1.185308, accuarcy : 0.702351\n",
      "total step : 374 \n",
      "error : 1.182506, accuarcy : 0.702851\n",
      "total step : 375 \n",
      "error : 1.179718, accuarcy : 0.703352\n",
      "total step : 376 \n",
      "error : 1.176943, accuarcy : 0.703352\n",
      "total step : 377 \n",
      "error : 1.174181, accuarcy : 0.703352\n",
      "total step : 378 \n",
      "error : 1.171432, accuarcy : 0.703352\n",
      "total step : 379 \n",
      "error : 1.168696, accuarcy : 0.704352\n",
      "total step : 380 \n",
      "error : 1.165973, accuarcy : 0.704352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 381 \n",
      "error : 1.163263, accuarcy : 0.705853\n",
      "total step : 382 \n",
      "error : 1.160566, accuarcy : 0.706353\n",
      "total step : 383 \n",
      "error : 1.157882, accuarcy : 0.706853\n",
      "total step : 384 \n",
      "error : 1.155210, accuarcy : 0.708854\n",
      "total step : 385 \n",
      "error : 1.152550, accuarcy : 0.710355\n",
      "total step : 386 \n",
      "error : 1.149903, accuarcy : 0.710355\n",
      "total step : 387 \n",
      "error : 1.147269, accuarcy : 0.711856\n",
      "total step : 388 \n",
      "error : 1.144646, accuarcy : 0.713357\n",
      "total step : 389 \n",
      "error : 1.142036, accuarcy : 0.713357\n",
      "total step : 390 \n",
      "error : 1.139438, accuarcy : 0.713357\n",
      "total step : 391 \n",
      "error : 1.136852, accuarcy : 0.713357\n",
      "total step : 392 \n",
      "error : 1.134279, accuarcy : 0.712856\n",
      "total step : 393 \n",
      "error : 1.131717, accuarcy : 0.714857\n",
      "total step : 394 \n",
      "error : 1.129167, accuarcy : 0.715358\n",
      "total step : 395 \n",
      "error : 1.126628, accuarcy : 0.715858\n",
      "total step : 396 \n",
      "error : 1.124102, accuarcy : 0.715858\n",
      "total step : 397 \n",
      "error : 1.121587, accuarcy : 0.716358\n",
      "total step : 398 \n",
      "error : 1.119083, accuarcy : 0.716858\n",
      "total step : 399 \n",
      "error : 1.116592, accuarcy : 0.717859\n",
      "total step : 400 \n",
      "error : 1.114111, accuarcy : 0.718359\n",
      "total step : 401 \n",
      "error : 1.111642, accuarcy : 0.718859\n",
      "total step : 402 \n",
      "error : 1.109184, accuarcy : 0.719360\n",
      "total step : 403 \n",
      "error : 1.106738, accuarcy : 0.719860\n",
      "total step : 404 \n",
      "error : 1.104303, accuarcy : 0.719360\n",
      "total step : 405 \n",
      "error : 1.101879, accuarcy : 0.719860\n",
      "total step : 406 \n",
      "error : 1.099465, accuarcy : 0.719860\n",
      "total step : 407 \n",
      "error : 1.097063, accuarcy : 0.720360\n",
      "total step : 408 \n",
      "error : 1.094672, accuarcy : 0.720860\n",
      "total step : 409 \n",
      "error : 1.092292, accuarcy : 0.720860\n",
      "total step : 410 \n",
      "error : 1.089922, accuarcy : 0.721361\n",
      "total step : 411 \n",
      "error : 1.087564, accuarcy : 0.721361\n",
      "total step : 412 \n",
      "error : 1.085215, accuarcy : 0.722861\n",
      "total step : 413 \n",
      "error : 1.082878, accuarcy : 0.723362\n",
      "total step : 414 \n",
      "error : 1.080551, accuarcy : 0.723362\n",
      "total step : 415 \n",
      "error : 1.078235, accuarcy : 0.723362\n",
      "total step : 416 \n",
      "error : 1.075929, accuarcy : 0.723362\n",
      "total step : 417 \n",
      "error : 1.073633, accuarcy : 0.723362\n",
      "total step : 418 \n",
      "error : 1.071348, accuarcy : 0.723862\n",
      "total step : 419 \n",
      "error : 1.069073, accuarcy : 0.724362\n",
      "total step : 420 \n",
      "error : 1.066808, accuarcy : 0.724362\n",
      "total step : 421 \n",
      "error : 1.064553, accuarcy : 0.724362\n",
      "total step : 422 \n",
      "error : 1.062309, accuarcy : 0.724862\n",
      "total step : 423 \n",
      "error : 1.060074, accuarcy : 0.724862\n",
      "total step : 424 \n",
      "error : 1.057849, accuarcy : 0.725363\n",
      "total step : 425 \n",
      "error : 1.055635, accuarcy : 0.725363\n",
      "total step : 426 \n",
      "error : 1.053430, accuarcy : 0.725863\n",
      "total step : 427 \n",
      "error : 1.051235, accuarcy : 0.726363\n",
      "total step : 428 \n",
      "error : 1.049050, accuarcy : 0.726363\n",
      "total step : 429 \n",
      "error : 1.046875, accuarcy : 0.726363\n",
      "total step : 430 \n",
      "error : 1.044709, accuarcy : 0.726863\n",
      "total step : 431 \n",
      "error : 1.042553, accuarcy : 0.728364\n",
      "total step : 432 \n",
      "error : 1.040406, accuarcy : 0.730365\n",
      "total step : 433 \n",
      "error : 1.038269, accuarcy : 0.730865\n",
      "total step : 434 \n",
      "error : 1.036141, accuarcy : 0.731866\n",
      "total step : 435 \n",
      "error : 1.034023, accuarcy : 0.731866\n",
      "total step : 436 \n",
      "error : 1.031914, accuarcy : 0.732866\n",
      "total step : 437 \n",
      "error : 1.029815, accuarcy : 0.733867\n",
      "total step : 438 \n",
      "error : 1.027724, accuarcy : 0.733867\n",
      "total step : 439 \n",
      "error : 1.025643, accuarcy : 0.734367\n",
      "total step : 440 \n",
      "error : 1.023571, accuarcy : 0.734867\n",
      "total step : 441 \n",
      "error : 1.021509, accuarcy : 0.734867\n",
      "total step : 442 \n",
      "error : 1.019455, accuarcy : 0.734867\n",
      "total step : 443 \n",
      "error : 1.017410, accuarcy : 0.734867\n",
      "total step : 444 \n",
      "error : 1.015374, accuarcy : 0.734867\n",
      "total step : 445 \n",
      "error : 1.013347, accuarcy : 0.734867\n",
      "total step : 446 \n",
      "error : 1.011329, accuarcy : 0.734867\n",
      "total step : 447 \n",
      "error : 1.009320, accuarcy : 0.735868\n",
      "total step : 448 \n",
      "error : 1.007319, accuarcy : 0.736868\n",
      "total step : 449 \n",
      "error : 1.005328, accuarcy : 0.737369\n",
      "total step : 450 \n",
      "error : 1.003345, accuarcy : 0.737369\n",
      "total step : 451 \n",
      "error : 1.001370, accuarcy : 0.738369\n",
      "total step : 452 \n",
      "error : 0.999404, accuarcy : 0.738869\n",
      "total step : 453 \n",
      "error : 0.997447, accuarcy : 0.738869\n",
      "total step : 454 \n",
      "error : 0.995498, accuarcy : 0.739370\n",
      "total step : 455 \n",
      "error : 0.993558, accuarcy : 0.739370\n",
      "total step : 456 \n",
      "error : 0.991626, accuarcy : 0.740370\n",
      "total step : 457 \n",
      "error : 0.989702, accuarcy : 0.741371\n",
      "total step : 458 \n",
      "error : 0.987787, accuarcy : 0.741871\n",
      "total step : 459 \n",
      "error : 0.985880, accuarcy : 0.741871\n",
      "total step : 460 \n",
      "error : 0.983981, accuarcy : 0.741871\n",
      "total step : 461 \n",
      "error : 0.982090, accuarcy : 0.742871\n",
      "total step : 462 \n",
      "error : 0.980207, accuarcy : 0.743372\n",
      "total step : 463 \n",
      "error : 0.978333, accuarcy : 0.743872\n",
      "total step : 464 \n",
      "error : 0.976467, accuarcy : 0.743872\n",
      "total step : 465 \n",
      "error : 0.974608, accuarcy : 0.743872\n",
      "total step : 466 \n",
      "error : 0.972758, accuarcy : 0.744372\n",
      "total step : 467 \n",
      "error : 0.970915, accuarcy : 0.744372\n",
      "total step : 468 \n",
      "error : 0.969080, accuarcy : 0.744372\n",
      "total step : 469 \n",
      "error : 0.967254, accuarcy : 0.745373\n",
      "total step : 470 \n",
      "error : 0.965434, accuarcy : 0.745873\n",
      "total step : 471 \n",
      "error : 0.963623, accuarcy : 0.746873\n",
      "total step : 472 \n",
      "error : 0.961820, accuarcy : 0.746873\n",
      "total step : 473 \n",
      "error : 0.960024, accuarcy : 0.747374\n",
      "total step : 474 \n",
      "error : 0.958235, accuarcy : 0.747874\n",
      "total step : 475 \n",
      "error : 0.956454, accuarcy : 0.747874\n",
      "total step : 476 \n",
      "error : 0.954681, accuarcy : 0.748874\n",
      "total step : 477 \n",
      "error : 0.952916, accuarcy : 0.749875\n",
      "total step : 478 \n",
      "error : 0.951157, accuarcy : 0.750375\n",
      "total step : 479 \n",
      "error : 0.949406, accuarcy : 0.750375\n",
      "total step : 480 \n",
      "error : 0.947663, accuarcy : 0.750875\n",
      "total step : 481 \n",
      "error : 0.945927, accuarcy : 0.750875\n",
      "total step : 482 \n",
      "error : 0.944198, accuarcy : 0.751376\n",
      "total step : 483 \n",
      "error : 0.942476, accuarcy : 0.751376\n",
      "total step : 484 \n",
      "error : 0.940762, accuarcy : 0.751876\n",
      "total step : 485 \n",
      "error : 0.939055, accuarcy : 0.752876\n",
      "total step : 486 \n",
      "error : 0.937355, accuarcy : 0.752876\n",
      "total step : 487 \n",
      "error : 0.935662, accuarcy : 0.752876\n",
      "total step : 488 \n",
      "error : 0.933976, accuarcy : 0.752876\n",
      "total step : 489 \n",
      "error : 0.932297, accuarcy : 0.753377\n",
      "total step : 490 \n",
      "error : 0.930625, accuarcy : 0.752876\n",
      "total step : 491 \n",
      "error : 0.928960, accuarcy : 0.752876\n",
      "total step : 492 \n",
      "error : 0.927302, accuarcy : 0.753377\n",
      "total step : 493 \n",
      "error : 0.925651, accuarcy : 0.753877\n",
      "total step : 494 \n",
      "error : 0.924007, accuarcy : 0.755378\n",
      "total step : 495 \n",
      "error : 0.922369, accuarcy : 0.754877\n",
      "total step : 496 \n",
      "error : 0.920739, accuarcy : 0.754877\n",
      "total step : 497 \n",
      "error : 0.919114, accuarcy : 0.755878\n",
      "total step : 498 \n",
      "error : 0.917497, accuarcy : 0.755878\n",
      "total step : 499 \n",
      "error : 0.915886, accuarcy : 0.755878\n",
      "total step : 500 \n",
      "error : 0.914282, accuarcy : 0.755878\n",
      "total step : 501 \n",
      "error : 0.912685, accuarcy : 0.756378\n",
      "total step : 502 \n",
      "error : 0.911094, accuarcy : 0.758379\n",
      "total step : 503 \n",
      "error : 0.909509, accuarcy : 0.758379\n",
      "total step : 504 \n",
      "error : 0.907932, accuarcy : 0.758379\n",
      "total step : 505 \n",
      "error : 0.906360, accuarcy : 0.758379\n",
      "total step : 506 \n",
      "error : 0.904795, accuarcy : 0.758379\n",
      "total step : 507 \n",
      "error : 0.903236, accuarcy : 0.757879\n",
      "total step : 508 \n",
      "error : 0.901684, accuarcy : 0.758379\n",
      "total step : 509 \n",
      "error : 0.900138, accuarcy : 0.757879\n",
      "total step : 510 \n",
      "error : 0.898598, accuarcy : 0.758879\n",
      "total step : 511 \n",
      "error : 0.897064, accuarcy : 0.758879\n",
      "total step : 512 \n",
      "error : 0.895537, accuarcy : 0.758879\n",
      "total step : 513 \n",
      "error : 0.894015, accuarcy : 0.759380\n",
      "total step : 514 \n",
      "error : 0.892500, accuarcy : 0.759380\n",
      "total step : 515 \n",
      "error : 0.890991, accuarcy : 0.759380\n",
      "total step : 516 \n",
      "error : 0.889488, accuarcy : 0.759880\n",
      "total step : 517 \n",
      "error : 0.887992, accuarcy : 0.760880\n",
      "total step : 518 \n",
      "error : 0.886501, accuarcy : 0.760880\n",
      "total step : 519 \n",
      "error : 0.885016, accuarcy : 0.760880\n",
      "total step : 520 \n",
      "error : 0.883537, accuarcy : 0.761881\n",
      "total step : 521 \n",
      "error : 0.882064, accuarcy : 0.761881\n",
      "total step : 522 \n",
      "error : 0.880596, accuarcy : 0.761881\n",
      "total step : 523 \n",
      "error : 0.879135, accuarcy : 0.761881\n",
      "total step : 524 \n",
      "error : 0.877680, accuarcy : 0.761881\n",
      "total step : 525 \n",
      "error : 0.876230, accuarcy : 0.761881\n",
      "total step : 526 \n",
      "error : 0.874786, accuarcy : 0.762881\n",
      "total step : 527 \n",
      "error : 0.873348, accuarcy : 0.762881\n",
      "total step : 528 \n",
      "error : 0.871915, accuarcy : 0.762881\n",
      "total step : 529 \n",
      "error : 0.870488, accuarcy : 0.762881\n",
      "total step : 530 \n",
      "error : 0.869067, accuarcy : 0.762881\n",
      "total step : 531 \n",
      "error : 0.867651, accuarcy : 0.763382\n",
      "total step : 532 \n",
      "error : 0.866241, accuarcy : 0.763882\n",
      "total step : 533 \n",
      "error : 0.864837, accuarcy : 0.763882\n",
      "total step : 534 \n",
      "error : 0.863438, accuarcy : 0.763382\n",
      "total step : 535 \n",
      "error : 0.862044, accuarcy : 0.763882\n",
      "total step : 536 \n",
      "error : 0.860656, accuarcy : 0.764382\n",
      "total step : 537 \n",
      "error : 0.859274, accuarcy : 0.764882\n",
      "total step : 538 \n",
      "error : 0.857896, accuarcy : 0.765383\n",
      "total step : 539 \n",
      "error : 0.856525, accuarcy : 0.766383\n",
      "total step : 540 \n",
      "error : 0.855158, accuarcy : 0.766383\n",
      "total step : 541 \n",
      "error : 0.853797, accuarcy : 0.767384\n",
      "total step : 542 \n",
      "error : 0.852441, accuarcy : 0.767384\n",
      "total step : 543 \n",
      "error : 0.851091, accuarcy : 0.767884\n",
      "total step : 544 \n",
      "error : 0.849745, accuarcy : 0.767884\n",
      "total step : 545 \n",
      "error : 0.848405, accuarcy : 0.768384\n",
      "total step : 546 \n",
      "error : 0.847070, accuarcy : 0.768884\n",
      "total step : 547 \n",
      "error : 0.845741, accuarcy : 0.769385\n",
      "total step : 548 \n",
      "error : 0.844416, accuarcy : 0.770385\n",
      "total step : 549 \n",
      "error : 0.843097, accuarcy : 0.771386\n",
      "total step : 550 \n",
      "error : 0.841782, accuarcy : 0.771886\n",
      "total step : 551 \n",
      "error : 0.840473, accuarcy : 0.771886\n",
      "total step : 552 \n",
      "error : 0.839169, accuarcy : 0.771886\n",
      "total step : 553 \n",
      "error : 0.837869, accuarcy : 0.771886\n",
      "total step : 554 \n",
      "error : 0.836575, accuarcy : 0.771886\n",
      "total step : 555 \n",
      "error : 0.835286, accuarcy : 0.772386\n",
      "total step : 556 \n",
      "error : 0.834001, accuarcy : 0.773387\n",
      "total step : 557 \n",
      "error : 0.832722, accuarcy : 0.773387\n",
      "total step : 558 \n",
      "error : 0.831447, accuarcy : 0.773887\n",
      "total step : 559 \n",
      "error : 0.830178, accuarcy : 0.774387\n",
      "total step : 560 \n",
      "error : 0.828913, accuarcy : 0.774887\n",
      "total step : 561 \n",
      "error : 0.827653, accuarcy : 0.775888\n",
      "total step : 562 \n",
      "error : 0.826397, accuarcy : 0.775888\n",
      "total step : 563 \n",
      "error : 0.825147, accuarcy : 0.775388\n",
      "total step : 564 \n",
      "error : 0.823901, accuarcy : 0.775388\n",
      "total step : 565 \n",
      "error : 0.822660, accuarcy : 0.775388\n",
      "total step : 566 \n",
      "error : 0.821424, accuarcy : 0.775388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 567 \n",
      "error : 0.820192, accuarcy : 0.775888\n",
      "total step : 568 \n",
      "error : 0.818965, accuarcy : 0.775888\n",
      "total step : 569 \n",
      "error : 0.817743, accuarcy : 0.776388\n",
      "total step : 570 \n",
      "error : 0.816525, accuarcy : 0.776888\n",
      "total step : 571 \n",
      "error : 0.815312, accuarcy : 0.777389\n",
      "total step : 572 \n",
      "error : 0.814103, accuarcy : 0.777889\n",
      "total step : 573 \n",
      "error : 0.812899, accuarcy : 0.778389\n",
      "total step : 574 \n",
      "error : 0.811699, accuarcy : 0.778389\n",
      "total step : 575 \n",
      "error : 0.810504, accuarcy : 0.778389\n",
      "total step : 576 \n",
      "error : 0.809314, accuarcy : 0.778389\n",
      "total step : 577 \n",
      "error : 0.808128, accuarcy : 0.778889\n",
      "total step : 578 \n",
      "error : 0.806946, accuarcy : 0.779390\n",
      "total step : 579 \n",
      "error : 0.805769, accuarcy : 0.779890\n",
      "total step : 580 \n",
      "error : 0.804596, accuarcy : 0.779890\n",
      "total step : 581 \n",
      "error : 0.803427, accuarcy : 0.780390\n",
      "total step : 582 \n",
      "error : 0.802263, accuarcy : 0.780390\n",
      "total step : 583 \n",
      "error : 0.801103, accuarcy : 0.781391\n",
      "total step : 584 \n",
      "error : 0.799948, accuarcy : 0.782391\n",
      "total step : 585 \n",
      "error : 0.798796, accuarcy : 0.782391\n",
      "total step : 586 \n",
      "error : 0.797649, accuarcy : 0.782391\n",
      "total step : 587 \n",
      "error : 0.796507, accuarcy : 0.782891\n",
      "total step : 588 \n",
      "error : 0.795368, accuarcy : 0.782891\n",
      "total step : 589 \n",
      "error : 0.794234, accuarcy : 0.782891\n",
      "total step : 590 \n",
      "error : 0.793104, accuarcy : 0.782891\n",
      "total step : 591 \n",
      "error : 0.791978, accuarcy : 0.782891\n",
      "total step : 592 \n",
      "error : 0.790856, accuarcy : 0.783392\n",
      "total step : 593 \n",
      "error : 0.789738, accuarcy : 0.783392\n",
      "total step : 594 \n",
      "error : 0.788625, accuarcy : 0.783392\n",
      "total step : 595 \n",
      "error : 0.787515, accuarcy : 0.783892\n",
      "total step : 596 \n",
      "error : 0.786410, accuarcy : 0.783892\n",
      "total step : 597 \n",
      "error : 0.785308, accuarcy : 0.783892\n",
      "total step : 598 \n",
      "error : 0.784211, accuarcy : 0.783892\n",
      "total step : 599 \n",
      "error : 0.783118, accuarcy : 0.783892\n",
      "total step : 600 \n",
      "error : 0.782028, accuarcy : 0.783892\n",
      "total step : 601 \n",
      "error : 0.780943, accuarcy : 0.783892\n",
      "total step : 602 \n",
      "error : 0.779862, accuarcy : 0.783892\n",
      "total step : 603 \n",
      "error : 0.778784, accuarcy : 0.783892\n",
      "total step : 604 \n",
      "error : 0.777711, accuarcy : 0.784892\n",
      "total step : 605 \n",
      "error : 0.776641, accuarcy : 0.785393\n",
      "total step : 606 \n",
      "error : 0.775575, accuarcy : 0.785393\n",
      "total step : 607 \n",
      "error : 0.774513, accuarcy : 0.785893\n",
      "total step : 608 \n",
      "error : 0.773455, accuarcy : 0.785893\n",
      "total step : 609 \n",
      "error : 0.772401, accuarcy : 0.785893\n",
      "total step : 610 \n",
      "error : 0.771350, accuarcy : 0.785893\n",
      "total step : 611 \n",
      "error : 0.770304, accuarcy : 0.785893\n",
      "total step : 612 \n",
      "error : 0.769261, accuarcy : 0.786893\n",
      "total step : 613 \n",
      "error : 0.768222, accuarcy : 0.786393\n",
      "total step : 614 \n",
      "error : 0.767186, accuarcy : 0.786393\n",
      "total step : 615 \n",
      "error : 0.766155, accuarcy : 0.786393\n",
      "total step : 616 \n",
      "error : 0.765127, accuarcy : 0.786893\n",
      "total step : 617 \n",
      "error : 0.764102, accuarcy : 0.786893\n",
      "total step : 618 \n",
      "error : 0.763082, accuarcy : 0.786893\n",
      "total step : 619 \n",
      "error : 0.762065, accuarcy : 0.786893\n",
      "total step : 620 \n",
      "error : 0.761051, accuarcy : 0.787394\n",
      "total step : 621 \n",
      "error : 0.760042, accuarcy : 0.787894\n",
      "total step : 622 \n",
      "error : 0.759036, accuarcy : 0.787894\n",
      "total step : 623 \n",
      "error : 0.758033, accuarcy : 0.787394\n",
      "total step : 624 \n",
      "error : 0.757034, accuarcy : 0.787894\n",
      "total step : 625 \n",
      "error : 0.756039, accuarcy : 0.787894\n",
      "total step : 626 \n",
      "error : 0.755047, accuarcy : 0.787894\n",
      "total step : 627 \n",
      "error : 0.754058, accuarcy : 0.787894\n",
      "total step : 628 \n",
      "error : 0.753073, accuarcy : 0.787894\n",
      "total step : 629 \n",
      "error : 0.752092, accuarcy : 0.787894\n",
      "total step : 630 \n",
      "error : 0.751114, accuarcy : 0.787894\n",
      "total step : 631 \n",
      "error : 0.750139, accuarcy : 0.788394\n",
      "total step : 632 \n",
      "error : 0.749168, accuarcy : 0.788394\n",
      "total step : 633 \n",
      "error : 0.748201, accuarcy : 0.788894\n",
      "total step : 634 \n",
      "error : 0.747236, accuarcy : 0.788894\n",
      "total step : 635 \n",
      "error : 0.746276, accuarcy : 0.789395\n",
      "total step : 636 \n",
      "error : 0.745318, accuarcy : 0.789395\n",
      "total step : 637 \n",
      "error : 0.744364, accuarcy : 0.789395\n",
      "total step : 638 \n",
      "error : 0.743413, accuarcy : 0.790395\n",
      "total step : 639 \n",
      "error : 0.742465, accuarcy : 0.790395\n",
      "total step : 640 \n",
      "error : 0.741521, accuarcy : 0.790895\n",
      "total step : 641 \n",
      "error : 0.740580, accuarcy : 0.791396\n",
      "total step : 642 \n",
      "error : 0.739643, accuarcy : 0.791896\n",
      "total step : 643 \n",
      "error : 0.738708, accuarcy : 0.791896\n",
      "total step : 644 \n",
      "error : 0.737777, accuarcy : 0.791396\n",
      "total step : 645 \n",
      "error : 0.736849, accuarcy : 0.791396\n",
      "total step : 646 \n",
      "error : 0.735925, accuarcy : 0.792396\n",
      "total step : 647 \n",
      "error : 0.735003, accuarcy : 0.792396\n",
      "total step : 648 \n",
      "error : 0.734085, accuarcy : 0.792396\n",
      "total step : 649 \n",
      "error : 0.733170, accuarcy : 0.792396\n",
      "total step : 650 \n",
      "error : 0.732258, accuarcy : 0.792396\n",
      "total step : 651 \n",
      "error : 0.731349, accuarcy : 0.792396\n",
      "total step : 652 \n",
      "error : 0.730443, accuarcy : 0.792396\n",
      "total step : 653 \n",
      "error : 0.729540, accuarcy : 0.792396\n",
      "total step : 654 \n",
      "error : 0.728641, accuarcy : 0.792396\n",
      "total step : 655 \n",
      "error : 0.727744, accuarcy : 0.792896\n",
      "total step : 656 \n",
      "error : 0.726851, accuarcy : 0.792896\n",
      "total step : 657 \n",
      "error : 0.725961, accuarcy : 0.793397\n",
      "total step : 658 \n",
      "error : 0.725073, accuarcy : 0.793397\n",
      "total step : 659 \n",
      "error : 0.724189, accuarcy : 0.793397\n",
      "total step : 660 \n",
      "error : 0.723308, accuarcy : 0.793897\n",
      "total step : 661 \n",
      "error : 0.722429, accuarcy : 0.793897\n",
      "total step : 662 \n",
      "error : 0.721554, accuarcy : 0.793397\n",
      "total step : 663 \n",
      "error : 0.720682, accuarcy : 0.793397\n",
      "total step : 664 \n",
      "error : 0.719812, accuarcy : 0.793397\n",
      "total step : 665 \n",
      "error : 0.718946, accuarcy : 0.793397\n",
      "total step : 666 \n",
      "error : 0.718082, accuarcy : 0.793397\n",
      "total step : 667 \n",
      "error : 0.717222, accuarcy : 0.793397\n",
      "total step : 668 \n",
      "error : 0.716364, accuarcy : 0.793897\n",
      "total step : 669 \n",
      "error : 0.715509, accuarcy : 0.793897\n",
      "total step : 670 \n",
      "error : 0.714657, accuarcy : 0.794397\n",
      "total step : 671 \n",
      "error : 0.713808, accuarcy : 0.794397\n",
      "total step : 672 \n",
      "error : 0.712962, accuarcy : 0.794897\n",
      "total step : 673 \n",
      "error : 0.712118, accuarcy : 0.794897\n",
      "total step : 674 \n",
      "error : 0.711278, accuarcy : 0.794897\n",
      "total step : 675 \n",
      "error : 0.710440, accuarcy : 0.794397\n",
      "total step : 676 \n",
      "error : 0.709605, accuarcy : 0.794897\n",
      "total step : 677 \n",
      "error : 0.708772, accuarcy : 0.795398\n",
      "total step : 678 \n",
      "error : 0.707943, accuarcy : 0.795398\n",
      "total step : 679 \n",
      "error : 0.707116, accuarcy : 0.795898\n",
      "total step : 680 \n",
      "error : 0.706292, accuarcy : 0.796398\n",
      "total step : 681 \n",
      "error : 0.705470, accuarcy : 0.796398\n",
      "total step : 682 \n",
      "error : 0.704652, accuarcy : 0.796398\n",
      "total step : 683 \n",
      "error : 0.703836, accuarcy : 0.796898\n",
      "total step : 684 \n",
      "error : 0.703023, accuarcy : 0.797899\n",
      "total step : 685 \n",
      "error : 0.702212, accuarcy : 0.797899\n",
      "total step : 686 \n",
      "error : 0.701404, accuarcy : 0.798899\n",
      "total step : 687 \n",
      "error : 0.700599, accuarcy : 0.798899\n",
      "total step : 688 \n",
      "error : 0.699796, accuarcy : 0.798899\n",
      "total step : 689 \n",
      "error : 0.698996, accuarcy : 0.799900\n",
      "total step : 690 \n",
      "error : 0.698199, accuarcy : 0.799900\n",
      "total step : 691 \n",
      "error : 0.697404, accuarcy : 0.800400\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for number 5\n",
      "total step : 1 \n",
      "error : 3.376899, accuarcy : 0.508254\n",
      "total step : 2 \n",
      "error : 3.351898, accuarcy : 0.507254\n",
      "total step : 3 \n",
      "error : 3.327558, accuarcy : 0.507254\n",
      "total step : 4 \n",
      "error : 3.303867, accuarcy : 0.506753\n",
      "total step : 5 \n",
      "error : 3.280815, accuarcy : 0.506253\n",
      "total step : 6 \n",
      "error : 3.258390, accuarcy : 0.507254\n",
      "total step : 7 \n",
      "error : 3.236578, accuarcy : 0.507254\n",
      "total step : 8 \n",
      "error : 3.215367, accuarcy : 0.507254\n",
      "total step : 9 \n",
      "error : 3.194742, accuarcy : 0.507254\n",
      "total step : 10 \n",
      "error : 3.174689, accuarcy : 0.507254\n",
      "total step : 11 \n",
      "error : 3.155193, accuarcy : 0.505753\n",
      "total step : 12 \n",
      "error : 3.136239, accuarcy : 0.505753\n",
      "total step : 13 \n",
      "error : 3.117813, accuarcy : 0.505753\n",
      "total step : 14 \n",
      "error : 3.099899, accuarcy : 0.503752\n",
      "total step : 15 \n",
      "error : 3.082481, accuarcy : 0.501751\n",
      "total step : 16 \n",
      "error : 3.065547, accuarcy : 0.501751\n",
      "total step : 17 \n",
      "error : 3.049079, accuarcy : 0.502751\n",
      "total step : 18 \n",
      "error : 3.033065, accuarcy : 0.503252\n",
      "total step : 19 \n",
      "error : 3.017490, accuarcy : 0.501751\n",
      "total step : 20 \n",
      "error : 3.002341, accuarcy : 0.502251\n",
      "total step : 21 \n",
      "error : 2.987602, accuarcy : 0.501251\n",
      "total step : 22 \n",
      "error : 2.973262, accuarcy : 0.499750\n",
      "total step : 23 \n",
      "error : 2.959308, accuarcy : 0.500250\n",
      "total step : 24 \n",
      "error : 2.945726, accuarcy : 0.500750\n",
      "total step : 25 \n",
      "error : 2.932504, accuarcy : 0.500750\n",
      "total step : 26 \n",
      "error : 2.919630, accuarcy : 0.501751\n",
      "total step : 27 \n",
      "error : 2.907092, accuarcy : 0.501251\n",
      "total step : 28 \n",
      "error : 2.894879, accuarcy : 0.501751\n",
      "total step : 29 \n",
      "error : 2.882979, accuarcy : 0.504252\n",
      "total step : 30 \n",
      "error : 2.871382, accuarcy : 0.503252\n",
      "total step : 31 \n",
      "error : 2.860077, accuarcy : 0.502751\n",
      "total step : 32 \n",
      "error : 2.849053, accuarcy : 0.503252\n",
      "total step : 33 \n",
      "error : 2.838300, accuarcy : 0.502751\n",
      "total step : 34 \n",
      "error : 2.827808, accuarcy : 0.502251\n",
      "total step : 35 \n",
      "error : 2.817568, accuarcy : 0.501251\n",
      "total step : 36 \n",
      "error : 2.807570, accuarcy : 0.499250\n",
      "total step : 37 \n",
      "error : 2.797805, accuarcy : 0.499250\n",
      "total step : 38 \n",
      "error : 2.788264, accuarcy : 0.499750\n",
      "total step : 39 \n",
      "error : 2.778939, accuarcy : 0.500750\n",
      "total step : 40 \n",
      "error : 2.769821, accuarcy : 0.500750\n",
      "total step : 41 \n",
      "error : 2.760902, accuarcy : 0.501751\n",
      "total step : 42 \n",
      "error : 2.752175, accuarcy : 0.502251\n",
      "total step : 43 \n",
      "error : 2.743631, accuarcy : 0.502251\n",
      "total step : 44 \n",
      "error : 2.735264, accuarcy : 0.502751\n",
      "total step : 45 \n",
      "error : 2.727066, accuarcy : 0.502751\n",
      "total step : 46 \n",
      "error : 2.719030, accuarcy : 0.503252\n",
      "total step : 47 \n",
      "error : 2.711150, accuarcy : 0.503752\n",
      "total step : 48 \n",
      "error : 2.703420, accuarcy : 0.503752\n",
      "total step : 49 \n",
      "error : 2.695833, accuarcy : 0.501751\n",
      "total step : 50 \n",
      "error : 2.688384, accuarcy : 0.502751\n",
      "total step : 51 \n",
      "error : 2.681067, accuarcy : 0.501751\n",
      "total step : 52 \n",
      "error : 2.673876, accuarcy : 0.503752\n",
      "total step : 53 \n",
      "error : 2.666807, accuarcy : 0.505753\n",
      "total step : 54 \n",
      "error : 2.659854, accuarcy : 0.506253\n",
      "total step : 55 \n",
      "error : 2.653012, accuarcy : 0.505253\n",
      "total step : 56 \n",
      "error : 2.646278, accuarcy : 0.505253\n",
      "total step : 57 \n",
      "error : 2.639646, accuarcy : 0.506753\n",
      "total step : 58 \n",
      "error : 2.633112, accuarcy : 0.507254\n",
      "total step : 59 \n",
      "error : 2.626673, accuarcy : 0.507754\n",
      "total step : 60 \n",
      "error : 2.620324, accuarcy : 0.508754\n",
      "total step : 61 \n",
      "error : 2.614062, accuarcy : 0.509255\n",
      "total step : 62 \n",
      "error : 2.607883, accuarcy : 0.507254\n",
      "total step : 63 \n",
      "error : 2.601784, accuarcy : 0.506253\n",
      "total step : 64 \n",
      "error : 2.595762, accuarcy : 0.505253\n",
      "total step : 65 \n",
      "error : 2.589814, accuarcy : 0.506753\n",
      "total step : 66 \n",
      "error : 2.583936, accuarcy : 0.506753\n",
      "total step : 67 \n",
      "error : 2.578126, accuarcy : 0.506753\n",
      "total step : 68 \n",
      "error : 2.572382, accuarcy : 0.505253\n",
      "total step : 69 \n",
      "error : 2.566700, accuarcy : 0.505253\n",
      "total step : 70 \n",
      "error : 2.561079, accuarcy : 0.503252\n",
      "total step : 71 \n",
      "error : 2.555516, accuarcy : 0.503752\n",
      "total step : 72 \n",
      "error : 2.550008, accuarcy : 0.503752\n",
      "total step : 73 \n",
      "error : 2.544555, accuarcy : 0.503752\n",
      "total step : 74 \n",
      "error : 2.539153, accuarcy : 0.502751\n",
      "total step : 75 \n",
      "error : 2.533801, accuarcy : 0.502751\n",
      "total step : 76 \n",
      "error : 2.528496, accuarcy : 0.501751\n",
      "total step : 77 \n",
      "error : 2.523239, accuarcy : 0.501751\n",
      "total step : 78 \n",
      "error : 2.518025, accuarcy : 0.501251\n",
      "total step : 79 \n",
      "error : 2.512855, accuarcy : 0.502251\n",
      "total step : 80 \n",
      "error : 2.507727, accuarcy : 0.503752\n",
      "total step : 81 \n",
      "error : 2.502638, accuarcy : 0.503252\n",
      "total step : 82 \n",
      "error : 2.497589, accuarcy : 0.504752\n",
      "total step : 83 \n",
      "error : 2.492577, accuarcy : 0.505753\n",
      "total step : 84 \n",
      "error : 2.487601, accuarcy : 0.504752\n",
      "total step : 85 \n",
      "error : 2.482660, accuarcy : 0.505253\n",
      "total step : 86 \n",
      "error : 2.477753, accuarcy : 0.505253\n",
      "total step : 87 \n",
      "error : 2.472879, accuarcy : 0.504752\n",
      "total step : 88 \n",
      "error : 2.468037, accuarcy : 0.504752\n",
      "total step : 89 \n",
      "error : 2.463226, accuarcy : 0.504252\n",
      "total step : 90 \n",
      "error : 2.458444, accuarcy : 0.504252\n",
      "total step : 91 \n",
      "error : 2.453692, accuarcy : 0.504252\n",
      "total step : 92 \n",
      "error : 2.448968, accuarcy : 0.505753\n",
      "total step : 93 \n",
      "error : 2.444272, accuarcy : 0.506253\n",
      "total step : 94 \n",
      "error : 2.439602, accuarcy : 0.507254\n",
      "total step : 95 \n",
      "error : 2.434958, accuarcy : 0.508254\n",
      "total step : 96 \n",
      "error : 2.430340, accuarcy : 0.507254\n",
      "total step : 97 \n",
      "error : 2.425746, accuarcy : 0.506253\n",
      "total step : 98 \n",
      "error : 2.421176, accuarcy : 0.505753\n",
      "total step : 99 \n",
      "error : 2.416629, accuarcy : 0.506753\n",
      "total step : 100 \n",
      "error : 2.412106, accuarcy : 0.506753\n",
      "total step : 101 \n",
      "error : 2.407604, accuarcy : 0.507254\n",
      "total step : 102 \n",
      "error : 2.403125, accuarcy : 0.507254\n",
      "total step : 103 \n",
      "error : 2.398666, accuarcy : 0.507254\n",
      "total step : 104 \n",
      "error : 2.394229, accuarcy : 0.508254\n",
      "total step : 105 \n",
      "error : 2.389811, accuarcy : 0.508254\n",
      "total step : 106 \n",
      "error : 2.385414, accuarcy : 0.508754\n",
      "total step : 107 \n",
      "error : 2.381036, accuarcy : 0.509255\n",
      "total step : 108 \n",
      "error : 2.376678, accuarcy : 0.508754\n",
      "total step : 109 \n",
      "error : 2.372338, accuarcy : 0.509755\n",
      "total step : 110 \n",
      "error : 2.368016, accuarcy : 0.511256\n",
      "total step : 111 \n",
      "error : 2.363713, accuarcy : 0.511256\n",
      "total step : 112 \n",
      "error : 2.359427, accuarcy : 0.511256\n",
      "total step : 113 \n",
      "error : 2.355159, accuarcy : 0.511256\n",
      "total step : 114 \n",
      "error : 2.350908, accuarcy : 0.511256\n",
      "total step : 115 \n",
      "error : 2.346673, accuarcy : 0.511256\n",
      "total step : 116 \n",
      "error : 2.342456, accuarcy : 0.511256\n",
      "total step : 117 \n",
      "error : 2.338254, accuarcy : 0.511256\n",
      "total step : 118 \n",
      "error : 2.334069, accuarcy : 0.511756\n",
      "total step : 119 \n",
      "error : 2.329900, accuarcy : 0.512256\n",
      "total step : 120 \n",
      "error : 2.325746, accuarcy : 0.512756\n",
      "total step : 121 \n",
      "error : 2.321608, accuarcy : 0.513257\n",
      "total step : 122 \n",
      "error : 2.317485, accuarcy : 0.513757\n",
      "total step : 123 \n",
      "error : 2.313377, accuarcy : 0.513257\n",
      "total step : 124 \n",
      "error : 2.309284, accuarcy : 0.514257\n",
      "total step : 125 \n",
      "error : 2.305205, accuarcy : 0.515758\n",
      "total step : 126 \n",
      "error : 2.301141, accuarcy : 0.515758\n",
      "total step : 127 \n",
      "error : 2.297091, accuarcy : 0.516258\n",
      "total step : 128 \n",
      "error : 2.293056, accuarcy : 0.516758\n",
      "total step : 129 \n",
      "error : 2.289034, accuarcy : 0.517259\n",
      "total step : 130 \n",
      "error : 2.285026, accuarcy : 0.516758\n",
      "total step : 131 \n",
      "error : 2.281033, accuarcy : 0.517759\n",
      "total step : 132 \n",
      "error : 2.277052, accuarcy : 0.518259\n",
      "total step : 133 \n",
      "error : 2.273085, accuarcy : 0.518759\n",
      "total step : 134 \n",
      "error : 2.269132, accuarcy : 0.519260\n",
      "total step : 135 \n",
      "error : 2.265192, accuarcy : 0.521261\n",
      "total step : 136 \n",
      "error : 2.261265, accuarcy : 0.522261\n",
      "total step : 137 \n",
      "error : 2.257351, accuarcy : 0.522261\n",
      "total step : 138 \n",
      "error : 2.253449, accuarcy : 0.522761\n",
      "total step : 139 \n",
      "error : 2.249561, accuarcy : 0.522761\n",
      "total step : 140 \n",
      "error : 2.245685, accuarcy : 0.523262\n",
      "total step : 141 \n",
      "error : 2.241822, accuarcy : 0.525263\n",
      "total step : 142 \n",
      "error : 2.237972, accuarcy : 0.525763\n",
      "total step : 143 \n",
      "error : 2.234134, accuarcy : 0.526263\n",
      "total step : 144 \n",
      "error : 2.230308, accuarcy : 0.526263\n",
      "total step : 145 \n",
      "error : 2.226495, accuarcy : 0.526763\n",
      "total step : 146 \n",
      "error : 2.222693, accuarcy : 0.528264\n",
      "total step : 147 \n",
      "error : 2.218904, accuarcy : 0.528764\n",
      "total step : 148 \n",
      "error : 2.215127, accuarcy : 0.529765\n",
      "total step : 149 \n",
      "error : 2.211362, accuarcy : 0.530765\n",
      "total step : 150 \n",
      "error : 2.207609, accuarcy : 0.531266\n",
      "total step : 151 \n",
      "error : 2.203867, accuarcy : 0.531266\n",
      "total step : 152 \n",
      "error : 2.200138, accuarcy : 0.531266\n",
      "total step : 153 \n",
      "error : 2.196420, accuarcy : 0.531266\n",
      "total step : 154 \n",
      "error : 2.192714, accuarcy : 0.532266\n",
      "total step : 155 \n",
      "error : 2.189019, accuarcy : 0.532766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 156 \n",
      "error : 2.185336, accuarcy : 0.532766\n",
      "total step : 157 \n",
      "error : 2.181664, accuarcy : 0.532766\n",
      "total step : 158 \n",
      "error : 2.178004, accuarcy : 0.534767\n",
      "total step : 159 \n",
      "error : 2.174355, accuarcy : 0.535268\n",
      "total step : 160 \n",
      "error : 2.170717, accuarcy : 0.536268\n",
      "total step : 161 \n",
      "error : 2.167091, accuarcy : 0.536768\n",
      "total step : 162 \n",
      "error : 2.163475, accuarcy : 0.537769\n",
      "total step : 163 \n",
      "error : 2.159871, accuarcy : 0.538269\n",
      "total step : 164 \n",
      "error : 2.156278, accuarcy : 0.538769\n",
      "total step : 165 \n",
      "error : 2.152696, accuarcy : 0.539770\n",
      "total step : 166 \n",
      "error : 2.149125, accuarcy : 0.540770\n",
      "total step : 167 \n",
      "error : 2.145565, accuarcy : 0.541271\n",
      "total step : 168 \n",
      "error : 2.142016, accuarcy : 0.541271\n",
      "total step : 169 \n",
      "error : 2.138478, accuarcy : 0.541271\n",
      "total step : 170 \n",
      "error : 2.134950, accuarcy : 0.541771\n",
      "total step : 171 \n",
      "error : 2.131433, accuarcy : 0.541271\n",
      "total step : 172 \n",
      "error : 2.127927, accuarcy : 0.542771\n",
      "total step : 173 \n",
      "error : 2.124432, accuarcy : 0.543272\n",
      "total step : 174 \n",
      "error : 2.120947, accuarcy : 0.544272\n",
      "total step : 175 \n",
      "error : 2.117473, accuarcy : 0.544772\n",
      "total step : 176 \n",
      "error : 2.114009, accuarcy : 0.544772\n",
      "total step : 177 \n",
      "error : 2.110556, accuarcy : 0.545273\n",
      "total step : 178 \n",
      "error : 2.107113, accuarcy : 0.545273\n",
      "total step : 179 \n",
      "error : 2.103681, accuarcy : 0.545273\n",
      "total step : 180 \n",
      "error : 2.100259, accuarcy : 0.546773\n",
      "total step : 181 \n",
      "error : 2.096848, accuarcy : 0.547274\n",
      "total step : 182 \n",
      "error : 2.093446, accuarcy : 0.547274\n",
      "total step : 183 \n",
      "error : 2.090055, accuarcy : 0.547274\n",
      "total step : 184 \n",
      "error : 2.086675, accuarcy : 0.547774\n",
      "total step : 185 \n",
      "error : 2.083304, accuarcy : 0.547774\n",
      "total step : 186 \n",
      "error : 2.079944, accuarcy : 0.548274\n",
      "total step : 187 \n",
      "error : 2.076593, accuarcy : 0.549275\n",
      "total step : 188 \n",
      "error : 2.073253, accuarcy : 0.549275\n",
      "total step : 189 \n",
      "error : 2.069923, accuarcy : 0.550275\n",
      "total step : 190 \n",
      "error : 2.066602, accuarcy : 0.550275\n",
      "total step : 191 \n",
      "error : 2.063292, accuarcy : 0.551776\n",
      "total step : 192 \n",
      "error : 2.059992, accuarcy : 0.552276\n",
      "total step : 193 \n",
      "error : 2.056701, accuarcy : 0.552276\n",
      "total step : 194 \n",
      "error : 2.053421, accuarcy : 0.553277\n",
      "total step : 195 \n",
      "error : 2.050150, accuarcy : 0.553777\n",
      "total step : 196 \n",
      "error : 2.046889, accuarcy : 0.553277\n",
      "total step : 197 \n",
      "error : 2.043638, accuarcy : 0.553777\n",
      "total step : 198 \n",
      "error : 2.040396, accuarcy : 0.554277\n",
      "total step : 199 \n",
      "error : 2.037164, accuarcy : 0.554777\n",
      "total step : 200 \n",
      "error : 2.033942, accuarcy : 0.554777\n",
      "total step : 201 \n",
      "error : 2.030729, accuarcy : 0.554277\n",
      "total step : 202 \n",
      "error : 2.027526, accuarcy : 0.555278\n",
      "total step : 203 \n",
      "error : 2.024332, accuarcy : 0.556278\n",
      "total step : 204 \n",
      "error : 2.021148, accuarcy : 0.557279\n",
      "total step : 205 \n",
      "error : 2.017973, accuarcy : 0.558279\n",
      "total step : 206 \n",
      "error : 2.014808, accuarcy : 0.558779\n",
      "total step : 207 \n",
      "error : 2.011652, accuarcy : 0.560280\n",
      "total step : 208 \n",
      "error : 2.008506, accuarcy : 0.560780\n",
      "total step : 209 \n",
      "error : 2.005369, accuarcy : 0.560780\n",
      "total step : 210 \n",
      "error : 2.002241, accuarcy : 0.561781\n",
      "total step : 211 \n",
      "error : 1.999122, accuarcy : 0.562281\n",
      "total step : 212 \n",
      "error : 1.996013, accuarcy : 0.562281\n",
      "total step : 213 \n",
      "error : 1.992912, accuarcy : 0.562781\n",
      "total step : 214 \n",
      "error : 1.989821, accuarcy : 0.562781\n",
      "total step : 215 \n",
      "error : 1.986739, accuarcy : 0.562281\n",
      "total step : 216 \n",
      "error : 1.983667, accuarcy : 0.562781\n",
      "total step : 217 \n",
      "error : 1.980603, accuarcy : 0.562781\n",
      "total step : 218 \n",
      "error : 1.977548, accuarcy : 0.562781\n",
      "total step : 219 \n",
      "error : 1.974502, accuarcy : 0.562781\n",
      "total step : 220 \n",
      "error : 1.971465, accuarcy : 0.564282\n",
      "total step : 221 \n",
      "error : 1.968437, accuarcy : 0.564782\n",
      "total step : 222 \n",
      "error : 1.965418, accuarcy : 0.564782\n",
      "total step : 223 \n",
      "error : 1.962408, accuarcy : 0.565283\n",
      "total step : 224 \n",
      "error : 1.959406, accuarcy : 0.565283\n",
      "total step : 225 \n",
      "error : 1.956414, accuarcy : 0.566783\n",
      "total step : 226 \n",
      "error : 1.953430, accuarcy : 0.567784\n",
      "total step : 227 \n",
      "error : 1.950455, accuarcy : 0.567784\n",
      "total step : 228 \n",
      "error : 1.947488, accuarcy : 0.568284\n",
      "total step : 229 \n",
      "error : 1.944530, accuarcy : 0.569285\n",
      "total step : 230 \n",
      "error : 1.941581, accuarcy : 0.569285\n",
      "total step : 231 \n",
      "error : 1.938641, accuarcy : 0.569285\n",
      "total step : 232 \n",
      "error : 1.935709, accuarcy : 0.569785\n",
      "total step : 233 \n",
      "error : 1.932785, accuarcy : 0.570285\n",
      "total step : 234 \n",
      "error : 1.929870, accuarcy : 0.570785\n",
      "total step : 235 \n",
      "error : 1.926964, accuarcy : 0.570785\n",
      "total step : 236 \n",
      "error : 1.924065, accuarcy : 0.570785\n",
      "total step : 237 \n",
      "error : 1.921176, accuarcy : 0.571286\n",
      "total step : 238 \n",
      "error : 1.918294, accuarcy : 0.571286\n",
      "total step : 239 \n",
      "error : 1.915421, accuarcy : 0.572786\n",
      "total step : 240 \n",
      "error : 1.912557, accuarcy : 0.573287\n",
      "total step : 241 \n",
      "error : 1.909700, accuarcy : 0.573287\n",
      "total step : 242 \n",
      "error : 1.906852, accuarcy : 0.574287\n",
      "total step : 243 \n",
      "error : 1.904012, accuarcy : 0.575288\n",
      "total step : 244 \n",
      "error : 1.901180, accuarcy : 0.575288\n",
      "total step : 245 \n",
      "error : 1.898357, accuarcy : 0.575288\n",
      "total step : 246 \n",
      "error : 1.895541, accuarcy : 0.575288\n",
      "total step : 247 \n",
      "error : 1.892734, accuarcy : 0.575788\n",
      "total step : 248 \n",
      "error : 1.889935, accuarcy : 0.575788\n",
      "total step : 249 \n",
      "error : 1.887143, accuarcy : 0.575788\n",
      "total step : 250 \n",
      "error : 1.884360, accuarcy : 0.575788\n",
      "total step : 251 \n",
      "error : 1.881585, accuarcy : 0.576288\n",
      "total step : 252 \n",
      "error : 1.878817, accuarcy : 0.576788\n",
      "total step : 253 \n",
      "error : 1.876058, accuarcy : 0.576788\n",
      "total step : 254 \n",
      "error : 1.873306, accuarcy : 0.576788\n",
      "total step : 255 \n",
      "error : 1.870563, accuarcy : 0.577289\n",
      "total step : 256 \n",
      "error : 1.867827, accuarcy : 0.577289\n",
      "total step : 257 \n",
      "error : 1.865099, accuarcy : 0.578289\n",
      "total step : 258 \n",
      "error : 1.862378, accuarcy : 0.578289\n",
      "total step : 259 \n",
      "error : 1.859666, accuarcy : 0.578289\n",
      "total step : 260 \n",
      "error : 1.856961, accuarcy : 0.578289\n",
      "total step : 261 \n",
      "error : 1.854264, accuarcy : 0.578289\n",
      "total step : 262 \n",
      "error : 1.851574, accuarcy : 0.578289\n",
      "total step : 263 \n",
      "error : 1.848892, accuarcy : 0.578289\n",
      "total step : 264 \n",
      "error : 1.846218, accuarcy : 0.579290\n",
      "total step : 265 \n",
      "error : 1.843551, accuarcy : 0.580790\n",
      "total step : 266 \n",
      "error : 1.840892, accuarcy : 0.581291\n",
      "total step : 267 \n",
      "error : 1.838240, accuarcy : 0.582291\n",
      "total step : 268 \n",
      "error : 1.835596, accuarcy : 0.582291\n",
      "total step : 269 \n",
      "error : 1.832959, accuarcy : 0.582291\n",
      "total step : 270 \n",
      "error : 1.830330, accuarcy : 0.582291\n",
      "total step : 271 \n",
      "error : 1.827708, accuarcy : 0.581791\n",
      "total step : 272 \n",
      "error : 1.825093, accuarcy : 0.581791\n",
      "total step : 273 \n",
      "error : 1.822486, accuarcy : 0.581791\n",
      "total step : 274 \n",
      "error : 1.819886, accuarcy : 0.581791\n",
      "total step : 275 \n",
      "error : 1.817294, accuarcy : 0.582291\n",
      "total step : 276 \n",
      "error : 1.814708, accuarcy : 0.582791\n",
      "total step : 277 \n",
      "error : 1.812130, accuarcy : 0.582791\n",
      "total step : 278 \n",
      "error : 1.809559, accuarcy : 0.582791\n",
      "total step : 279 \n",
      "error : 1.806995, accuarcy : 0.583292\n",
      "total step : 280 \n",
      "error : 1.804439, accuarcy : 0.583792\n",
      "total step : 281 \n",
      "error : 1.801889, accuarcy : 0.583792\n",
      "total step : 282 \n",
      "error : 1.799347, accuarcy : 0.583792\n",
      "total step : 283 \n",
      "error : 1.796811, accuarcy : 0.584292\n",
      "total step : 284 \n",
      "error : 1.794283, accuarcy : 0.584292\n",
      "total step : 285 \n",
      "error : 1.791762, accuarcy : 0.584792\n",
      "total step : 286 \n",
      "error : 1.789247, accuarcy : 0.584792\n",
      "total step : 287 \n",
      "error : 1.786740, accuarcy : 0.585793\n",
      "total step : 288 \n",
      "error : 1.784239, accuarcy : 0.586793\n",
      "total step : 289 \n",
      "error : 1.781746, accuarcy : 0.587294\n",
      "total step : 290 \n",
      "error : 1.779259, accuarcy : 0.587794\n",
      "total step : 291 \n",
      "error : 1.776780, accuarcy : 0.588294\n",
      "total step : 292 \n",
      "error : 1.774307, accuarcy : 0.588294\n",
      "total step : 293 \n",
      "error : 1.771840, accuarcy : 0.589295\n",
      "total step : 294 \n",
      "error : 1.769381, accuarcy : 0.590795\n",
      "total step : 295 \n",
      "error : 1.766928, accuarcy : 0.591796\n",
      "total step : 296 \n",
      "error : 1.764482, accuarcy : 0.592296\n",
      "total step : 297 \n",
      "error : 1.762043, accuarcy : 0.592296\n",
      "total step : 298 \n",
      "error : 1.759611, accuarcy : 0.592296\n",
      "total step : 299 \n",
      "error : 1.757185, accuarcy : 0.592296\n",
      "total step : 300 \n",
      "error : 1.754766, accuarcy : 0.592796\n",
      "total step : 301 \n",
      "error : 1.752353, accuarcy : 0.592796\n",
      "total step : 302 \n",
      "error : 1.749947, accuarcy : 0.593797\n",
      "total step : 303 \n",
      "error : 1.747547, accuarcy : 0.594297\n",
      "total step : 304 \n",
      "error : 1.745154, accuarcy : 0.595298\n",
      "total step : 305 \n",
      "error : 1.742768, accuarcy : 0.595798\n",
      "total step : 306 \n",
      "error : 1.740388, accuarcy : 0.595798\n",
      "total step : 307 \n",
      "error : 1.738014, accuarcy : 0.595798\n",
      "total step : 308 \n",
      "error : 1.735647, accuarcy : 0.596798\n",
      "total step : 309 \n",
      "error : 1.733286, accuarcy : 0.597799\n",
      "total step : 310 \n",
      "error : 1.730932, accuarcy : 0.598299\n",
      "total step : 311 \n",
      "error : 1.728584, accuarcy : 0.598299\n",
      "total step : 312 \n",
      "error : 1.726242, accuarcy : 0.599300\n",
      "total step : 313 \n",
      "error : 1.723907, accuarcy : 0.599800\n",
      "total step : 314 \n",
      "error : 1.721578, accuarcy : 0.599800\n",
      "total step : 315 \n",
      "error : 1.719255, accuarcy : 0.600800\n",
      "total step : 316 \n",
      "error : 1.716938, accuarcy : 0.600800\n",
      "total step : 317 \n",
      "error : 1.714628, accuarcy : 0.601301\n",
      "total step : 318 \n",
      "error : 1.712323, accuarcy : 0.601801\n",
      "total step : 319 \n",
      "error : 1.710025, accuarcy : 0.601801\n",
      "total step : 320 \n",
      "error : 1.707733, accuarcy : 0.601801\n",
      "total step : 321 \n",
      "error : 1.705447, accuarcy : 0.601801\n",
      "total step : 322 \n",
      "error : 1.703168, accuarcy : 0.601801\n",
      "total step : 323 \n",
      "error : 1.700894, accuarcy : 0.602301\n",
      "total step : 324 \n",
      "error : 1.698626, accuarcy : 0.602801\n",
      "total step : 325 \n",
      "error : 1.696365, accuarcy : 0.603302\n",
      "total step : 326 \n",
      "error : 1.694109, accuarcy : 0.603302\n",
      "total step : 327 \n",
      "error : 1.691860, accuarcy : 0.603802\n",
      "total step : 328 \n",
      "error : 1.689616, accuarcy : 0.604302\n",
      "total step : 329 \n",
      "error : 1.687378, accuarcy : 0.604302\n",
      "total step : 330 \n",
      "error : 1.685146, accuarcy : 0.604302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 331 \n",
      "error : 1.682920, accuarcy : 0.604302\n",
      "total step : 332 \n",
      "error : 1.680700, accuarcy : 0.604302\n",
      "total step : 333 \n",
      "error : 1.678486, accuarcy : 0.605303\n",
      "total step : 334 \n",
      "error : 1.676277, accuarcy : 0.606803\n",
      "total step : 335 \n",
      "error : 1.674075, accuarcy : 0.607804\n",
      "total step : 336 \n",
      "error : 1.671878, accuarcy : 0.608304\n",
      "total step : 337 \n",
      "error : 1.669687, accuarcy : 0.608804\n",
      "total step : 338 \n",
      "error : 1.667501, accuarcy : 0.609305\n",
      "total step : 339 \n",
      "error : 1.665322, accuarcy : 0.609305\n",
      "total step : 340 \n",
      "error : 1.663148, accuarcy : 0.609305\n",
      "total step : 341 \n",
      "error : 1.660979, accuarcy : 0.609305\n",
      "total step : 342 \n",
      "error : 1.658817, accuarcy : 0.610305\n",
      "total step : 343 \n",
      "error : 1.656660, accuarcy : 0.610305\n",
      "total step : 344 \n",
      "error : 1.654508, accuarcy : 0.610805\n",
      "total step : 345 \n",
      "error : 1.652362, accuarcy : 0.611306\n",
      "total step : 346 \n",
      "error : 1.650222, accuarcy : 0.611806\n",
      "total step : 347 \n",
      "error : 1.648087, accuarcy : 0.611806\n",
      "total step : 348 \n",
      "error : 1.645958, accuarcy : 0.611806\n",
      "total step : 349 \n",
      "error : 1.643834, accuarcy : 0.612806\n",
      "total step : 350 \n",
      "error : 1.641716, accuarcy : 0.612806\n",
      "total step : 351 \n",
      "error : 1.639603, accuarcy : 0.613307\n",
      "total step : 352 \n",
      "error : 1.637496, accuarcy : 0.614307\n",
      "total step : 353 \n",
      "error : 1.635394, accuarcy : 0.614307\n",
      "total step : 354 \n",
      "error : 1.633297, accuarcy : 0.615308\n",
      "total step : 355 \n",
      "error : 1.631206, accuarcy : 0.615808\n",
      "total step : 356 \n",
      "error : 1.629120, accuarcy : 0.615808\n",
      "total step : 357 \n",
      "error : 1.627040, accuarcy : 0.616308\n",
      "total step : 358 \n",
      "error : 1.624964, accuarcy : 0.616808\n",
      "total step : 359 \n",
      "error : 1.622895, accuarcy : 0.617309\n",
      "total step : 360 \n",
      "error : 1.620830, accuarcy : 0.617309\n",
      "total step : 361 \n",
      "error : 1.618771, accuarcy : 0.617309\n",
      "total step : 362 \n",
      "error : 1.616716, accuarcy : 0.617309\n",
      "total step : 363 \n",
      "error : 1.614668, accuarcy : 0.617309\n",
      "total step : 364 \n",
      "error : 1.612624, accuarcy : 0.617309\n",
      "total step : 365 \n",
      "error : 1.610585, accuarcy : 0.617809\n",
      "total step : 366 \n",
      "error : 1.608552, accuarcy : 0.617809\n",
      "total step : 367 \n",
      "error : 1.606524, accuarcy : 0.618809\n",
      "total step : 368 \n",
      "error : 1.604501, accuarcy : 0.619310\n",
      "total step : 369 \n",
      "error : 1.602483, accuarcy : 0.619310\n",
      "total step : 370 \n",
      "error : 1.600470, accuarcy : 0.619810\n",
      "total step : 371 \n",
      "error : 1.598462, accuarcy : 0.620310\n",
      "total step : 372 \n",
      "error : 1.596459, accuarcy : 0.620310\n",
      "total step : 373 \n",
      "error : 1.594461, accuarcy : 0.620310\n",
      "total step : 374 \n",
      "error : 1.592469, accuarcy : 0.620310\n",
      "total step : 375 \n",
      "error : 1.590481, accuarcy : 0.620810\n",
      "total step : 376 \n",
      "error : 1.588498, accuarcy : 0.620810\n",
      "total step : 377 \n",
      "error : 1.586520, accuarcy : 0.622311\n",
      "total step : 378 \n",
      "error : 1.584547, accuarcy : 0.622811\n",
      "total step : 379 \n",
      "error : 1.582580, accuarcy : 0.622811\n",
      "total step : 380 \n",
      "error : 1.580616, accuarcy : 0.622811\n",
      "total step : 381 \n",
      "error : 1.578658, accuarcy : 0.622811\n",
      "total step : 382 \n",
      "error : 1.576705, accuarcy : 0.623812\n",
      "total step : 383 \n",
      "error : 1.574757, accuarcy : 0.623812\n",
      "total step : 384 \n",
      "error : 1.572813, accuarcy : 0.624312\n",
      "total step : 385 \n",
      "error : 1.570874, accuarcy : 0.624312\n",
      "total step : 386 \n",
      "error : 1.568940, accuarcy : 0.624312\n",
      "total step : 387 \n",
      "error : 1.567011, accuarcy : 0.624312\n",
      "total step : 388 \n",
      "error : 1.565087, accuarcy : 0.624312\n",
      "total step : 389 \n",
      "error : 1.563167, accuarcy : 0.624812\n",
      "total step : 390 \n",
      "error : 1.561252, accuarcy : 0.625813\n",
      "total step : 391 \n",
      "error : 1.559342, accuarcy : 0.625813\n",
      "total step : 392 \n",
      "error : 1.557436, accuarcy : 0.626313\n",
      "total step : 393 \n",
      "error : 1.555535, accuarcy : 0.626313\n",
      "total step : 394 \n",
      "error : 1.553639, accuarcy : 0.625813\n",
      "total step : 395 \n",
      "error : 1.551748, accuarcy : 0.626313\n",
      "total step : 396 \n",
      "error : 1.549861, accuarcy : 0.627814\n",
      "total step : 397 \n",
      "error : 1.547979, accuarcy : 0.627814\n",
      "total step : 398 \n",
      "error : 1.546101, accuarcy : 0.628314\n",
      "total step : 399 \n",
      "error : 1.544228, accuarcy : 0.628314\n",
      "total step : 400 \n",
      "error : 1.542359, accuarcy : 0.628314\n",
      "total step : 401 \n",
      "error : 1.540495, accuarcy : 0.628814\n",
      "total step : 402 \n",
      "error : 1.538636, accuarcy : 0.628814\n",
      "total step : 403 \n",
      "error : 1.536781, accuarcy : 0.628814\n",
      "total step : 404 \n",
      "error : 1.534931, accuarcy : 0.628814\n",
      "total step : 405 \n",
      "error : 1.533085, accuarcy : 0.628814\n",
      "total step : 406 \n",
      "error : 1.531243, accuarcy : 0.629315\n",
      "total step : 407 \n",
      "error : 1.529406, accuarcy : 0.629315\n",
      "total step : 408 \n",
      "error : 1.527574, accuarcy : 0.629315\n",
      "total step : 409 \n",
      "error : 1.525746, accuarcy : 0.629315\n",
      "total step : 410 \n",
      "error : 1.523922, accuarcy : 0.629315\n",
      "total step : 411 \n",
      "error : 1.522103, accuarcy : 0.629815\n",
      "total step : 412 \n",
      "error : 1.520288, accuarcy : 0.629815\n",
      "total step : 413 \n",
      "error : 1.518478, accuarcy : 0.629815\n",
      "total step : 414 \n",
      "error : 1.516671, accuarcy : 0.629815\n",
      "total step : 415 \n",
      "error : 1.514870, accuarcy : 0.630315\n",
      "total step : 416 \n",
      "error : 1.513072, accuarcy : 0.630315\n",
      "total step : 417 \n",
      "error : 1.511279, accuarcy : 0.630315\n",
      "total step : 418 \n",
      "error : 1.509490, accuarcy : 0.630315\n",
      "total step : 419 \n",
      "error : 1.507705, accuarcy : 0.631316\n",
      "total step : 420 \n",
      "error : 1.505925, accuarcy : 0.631316\n",
      "total step : 421 \n",
      "error : 1.504149, accuarcy : 0.631316\n",
      "total step : 422 \n",
      "error : 1.502377, accuarcy : 0.630815\n",
      "total step : 423 \n",
      "error : 1.500610, accuarcy : 0.631816\n",
      "total step : 424 \n",
      "error : 1.498846, accuarcy : 0.631816\n",
      "total step : 425 \n",
      "error : 1.497087, accuarcy : 0.632316\n",
      "total step : 426 \n",
      "error : 1.495332, accuarcy : 0.632816\n",
      "total step : 427 \n",
      "error : 1.493581, accuarcy : 0.633317\n",
      "total step : 428 \n",
      "error : 1.491835, accuarcy : 0.633817\n",
      "total step : 429 \n",
      "error : 1.490092, accuarcy : 0.634317\n",
      "total step : 430 \n",
      "error : 1.488354, accuarcy : 0.635318\n",
      "total step : 431 \n",
      "error : 1.486620, accuarcy : 0.635818\n",
      "total step : 432 \n",
      "error : 1.484889, accuarcy : 0.636318\n",
      "total step : 433 \n",
      "error : 1.483163, accuarcy : 0.636818\n",
      "total step : 434 \n",
      "error : 1.481441, accuarcy : 0.636318\n",
      "total step : 435 \n",
      "error : 1.479723, accuarcy : 0.636318\n",
      "total step : 436 \n",
      "error : 1.478009, accuarcy : 0.636818\n",
      "total step : 437 \n",
      "error : 1.476300, accuarcy : 0.637319\n",
      "total step : 438 \n",
      "error : 1.474594, accuarcy : 0.637819\n",
      "total step : 439 \n",
      "error : 1.472892, accuarcy : 0.637819\n",
      "total step : 440 \n",
      "error : 1.471194, accuarcy : 0.637819\n",
      "total step : 441 \n",
      "error : 1.469500, accuarcy : 0.638319\n",
      "total step : 442 \n",
      "error : 1.467811, accuarcy : 0.638319\n",
      "total step : 443 \n",
      "error : 1.466125, accuarcy : 0.639320\n",
      "total step : 444 \n",
      "error : 1.464443, accuarcy : 0.639820\n",
      "total step : 445 \n",
      "error : 1.462765, accuarcy : 0.639820\n",
      "total step : 446 \n",
      "error : 1.461091, accuarcy : 0.639820\n",
      "total step : 447 \n",
      "error : 1.459420, accuarcy : 0.639820\n",
      "total step : 448 \n",
      "error : 1.457754, accuarcy : 0.639320\n",
      "total step : 449 \n",
      "error : 1.456092, accuarcy : 0.639320\n",
      "total step : 450 \n",
      "error : 1.454433, accuarcy : 0.639820\n",
      "total step : 451 \n",
      "error : 1.452779, accuarcy : 0.640820\n",
      "total step : 452 \n",
      "error : 1.451128, accuarcy : 0.640820\n",
      "total step : 453 \n",
      "error : 1.449481, accuarcy : 0.640820\n",
      "total step : 454 \n",
      "error : 1.447838, accuarcy : 0.641821\n",
      "total step : 455 \n",
      "error : 1.446199, accuarcy : 0.641321\n",
      "total step : 456 \n",
      "error : 1.444563, accuarcy : 0.641321\n",
      "total step : 457 \n",
      "error : 1.442931, accuarcy : 0.641821\n",
      "total step : 458 \n",
      "error : 1.441303, accuarcy : 0.642321\n",
      "total step : 459 \n",
      "error : 1.439679, accuarcy : 0.642321\n",
      "total step : 460 \n",
      "error : 1.438059, accuarcy : 0.642321\n",
      "total step : 461 \n",
      "error : 1.436442, accuarcy : 0.642321\n",
      "total step : 462 \n",
      "error : 1.434829, accuarcy : 0.642821\n",
      "total step : 463 \n",
      "error : 1.433220, accuarcy : 0.642821\n",
      "total step : 464 \n",
      "error : 1.431614, accuarcy : 0.643822\n",
      "total step : 465 \n",
      "error : 1.430013, accuarcy : 0.643822\n",
      "total step : 466 \n",
      "error : 1.428414, accuarcy : 0.644322\n",
      "total step : 467 \n",
      "error : 1.426820, accuarcy : 0.644322\n",
      "total step : 468 \n",
      "error : 1.425229, accuarcy : 0.645323\n",
      "total step : 469 \n",
      "error : 1.423642, accuarcy : 0.645323\n",
      "total step : 470 \n",
      "error : 1.422059, accuarcy : 0.645823\n",
      "total step : 471 \n",
      "error : 1.420479, accuarcy : 0.645323\n",
      "total step : 472 \n",
      "error : 1.418902, accuarcy : 0.645823\n",
      "total step : 473 \n",
      "error : 1.417330, accuarcy : 0.646823\n",
      "total step : 474 \n",
      "error : 1.415761, accuarcy : 0.647824\n",
      "total step : 475 \n",
      "error : 1.414195, accuarcy : 0.647824\n",
      "total step : 476 \n",
      "error : 1.412633, accuarcy : 0.648324\n",
      "total step : 477 \n",
      "error : 1.411075, accuarcy : 0.648324\n",
      "total step : 478 \n",
      "error : 1.409520, accuarcy : 0.648824\n",
      "total step : 479 \n",
      "error : 1.407969, accuarcy : 0.648824\n",
      "total step : 480 \n",
      "error : 1.406422, accuarcy : 0.648824\n",
      "total step : 481 \n",
      "error : 1.404877, accuarcy : 0.649325\n",
      "total step : 482 \n",
      "error : 1.403337, accuarcy : 0.649325\n",
      "total step : 483 \n",
      "error : 1.401800, accuarcy : 0.648824\n",
      "total step : 484 \n",
      "error : 1.400266, accuarcy : 0.648824\n",
      "total step : 485 \n",
      "error : 1.398736, accuarcy : 0.648824\n",
      "total step : 486 \n",
      "error : 1.397209, accuarcy : 0.648824\n",
      "total step : 487 \n",
      "error : 1.395686, accuarcy : 0.649325\n",
      "total step : 488 \n",
      "error : 1.394166, accuarcy : 0.649825\n",
      "total step : 489 \n",
      "error : 1.392650, accuarcy : 0.650325\n",
      "total step : 490 \n",
      "error : 1.391137, accuarcy : 0.650325\n",
      "total step : 491 \n",
      "error : 1.389628, accuarcy : 0.650325\n",
      "total step : 492 \n",
      "error : 1.388122, accuarcy : 0.650825\n",
      "total step : 493 \n",
      "error : 1.386619, accuarcy : 0.651326\n",
      "total step : 494 \n",
      "error : 1.385120, accuarcy : 0.651326\n",
      "total step : 495 \n",
      "error : 1.383624, accuarcy : 0.651826\n",
      "total step : 496 \n",
      "error : 1.382131, accuarcy : 0.652326\n",
      "total step : 497 \n",
      "error : 1.380642, accuarcy : 0.652826\n",
      "total step : 498 \n",
      "error : 1.379156, accuarcy : 0.652826\n",
      "total step : 499 \n",
      "error : 1.377674, accuarcy : 0.652826\n",
      "total step : 500 \n",
      "error : 1.376195, accuarcy : 0.652826\n",
      "total step : 501 \n",
      "error : 1.374719, accuarcy : 0.653327\n",
      "total step : 502 \n",
      "error : 1.373247, accuarcy : 0.653327\n",
      "total step : 503 \n",
      "error : 1.371778, accuarcy : 0.653327\n",
      "total step : 504 \n",
      "error : 1.370312, accuarcy : 0.653327\n",
      "total step : 505 \n",
      "error : 1.368849, accuarcy : 0.653827\n",
      "total step : 506 \n",
      "error : 1.367390, accuarcy : 0.653827\n",
      "total step : 507 \n",
      "error : 1.365934, accuarcy : 0.653827\n",
      "total step : 508 \n",
      "error : 1.364481, accuarcy : 0.653827\n",
      "total step : 509 \n",
      "error : 1.363032, accuarcy : 0.654327\n",
      "total step : 510 \n",
      "error : 1.361585, accuarcy : 0.654327\n",
      "total step : 511 \n",
      "error : 1.360142, accuarcy : 0.654827\n",
      "total step : 512 \n",
      "error : 1.358702, accuarcy : 0.655828\n",
      "total step : 513 \n",
      "error : 1.357266, accuarcy : 0.656328\n",
      "total step : 514 \n",
      "error : 1.355832, accuarcy : 0.656328\n",
      "total step : 515 \n",
      "error : 1.354402, accuarcy : 0.656828\n",
      "total step : 516 \n",
      "error : 1.352975, accuarcy : 0.656828\n",
      "total step : 517 \n",
      "error : 1.351551, accuarcy : 0.657829\n",
      "total step : 518 \n",
      "error : 1.350131, accuarcy : 0.658329\n",
      "total step : 519 \n",
      "error : 1.348713, accuarcy : 0.658329\n",
      "total step : 520 \n",
      "error : 1.347299, accuarcy : 0.658329\n",
      "total step : 521 \n",
      "error : 1.345888, accuarcy : 0.657329\n",
      "total step : 522 \n",
      "error : 1.344480, accuarcy : 0.658329\n",
      "total step : 523 \n",
      "error : 1.343075, accuarcy : 0.658329\n",
      "total step : 524 \n",
      "error : 1.341673, accuarcy : 0.658329\n",
      "total step : 525 \n",
      "error : 1.340274, accuarcy : 0.658329\n",
      "total step : 526 \n",
      "error : 1.338878, accuarcy : 0.658829\n",
      "total step : 527 \n",
      "error : 1.337486, accuarcy : 0.660330\n",
      "total step : 528 \n",
      "error : 1.336096, accuarcy : 0.660830\n",
      "total step : 529 \n",
      "error : 1.334710, accuarcy : 0.660830\n",
      "total step : 530 \n",
      "error : 1.333327, accuarcy : 0.660830\n",
      "total step : 531 \n",
      "error : 1.331946, accuarcy : 0.660830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 532 \n",
      "error : 1.330569, accuarcy : 0.661331\n",
      "total step : 533 \n",
      "error : 1.329195, accuarcy : 0.661331\n",
      "total step : 534 \n",
      "error : 1.327824, accuarcy : 0.661331\n",
      "total step : 535 \n",
      "error : 1.326456, accuarcy : 0.662331\n",
      "total step : 536 \n",
      "error : 1.325091, accuarcy : 0.661831\n",
      "total step : 537 \n",
      "error : 1.323729, accuarcy : 0.661831\n",
      "total step : 538 \n",
      "error : 1.322370, accuarcy : 0.661831\n",
      "total step : 539 \n",
      "error : 1.321013, accuarcy : 0.661831\n",
      "total step : 540 \n",
      "error : 1.319660, accuarcy : 0.661831\n",
      "total step : 541 \n",
      "error : 1.318310, accuarcy : 0.661831\n",
      "total step : 542 \n",
      "error : 1.316963, accuarcy : 0.661831\n",
      "total step : 543 \n",
      "error : 1.315619, accuarcy : 0.662331\n",
      "total step : 544 \n",
      "error : 1.314278, accuarcy : 0.662831\n",
      "total step : 545 \n",
      "error : 1.312939, accuarcy : 0.662831\n",
      "total step : 546 \n",
      "error : 1.311604, accuarcy : 0.663832\n",
      "total step : 547 \n",
      "error : 1.310271, accuarcy : 0.664332\n",
      "total step : 548 \n",
      "error : 1.308942, accuarcy : 0.664332\n",
      "total step : 549 \n",
      "error : 1.307615, accuarcy : 0.664832\n",
      "total step : 550 \n",
      "error : 1.306291, accuarcy : 0.664832\n",
      "total step : 551 \n",
      "error : 1.304971, accuarcy : 0.665333\n",
      "total step : 552 \n",
      "error : 1.303653, accuarcy : 0.665333\n",
      "total step : 553 \n",
      "error : 1.302338, accuarcy : 0.665833\n",
      "total step : 554 \n",
      "error : 1.301025, accuarcy : 0.666333\n",
      "total step : 555 \n",
      "error : 1.299716, accuarcy : 0.667334\n",
      "total step : 556 \n",
      "error : 1.298410, accuarcy : 0.667834\n",
      "total step : 557 \n",
      "error : 1.297106, accuarcy : 0.667834\n",
      "total step : 558 \n",
      "error : 1.295805, accuarcy : 0.668334\n",
      "total step : 559 \n",
      "error : 1.294507, accuarcy : 0.668334\n",
      "total step : 560 \n",
      "error : 1.293212, accuarcy : 0.668334\n",
      "total step : 561 \n",
      "error : 1.291920, accuarcy : 0.668334\n",
      "total step : 562 \n",
      "error : 1.290630, accuarcy : 0.668834\n",
      "total step : 563 \n",
      "error : 1.289344, accuarcy : 0.668834\n",
      "total step : 564 \n",
      "error : 1.288060, accuarcy : 0.668834\n",
      "total step : 565 \n",
      "error : 1.286779, accuarcy : 0.669335\n",
      "total step : 566 \n",
      "error : 1.285500, accuarcy : 0.669335\n",
      "total step : 567 \n",
      "error : 1.284225, accuarcy : 0.669335\n",
      "total step : 568 \n",
      "error : 1.282952, accuarcy : 0.669335\n",
      "total step : 569 \n",
      "error : 1.281682, accuarcy : 0.669335\n",
      "total step : 570 \n",
      "error : 1.280415, accuarcy : 0.669335\n",
      "total step : 571 \n",
      "error : 1.279150, accuarcy : 0.670335\n",
      "total step : 572 \n",
      "error : 1.277888, accuarcy : 0.670335\n",
      "total step : 573 \n",
      "error : 1.276629, accuarcy : 0.670335\n",
      "total step : 574 \n",
      "error : 1.275373, accuarcy : 0.670835\n",
      "total step : 575 \n",
      "error : 1.274119, accuarcy : 0.670835\n",
      "total step : 576 \n",
      "error : 1.272868, accuarcy : 0.670835\n",
      "total step : 577 \n",
      "error : 1.271620, accuarcy : 0.670835\n",
      "total step : 578 \n",
      "error : 1.270375, accuarcy : 0.670835\n",
      "total step : 579 \n",
      "error : 1.269132, accuarcy : 0.670835\n",
      "total step : 580 \n",
      "error : 1.267892, accuarcy : 0.670835\n",
      "total step : 581 \n",
      "error : 1.266654, accuarcy : 0.670835\n",
      "total step : 582 \n",
      "error : 1.265419, accuarcy : 0.670835\n",
      "total step : 583 \n",
      "error : 1.264187, accuarcy : 0.670835\n",
      "total step : 584 \n",
      "error : 1.262958, accuarcy : 0.671336\n",
      "total step : 585 \n",
      "error : 1.261731, accuarcy : 0.671336\n",
      "total step : 586 \n",
      "error : 1.260507, accuarcy : 0.672336\n",
      "total step : 587 \n",
      "error : 1.259285, accuarcy : 0.672336\n",
      "total step : 588 \n",
      "error : 1.258066, accuarcy : 0.672336\n",
      "total step : 589 \n",
      "error : 1.256850, accuarcy : 0.672836\n",
      "total step : 590 \n",
      "error : 1.255636, accuarcy : 0.673337\n",
      "total step : 591 \n",
      "error : 1.254425, accuarcy : 0.673837\n",
      "total step : 592 \n",
      "error : 1.253216, accuarcy : 0.673837\n",
      "total step : 593 \n",
      "error : 1.252010, accuarcy : 0.673837\n",
      "total step : 594 \n",
      "error : 1.250807, accuarcy : 0.674337\n",
      "total step : 595 \n",
      "error : 1.249606, accuarcy : 0.674337\n",
      "total step : 596 \n",
      "error : 1.248408, accuarcy : 0.674337\n",
      "total step : 597 \n",
      "error : 1.247212, accuarcy : 0.674337\n",
      "total step : 598 \n",
      "error : 1.246019, accuarcy : 0.674837\n",
      "total step : 599 \n",
      "error : 1.244828, accuarcy : 0.674837\n",
      "total step : 600 \n",
      "error : 1.243640, accuarcy : 0.675338\n",
      "total step : 601 \n",
      "error : 1.242455, accuarcy : 0.675838\n",
      "total step : 602 \n",
      "error : 1.241272, accuarcy : 0.675838\n",
      "total step : 603 \n",
      "error : 1.240091, accuarcy : 0.675838\n",
      "total step : 604 \n",
      "error : 1.238913, accuarcy : 0.675838\n",
      "total step : 605 \n",
      "error : 1.237738, accuarcy : 0.676338\n",
      "total step : 606 \n",
      "error : 1.236565, accuarcy : 0.676338\n",
      "total step : 607 \n",
      "error : 1.235395, accuarcy : 0.676338\n",
      "total step : 608 \n",
      "error : 1.234227, accuarcy : 0.676838\n",
      "total step : 609 \n",
      "error : 1.233061, accuarcy : 0.676838\n",
      "total step : 610 \n",
      "error : 1.231898, accuarcy : 0.677339\n",
      "total step : 611 \n",
      "error : 1.230738, accuarcy : 0.677339\n",
      "total step : 612 \n",
      "error : 1.229580, accuarcy : 0.677339\n",
      "total step : 613 \n",
      "error : 1.228424, accuarcy : 0.677339\n",
      "total step : 614 \n",
      "error : 1.227271, accuarcy : 0.677339\n",
      "total step : 615 \n",
      "error : 1.226120, accuarcy : 0.677339\n",
      "total step : 616 \n",
      "error : 1.224972, accuarcy : 0.677839\n",
      "total step : 617 \n",
      "error : 1.223826, accuarcy : 0.677839\n",
      "total step : 618 \n",
      "error : 1.222682, accuarcy : 0.677839\n",
      "total step : 619 \n",
      "error : 1.221541, accuarcy : 0.679340\n",
      "total step : 620 \n",
      "error : 1.220403, accuarcy : 0.679840\n",
      "total step : 621 \n",
      "error : 1.219266, accuarcy : 0.679840\n",
      "total step : 622 \n",
      "error : 1.218133, accuarcy : 0.679840\n",
      "total step : 623 \n",
      "error : 1.217001, accuarcy : 0.680340\n",
      "total step : 624 \n",
      "error : 1.215872, accuarcy : 0.680840\n",
      "total step : 625 \n",
      "error : 1.214745, accuarcy : 0.680340\n",
      "total step : 626 \n",
      "error : 1.213621, accuarcy : 0.681341\n",
      "total step : 627 \n",
      "error : 1.212499, accuarcy : 0.681341\n",
      "total step : 628 \n",
      "error : 1.211379, accuarcy : 0.681341\n",
      "total step : 629 \n",
      "error : 1.210262, accuarcy : 0.681341\n",
      "total step : 630 \n",
      "error : 1.209147, accuarcy : 0.681341\n",
      "total step : 631 \n",
      "error : 1.208035, accuarcy : 0.681341\n",
      "total step : 632 \n",
      "error : 1.206924, accuarcy : 0.681341\n",
      "total step : 633 \n",
      "error : 1.205816, accuarcy : 0.681841\n",
      "total step : 634 \n",
      "error : 1.204711, accuarcy : 0.681841\n",
      "total step : 635 \n",
      "error : 1.203608, accuarcy : 0.681841\n",
      "total step : 636 \n",
      "error : 1.202507, accuarcy : 0.682341\n",
      "total step : 637 \n",
      "error : 1.201408, accuarcy : 0.682841\n",
      "total step : 638 \n",
      "error : 1.200311, accuarcy : 0.683842\n",
      "total step : 639 \n",
      "error : 1.199217, accuarcy : 0.683842\n",
      "total step : 640 \n",
      "error : 1.198126, accuarcy : 0.683842\n",
      "total step : 641 \n",
      "error : 1.197036, accuarcy : 0.684342\n",
      "total step : 642 \n",
      "error : 1.195949, accuarcy : 0.684342\n",
      "total step : 643 \n",
      "error : 1.194864, accuarcy : 0.685343\n",
      "total step : 644 \n",
      "error : 1.193781, accuarcy : 0.685843\n",
      "total step : 645 \n",
      "error : 1.192700, accuarcy : 0.685343\n",
      "total step : 646 \n",
      "error : 1.191622, accuarcy : 0.685843\n",
      "total step : 647 \n",
      "error : 1.190546, accuarcy : 0.685843\n",
      "total step : 648 \n",
      "error : 1.189472, accuarcy : 0.686343\n",
      "total step : 649 \n",
      "error : 1.188401, accuarcy : 0.686843\n",
      "total step : 650 \n",
      "error : 1.187331, accuarcy : 0.686843\n",
      "total step : 651 \n",
      "error : 1.186264, accuarcy : 0.686843\n",
      "total step : 652 \n",
      "error : 1.185199, accuarcy : 0.687344\n",
      "total step : 653 \n",
      "error : 1.184137, accuarcy : 0.686843\n",
      "total step : 654 \n",
      "error : 1.183076, accuarcy : 0.686843\n",
      "total step : 655 \n",
      "error : 1.182018, accuarcy : 0.687344\n",
      "total step : 656 \n",
      "error : 1.180962, accuarcy : 0.687344\n",
      "total step : 657 \n",
      "error : 1.179908, accuarcy : 0.687344\n",
      "total step : 658 \n",
      "error : 1.178856, accuarcy : 0.686843\n",
      "total step : 659 \n",
      "error : 1.177806, accuarcy : 0.687344\n",
      "total step : 660 \n",
      "error : 1.176759, accuarcy : 0.687344\n",
      "total step : 661 \n",
      "error : 1.175713, accuarcy : 0.687844\n",
      "total step : 662 \n",
      "error : 1.174670, accuarcy : 0.688344\n",
      "total step : 663 \n",
      "error : 1.173629, accuarcy : 0.688844\n",
      "total step : 664 \n",
      "error : 1.172590, accuarcy : 0.689345\n",
      "total step : 665 \n",
      "error : 1.171554, accuarcy : 0.689845\n",
      "total step : 666 \n",
      "error : 1.170519, accuarcy : 0.689845\n",
      "total step : 667 \n",
      "error : 1.169487, accuarcy : 0.689845\n",
      "total step : 668 \n",
      "error : 1.168456, accuarcy : 0.689845\n",
      "total step : 669 \n",
      "error : 1.167428, accuarcy : 0.690845\n",
      "total step : 670 \n",
      "error : 1.166402, accuarcy : 0.690845\n",
      "total step : 671 \n",
      "error : 1.165378, accuarcy : 0.690845\n",
      "total step : 672 \n",
      "error : 1.164356, accuarcy : 0.690845\n",
      "total step : 673 \n",
      "error : 1.163336, accuarcy : 0.690845\n",
      "total step : 674 \n",
      "error : 1.162318, accuarcy : 0.690845\n",
      "total step : 675 \n",
      "error : 1.161302, accuarcy : 0.690845\n",
      "total step : 676 \n",
      "error : 1.160289, accuarcy : 0.690845\n",
      "total step : 677 \n",
      "error : 1.159277, accuarcy : 0.690845\n",
      "total step : 678 \n",
      "error : 1.158267, accuarcy : 0.690845\n",
      "total step : 679 \n",
      "error : 1.157260, accuarcy : 0.690845\n",
      "total step : 680 \n",
      "error : 1.156254, accuarcy : 0.690845\n",
      "total step : 681 \n",
      "error : 1.155251, accuarcy : 0.691346\n",
      "total step : 682 \n",
      "error : 1.154250, accuarcy : 0.691846\n",
      "total step : 683 \n",
      "error : 1.153250, accuarcy : 0.691846\n",
      "total step : 684 \n",
      "error : 1.152253, accuarcy : 0.692346\n",
      "total step : 685 \n",
      "error : 1.151258, accuarcy : 0.692346\n",
      "total step : 686 \n",
      "error : 1.150264, accuarcy : 0.692346\n",
      "total step : 687 \n",
      "error : 1.149273, accuarcy : 0.692846\n",
      "total step : 688 \n",
      "error : 1.148284, accuarcy : 0.693347\n",
      "total step : 689 \n",
      "error : 1.147296, accuarcy : 0.693347\n",
      "total step : 690 \n",
      "error : 1.146311, accuarcy : 0.693847\n",
      "total step : 691 \n",
      "error : 1.145328, accuarcy : 0.693847\n",
      "total step : 692 \n",
      "error : 1.144347, accuarcy : 0.693847\n",
      "total step : 693 \n",
      "error : 1.143367, accuarcy : 0.693847\n",
      "total step : 694 \n",
      "error : 1.142390, accuarcy : 0.693847\n",
      "total step : 695 \n",
      "error : 1.141414, accuarcy : 0.694347\n",
      "total step : 696 \n",
      "error : 1.140441, accuarcy : 0.694347\n",
      "total step : 697 \n",
      "error : 1.139469, accuarcy : 0.694347\n",
      "total step : 698 \n",
      "error : 1.138500, accuarcy : 0.694347\n",
      "total step : 699 \n",
      "error : 1.137532, accuarcy : 0.694847\n",
      "total step : 700 \n",
      "error : 1.136567, accuarcy : 0.694847\n",
      "total step : 701 \n",
      "error : 1.135603, accuarcy : 0.695348\n",
      "total step : 702 \n",
      "error : 1.134641, accuarcy : 0.695848\n",
      "total step : 703 \n",
      "error : 1.133681, accuarcy : 0.695848\n",
      "total step : 704 \n",
      "error : 1.132723, accuarcy : 0.695848\n",
      "total step : 705 \n",
      "error : 1.131767, accuarcy : 0.695848\n",
      "total step : 706 \n",
      "error : 1.130813, accuarcy : 0.695848\n",
      "total step : 707 \n",
      "error : 1.129861, accuarcy : 0.695848\n",
      "total step : 708 \n",
      "error : 1.128910, accuarcy : 0.695848\n",
      "total step : 709 \n",
      "error : 1.127962, accuarcy : 0.695848\n",
      "total step : 710 \n",
      "error : 1.127015, accuarcy : 0.695848\n",
      "total step : 711 \n",
      "error : 1.126071, accuarcy : 0.696348\n",
      "total step : 712 \n",
      "error : 1.125128, accuarcy : 0.696848\n",
      "total step : 713 \n",
      "error : 1.124187, accuarcy : 0.696848\n",
      "total step : 714 \n",
      "error : 1.123248, accuarcy : 0.697349\n",
      "total step : 715 \n",
      "error : 1.122310, accuarcy : 0.697849\n",
      "total step : 716 \n",
      "error : 1.121375, accuarcy : 0.698349\n",
      "total step : 717 \n",
      "error : 1.120441, accuarcy : 0.698349\n",
      "total step : 718 \n",
      "error : 1.119510, accuarcy : 0.698349\n",
      "total step : 719 \n",
      "error : 1.118580, accuarcy : 0.698349\n",
      "total step : 720 \n",
      "error : 1.117652, accuarcy : 0.698349\n",
      "total step : 721 \n",
      "error : 1.116726, accuarcy : 0.698349\n",
      "total step : 722 \n",
      "error : 1.115801, accuarcy : 0.698349\n",
      "total step : 723 \n",
      "error : 1.114879, accuarcy : 0.698849\n",
      "total step : 724 \n",
      "error : 1.113958, accuarcy : 0.699350\n",
      "total step : 725 \n",
      "error : 1.113039, accuarcy : 0.699850\n",
      "total step : 726 \n",
      "error : 1.112122, accuarcy : 0.700350\n",
      "total step : 727 \n",
      "error : 1.111207, accuarcy : 0.700350\n",
      "total step : 728 \n",
      "error : 1.110293, accuarcy : 0.700350\n",
      "total step : 729 \n",
      "error : 1.109382, accuarcy : 0.700350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 730 \n",
      "error : 1.108472, accuarcy : 0.700850\n",
      "total step : 731 \n",
      "error : 1.107564, accuarcy : 0.700850\n",
      "total step : 732 \n",
      "error : 1.106657, accuarcy : 0.701351\n",
      "total step : 733 \n",
      "error : 1.105753, accuarcy : 0.701351\n",
      "total step : 734 \n",
      "error : 1.104850, accuarcy : 0.701851\n",
      "total step : 735 \n",
      "error : 1.103949, accuarcy : 0.702851\n",
      "total step : 736 \n",
      "error : 1.103049, accuarcy : 0.702851\n",
      "total step : 737 \n",
      "error : 1.102152, accuarcy : 0.702851\n",
      "total step : 738 \n",
      "error : 1.101256, accuarcy : 0.702851\n",
      "total step : 739 \n",
      "error : 1.100362, accuarcy : 0.702851\n",
      "total step : 740 \n",
      "error : 1.099470, accuarcy : 0.702851\n",
      "total step : 741 \n",
      "error : 1.098579, accuarcy : 0.703352\n",
      "total step : 742 \n",
      "error : 1.097690, accuarcy : 0.703352\n",
      "total step : 743 \n",
      "error : 1.096803, accuarcy : 0.703852\n",
      "total step : 744 \n",
      "error : 1.095917, accuarcy : 0.704852\n",
      "total step : 745 \n",
      "error : 1.095034, accuarcy : 0.704852\n",
      "total step : 746 \n",
      "error : 1.094152, accuarcy : 0.704852\n",
      "total step : 747 \n",
      "error : 1.093271, accuarcy : 0.705853\n",
      "total step : 748 \n",
      "error : 1.092393, accuarcy : 0.706853\n",
      "total step : 749 \n",
      "error : 1.091516, accuarcy : 0.707354\n",
      "total step : 750 \n",
      "error : 1.090641, accuarcy : 0.707854\n",
      "total step : 751 \n",
      "error : 1.089767, accuarcy : 0.707854\n",
      "total step : 752 \n",
      "error : 1.088895, accuarcy : 0.707854\n",
      "total step : 753 \n",
      "error : 1.088025, accuarcy : 0.707354\n",
      "total step : 754 \n",
      "error : 1.087157, accuarcy : 0.707354\n",
      "total step : 755 \n",
      "error : 1.086290, accuarcy : 0.707354\n",
      "total step : 756 \n",
      "error : 1.085425, accuarcy : 0.707354\n",
      "total step : 757 \n",
      "error : 1.084561, accuarcy : 0.707354\n",
      "total step : 758 \n",
      "error : 1.083699, accuarcy : 0.707354\n",
      "total step : 759 \n",
      "error : 1.082839, accuarcy : 0.707354\n",
      "total step : 760 \n",
      "error : 1.081981, accuarcy : 0.707854\n",
      "total step : 761 \n",
      "error : 1.081124, accuarcy : 0.708354\n",
      "total step : 762 \n",
      "error : 1.080268, accuarcy : 0.708354\n",
      "total step : 763 \n",
      "error : 1.079415, accuarcy : 0.708354\n",
      "total step : 764 \n",
      "error : 1.078563, accuarcy : 0.708354\n",
      "total step : 765 \n",
      "error : 1.077712, accuarcy : 0.708854\n",
      "total step : 766 \n",
      "error : 1.076864, accuarcy : 0.708854\n",
      "total step : 767 \n",
      "error : 1.076016, accuarcy : 0.709355\n",
      "total step : 768 \n",
      "error : 1.075171, accuarcy : 0.709855\n",
      "total step : 769 \n",
      "error : 1.074327, accuarcy : 0.709855\n",
      "total step : 770 \n",
      "error : 1.073485, accuarcy : 0.709855\n",
      "total step : 771 \n",
      "error : 1.072644, accuarcy : 0.710355\n",
      "total step : 772 \n",
      "error : 1.071805, accuarcy : 0.710355\n",
      "total step : 773 \n",
      "error : 1.070967, accuarcy : 0.710855\n",
      "total step : 774 \n",
      "error : 1.070131, accuarcy : 0.711356\n",
      "total step : 775 \n",
      "error : 1.069297, accuarcy : 0.711356\n",
      "total step : 776 \n",
      "error : 1.068464, accuarcy : 0.711356\n",
      "total step : 777 \n",
      "error : 1.067633, accuarcy : 0.711356\n",
      "total step : 778 \n",
      "error : 1.066803, accuarcy : 0.711856\n",
      "total step : 779 \n",
      "error : 1.065975, accuarcy : 0.711856\n",
      "total step : 780 \n",
      "error : 1.065149, accuarcy : 0.712856\n",
      "total step : 781 \n",
      "error : 1.064324, accuarcy : 0.712856\n",
      "total step : 782 \n",
      "error : 1.063500, accuarcy : 0.712856\n",
      "total step : 783 \n",
      "error : 1.062678, accuarcy : 0.713357\n",
      "total step : 784 \n",
      "error : 1.061858, accuarcy : 0.713357\n",
      "total step : 785 \n",
      "error : 1.061039, accuarcy : 0.713357\n",
      "total step : 786 \n",
      "error : 1.060222, accuarcy : 0.713357\n",
      "total step : 787 \n",
      "error : 1.059406, accuarcy : 0.713357\n",
      "total step : 788 \n",
      "error : 1.058592, accuarcy : 0.713357\n",
      "total step : 789 \n",
      "error : 1.057779, accuarcy : 0.713357\n",
      "total step : 790 \n",
      "error : 1.056968, accuarcy : 0.713357\n",
      "total step : 791 \n",
      "error : 1.056158, accuarcy : 0.713357\n",
      "total step : 792 \n",
      "error : 1.055350, accuarcy : 0.713357\n",
      "total step : 793 \n",
      "error : 1.054544, accuarcy : 0.713357\n",
      "total step : 794 \n",
      "error : 1.053738, accuarcy : 0.713357\n",
      "total step : 795 \n",
      "error : 1.052935, accuarcy : 0.713357\n",
      "total step : 796 \n",
      "error : 1.052133, accuarcy : 0.713357\n",
      "total step : 797 \n",
      "error : 1.051332, accuarcy : 0.713357\n",
      "total step : 798 \n",
      "error : 1.050533, accuarcy : 0.713357\n",
      "total step : 799 \n",
      "error : 1.049735, accuarcy : 0.713857\n",
      "total step : 800 \n",
      "error : 1.048939, accuarcy : 0.714357\n",
      "total step : 801 \n",
      "error : 1.048144, accuarcy : 0.714357\n",
      "total step : 802 \n",
      "error : 1.047351, accuarcy : 0.714357\n",
      "total step : 803 \n",
      "error : 1.046559, accuarcy : 0.714357\n",
      "total step : 804 \n",
      "error : 1.045769, accuarcy : 0.714857\n",
      "total step : 805 \n",
      "error : 1.044980, accuarcy : 0.715358\n",
      "total step : 806 \n",
      "error : 1.044193, accuarcy : 0.715858\n",
      "total step : 807 \n",
      "error : 1.043407, accuarcy : 0.715858\n",
      "total step : 808 \n",
      "error : 1.042622, accuarcy : 0.715858\n",
      "total step : 809 \n",
      "error : 1.041839, accuarcy : 0.714857\n",
      "total step : 810 \n",
      "error : 1.041057, accuarcy : 0.714857\n",
      "total step : 811 \n",
      "error : 1.040277, accuarcy : 0.715858\n",
      "total step : 812 \n",
      "error : 1.039498, accuarcy : 0.715858\n",
      "total step : 813 \n",
      "error : 1.038721, accuarcy : 0.715858\n",
      "total step : 814 \n",
      "error : 1.037945, accuarcy : 0.715858\n",
      "total step : 815 \n",
      "error : 1.037171, accuarcy : 0.716358\n",
      "total step : 816 \n",
      "error : 1.036398, accuarcy : 0.716358\n",
      "total step : 817 \n",
      "error : 1.035626, accuarcy : 0.716358\n",
      "total step : 818 \n",
      "error : 1.034856, accuarcy : 0.716358\n",
      "total step : 819 \n",
      "error : 1.034087, accuarcy : 0.716358\n",
      "total step : 820 \n",
      "error : 1.033319, accuarcy : 0.716358\n",
      "total step : 821 \n",
      "error : 1.032553, accuarcy : 0.716858\n",
      "total step : 822 \n",
      "error : 1.031789, accuarcy : 0.716858\n",
      "total step : 823 \n",
      "error : 1.031025, accuarcy : 0.716858\n",
      "total step : 824 \n",
      "error : 1.030264, accuarcy : 0.717359\n",
      "total step : 825 \n",
      "error : 1.029503, accuarcy : 0.717359\n",
      "total step : 826 \n",
      "error : 1.028744, accuarcy : 0.717359\n",
      "total step : 827 \n",
      "error : 1.027986, accuarcy : 0.717859\n",
      "total step : 828 \n",
      "error : 1.027230, accuarcy : 0.717859\n",
      "total step : 829 \n",
      "error : 1.026475, accuarcy : 0.717859\n",
      "total step : 830 \n",
      "error : 1.025721, accuarcy : 0.717859\n",
      "total step : 831 \n",
      "error : 1.024969, accuarcy : 0.717859\n",
      "total step : 832 \n",
      "error : 1.024218, accuarcy : 0.717859\n",
      "total step : 833 \n",
      "error : 1.023468, accuarcy : 0.717859\n",
      "total step : 834 \n",
      "error : 1.022720, accuarcy : 0.717859\n",
      "total step : 835 \n",
      "error : 1.021973, accuarcy : 0.717859\n",
      "total step : 836 \n",
      "error : 1.021228, accuarcy : 0.718359\n",
      "total step : 837 \n",
      "error : 1.020484, accuarcy : 0.718859\n",
      "total step : 838 \n",
      "error : 1.019741, accuarcy : 0.718859\n",
      "total step : 839 \n",
      "error : 1.018999, accuarcy : 0.718859\n",
      "total step : 840 \n",
      "error : 1.018259, accuarcy : 0.719360\n",
      "total step : 841 \n",
      "error : 1.017520, accuarcy : 0.720360\n",
      "total step : 842 \n",
      "error : 1.016783, accuarcy : 0.720360\n",
      "total step : 843 \n",
      "error : 1.016047, accuarcy : 0.720360\n",
      "total step : 844 \n",
      "error : 1.015312, accuarcy : 0.720360\n",
      "total step : 845 \n",
      "error : 1.014578, accuarcy : 0.719860\n",
      "total step : 846 \n",
      "error : 1.013846, accuarcy : 0.719860\n",
      "total step : 847 \n",
      "error : 1.013115, accuarcy : 0.719860\n",
      "total step : 848 \n",
      "error : 1.012385, accuarcy : 0.719860\n",
      "total step : 849 \n",
      "error : 1.011657, accuarcy : 0.719860\n",
      "total step : 850 \n",
      "error : 1.010930, accuarcy : 0.720860\n",
      "total step : 851 \n",
      "error : 1.010204, accuarcy : 0.720860\n",
      "total step : 852 \n",
      "error : 1.009480, accuarcy : 0.720860\n",
      "total step : 853 \n",
      "error : 1.008756, accuarcy : 0.721361\n",
      "total step : 854 \n",
      "error : 1.008034, accuarcy : 0.721361\n",
      "total step : 855 \n",
      "error : 1.007314, accuarcy : 0.721361\n",
      "total step : 856 \n",
      "error : 1.006594, accuarcy : 0.722361\n",
      "total step : 857 \n",
      "error : 1.005876, accuarcy : 0.722361\n",
      "total step : 858 \n",
      "error : 1.005159, accuarcy : 0.722361\n",
      "total step : 859 \n",
      "error : 1.004444, accuarcy : 0.722361\n",
      "total step : 860 \n",
      "error : 1.003730, accuarcy : 0.722861\n",
      "total step : 861 \n",
      "error : 1.003017, accuarcy : 0.722861\n",
      "total step : 862 \n",
      "error : 1.002305, accuarcy : 0.722861\n",
      "total step : 863 \n",
      "error : 1.001594, accuarcy : 0.723362\n",
      "total step : 864 \n",
      "error : 1.000885, accuarcy : 0.723362\n",
      "total step : 865 \n",
      "error : 1.000177, accuarcy : 0.724362\n",
      "total step : 866 \n",
      "error : 0.999470, accuarcy : 0.724362\n",
      "total step : 867 \n",
      "error : 0.998765, accuarcy : 0.724362\n",
      "total step : 868 \n",
      "error : 0.998060, accuarcy : 0.724362\n",
      "total step : 869 \n",
      "error : 0.997357, accuarcy : 0.724862\n",
      "total step : 870 \n",
      "error : 0.996655, accuarcy : 0.724862\n",
      "total step : 871 \n",
      "error : 0.995955, accuarcy : 0.724862\n",
      "total step : 872 \n",
      "error : 0.995255, accuarcy : 0.726363\n",
      "total step : 873 \n",
      "error : 0.994557, accuarcy : 0.726363\n",
      "total step : 874 \n",
      "error : 0.993860, accuarcy : 0.726863\n",
      "total step : 875 \n",
      "error : 0.993164, accuarcy : 0.726863\n",
      "total step : 876 \n",
      "error : 0.992470, accuarcy : 0.726863\n",
      "total step : 877 \n",
      "error : 0.991776, accuarcy : 0.727364\n",
      "total step : 878 \n",
      "error : 0.991084, accuarcy : 0.727864\n",
      "total step : 879 \n",
      "error : 0.990393, accuarcy : 0.727864\n",
      "total step : 880 \n",
      "error : 0.989704, accuarcy : 0.728364\n",
      "total step : 881 \n",
      "error : 0.989015, accuarcy : 0.728364\n",
      "total step : 882 \n",
      "error : 0.988328, accuarcy : 0.728864\n",
      "total step : 883 \n",
      "error : 0.987641, accuarcy : 0.729865\n",
      "total step : 884 \n",
      "error : 0.986956, accuarcy : 0.729865\n",
      "total step : 885 \n",
      "error : 0.986273, accuarcy : 0.730365\n",
      "total step : 886 \n",
      "error : 0.985590, accuarcy : 0.730865\n",
      "total step : 887 \n",
      "error : 0.984909, accuarcy : 0.730865\n",
      "total step : 888 \n",
      "error : 0.984228, accuarcy : 0.730865\n",
      "total step : 889 \n",
      "error : 0.983549, accuarcy : 0.730865\n",
      "total step : 890 \n",
      "error : 0.982871, accuarcy : 0.730865\n",
      "total step : 891 \n",
      "error : 0.982194, accuarcy : 0.731366\n",
      "total step : 892 \n",
      "error : 0.981519, accuarcy : 0.731366\n",
      "total step : 893 \n",
      "error : 0.980844, accuarcy : 0.731866\n",
      "total step : 894 \n",
      "error : 0.980171, accuarcy : 0.732366\n",
      "total step : 895 \n",
      "error : 0.979499, accuarcy : 0.732366\n",
      "total step : 896 \n",
      "error : 0.978828, accuarcy : 0.732366\n",
      "total step : 897 \n",
      "error : 0.978158, accuarcy : 0.732866\n",
      "total step : 898 \n",
      "error : 0.977489, accuarcy : 0.732866\n",
      "total step : 899 \n",
      "error : 0.976822, accuarcy : 0.732866\n",
      "total step : 900 \n",
      "error : 0.976155, accuarcy : 0.732866\n",
      "total step : 901 \n",
      "error : 0.975490, accuarcy : 0.733367\n",
      "total step : 902 \n",
      "error : 0.974826, accuarcy : 0.733367\n",
      "total step : 903 \n",
      "error : 0.974163, accuarcy : 0.733367\n",
      "total step : 904 \n",
      "error : 0.973501, accuarcy : 0.733367\n",
      "total step : 905 \n",
      "error : 0.972840, accuarcy : 0.733367\n",
      "total step : 906 \n",
      "error : 0.972180, accuarcy : 0.733367\n",
      "total step : 907 \n",
      "error : 0.971522, accuarcy : 0.733367\n",
      "total step : 908 \n",
      "error : 0.970864, accuarcy : 0.733367\n",
      "total step : 909 \n",
      "error : 0.970208, accuarcy : 0.733367\n",
      "total step : 910 \n",
      "error : 0.969553, accuarcy : 0.733867\n",
      "total step : 911 \n",
      "error : 0.968899, accuarcy : 0.733867\n",
      "total step : 912 \n",
      "error : 0.968246, accuarcy : 0.733867\n",
      "total step : 913 \n",
      "error : 0.967594, accuarcy : 0.734367\n",
      "total step : 914 \n",
      "error : 0.966943, accuarcy : 0.734867\n",
      "total step : 915 \n",
      "error : 0.966293, accuarcy : 0.734867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 916 \n",
      "error : 0.965645, accuarcy : 0.734867\n",
      "total step : 917 \n",
      "error : 0.964997, accuarcy : 0.734867\n",
      "total step : 918 \n",
      "error : 0.964351, accuarcy : 0.734867\n",
      "total step : 919 \n",
      "error : 0.963705, accuarcy : 0.734867\n",
      "total step : 920 \n",
      "error : 0.963061, accuarcy : 0.734867\n",
      "total step : 921 \n",
      "error : 0.962418, accuarcy : 0.734867\n",
      "total step : 922 \n",
      "error : 0.961776, accuarcy : 0.734867\n",
      "total step : 923 \n",
      "error : 0.961135, accuarcy : 0.734867\n",
      "total step : 924 \n",
      "error : 0.960495, accuarcy : 0.735368\n",
      "total step : 925 \n",
      "error : 0.959856, accuarcy : 0.735368\n",
      "total step : 926 \n",
      "error : 0.959218, accuarcy : 0.735368\n",
      "total step : 927 \n",
      "error : 0.958581, accuarcy : 0.735368\n",
      "total step : 928 \n",
      "error : 0.957945, accuarcy : 0.735368\n",
      "total step : 929 \n",
      "error : 0.957311, accuarcy : 0.735368\n",
      "total step : 930 \n",
      "error : 0.956677, accuarcy : 0.735368\n",
      "total step : 931 \n",
      "error : 0.956044, accuarcy : 0.735368\n",
      "total step : 932 \n",
      "error : 0.955413, accuarcy : 0.735368\n",
      "total step : 933 \n",
      "error : 0.954782, accuarcy : 0.735368\n",
      "total step : 934 \n",
      "error : 0.954153, accuarcy : 0.735368\n",
      "total step : 935 \n",
      "error : 0.953525, accuarcy : 0.735368\n",
      "total step : 936 \n",
      "error : 0.952897, accuarcy : 0.735368\n",
      "total step : 937 \n",
      "error : 0.952271, accuarcy : 0.735368\n",
      "total step : 938 \n",
      "error : 0.951646, accuarcy : 0.735368\n",
      "total step : 939 \n",
      "error : 0.951021, accuarcy : 0.735368\n",
      "total step : 940 \n",
      "error : 0.950398, accuarcy : 0.735368\n",
      "total step : 941 \n",
      "error : 0.949776, accuarcy : 0.735868\n",
      "total step : 942 \n",
      "error : 0.949155, accuarcy : 0.735868\n",
      "total step : 943 \n",
      "error : 0.948534, accuarcy : 0.736368\n",
      "total step : 944 \n",
      "error : 0.947915, accuarcy : 0.736368\n",
      "total step : 945 \n",
      "error : 0.947297, accuarcy : 0.736368\n",
      "total step : 946 \n",
      "error : 0.946680, accuarcy : 0.736368\n",
      "total step : 947 \n",
      "error : 0.946064, accuarcy : 0.736368\n",
      "total step : 948 \n",
      "error : 0.945449, accuarcy : 0.736368\n",
      "total step : 949 \n",
      "error : 0.944835, accuarcy : 0.736368\n",
      "total step : 950 \n",
      "error : 0.944222, accuarcy : 0.736368\n",
      "total step : 951 \n",
      "error : 0.943610, accuarcy : 0.736868\n",
      "total step : 952 \n",
      "error : 0.942998, accuarcy : 0.736868\n",
      "total step : 953 \n",
      "error : 0.942388, accuarcy : 0.736868\n",
      "total step : 954 \n",
      "error : 0.941779, accuarcy : 0.736368\n",
      "total step : 955 \n",
      "error : 0.941171, accuarcy : 0.736368\n",
      "total step : 956 \n",
      "error : 0.940564, accuarcy : 0.736368\n",
      "total step : 957 \n",
      "error : 0.939958, accuarcy : 0.736868\n",
      "total step : 958 \n",
      "error : 0.939353, accuarcy : 0.736868\n",
      "total step : 959 \n",
      "error : 0.938748, accuarcy : 0.736868\n",
      "total step : 960 \n",
      "error : 0.938145, accuarcy : 0.736868\n",
      "total step : 961 \n",
      "error : 0.937543, accuarcy : 0.736868\n",
      "total step : 962 \n",
      "error : 0.936942, accuarcy : 0.737369\n",
      "total step : 963 \n",
      "error : 0.936341, accuarcy : 0.738369\n",
      "total step : 964 \n",
      "error : 0.935742, accuarcy : 0.738369\n",
      "total step : 965 \n",
      "error : 0.935144, accuarcy : 0.738369\n",
      "total step : 966 \n",
      "error : 0.934546, accuarcy : 0.738869\n",
      "total step : 967 \n",
      "error : 0.933950, accuarcy : 0.738869\n",
      "total step : 968 \n",
      "error : 0.933354, accuarcy : 0.738869\n",
      "total step : 969 \n",
      "error : 0.932760, accuarcy : 0.738869\n",
      "total step : 970 \n",
      "error : 0.932166, accuarcy : 0.738869\n",
      "total step : 971 \n",
      "error : 0.931574, accuarcy : 0.738869\n",
      "total step : 972 \n",
      "error : 0.930982, accuarcy : 0.738869\n",
      "total step : 973 \n",
      "error : 0.930391, accuarcy : 0.738869\n",
      "total step : 974 \n",
      "error : 0.929801, accuarcy : 0.739370\n",
      "total step : 975 \n",
      "error : 0.929213, accuarcy : 0.739370\n",
      "total step : 976 \n",
      "error : 0.928625, accuarcy : 0.739370\n",
      "total step : 977 \n",
      "error : 0.928038, accuarcy : 0.739370\n",
      "total step : 978 \n",
      "error : 0.927452, accuarcy : 0.739370\n",
      "total step : 979 \n",
      "error : 0.926867, accuarcy : 0.739370\n",
      "total step : 980 \n",
      "error : 0.926283, accuarcy : 0.739370\n",
      "total step : 981 \n",
      "error : 0.925699, accuarcy : 0.739870\n",
      "total step : 982 \n",
      "error : 0.925117, accuarcy : 0.740370\n",
      "total step : 983 \n",
      "error : 0.924536, accuarcy : 0.740370\n",
      "total step : 984 \n",
      "error : 0.923955, accuarcy : 0.740370\n",
      "total step : 985 \n",
      "error : 0.923376, accuarcy : 0.740370\n",
      "total step : 986 \n",
      "error : 0.922797, accuarcy : 0.740870\n",
      "total step : 987 \n",
      "error : 0.922219, accuarcy : 0.740870\n",
      "total step : 988 \n",
      "error : 0.921642, accuarcy : 0.740870\n",
      "total step : 989 \n",
      "error : 0.921067, accuarcy : 0.740870\n",
      "total step : 990 \n",
      "error : 0.920492, accuarcy : 0.740870\n",
      "total step : 991 \n",
      "error : 0.919918, accuarcy : 0.740870\n",
      "total step : 992 \n",
      "error : 0.919344, accuarcy : 0.740870\n",
      "total step : 993 \n",
      "error : 0.918772, accuarcy : 0.740870\n",
      "total step : 994 \n",
      "error : 0.918201, accuarcy : 0.740870\n",
      "total step : 995 \n",
      "error : 0.917630, accuarcy : 0.740870\n",
      "total step : 996 \n",
      "error : 0.917061, accuarcy : 0.740870\n",
      "total step : 997 \n",
      "error : 0.916492, accuarcy : 0.740870\n",
      "total step : 998 \n",
      "error : 0.915924, accuarcy : 0.740870\n",
      "total step : 999 \n",
      "error : 0.915357, accuarcy : 0.740870\n",
      "total step : 1000 \n",
      "error : 0.914791, accuarcy : 0.740870\n",
      "total step : 1001 \n",
      "error : 0.914226, accuarcy : 0.740870\n",
      "total step : 1002 \n",
      "error : 0.913662, accuarcy : 0.740870\n",
      "total step : 1003 \n",
      "error : 0.913099, accuarcy : 0.741371\n",
      "total step : 1004 \n",
      "error : 0.912536, accuarcy : 0.741371\n",
      "total step : 1005 \n",
      "error : 0.911975, accuarcy : 0.741371\n",
      "total step : 1006 \n",
      "error : 0.911414, accuarcy : 0.741371\n",
      "total step : 1007 \n",
      "error : 0.910854, accuarcy : 0.741871\n",
      "total step : 1008 \n",
      "error : 0.910295, accuarcy : 0.741871\n",
      "total step : 1009 \n",
      "error : 0.909737, accuarcy : 0.741871\n",
      "total step : 1010 \n",
      "error : 0.909180, accuarcy : 0.741871\n",
      "total step : 1011 \n",
      "error : 0.908623, accuarcy : 0.742371\n",
      "total step : 1012 \n",
      "error : 0.908068, accuarcy : 0.742371\n",
      "total step : 1013 \n",
      "error : 0.907513, accuarcy : 0.743372\n",
      "total step : 1014 \n",
      "error : 0.906959, accuarcy : 0.743372\n",
      "total step : 1015 \n",
      "error : 0.906406, accuarcy : 0.743872\n",
      "total step : 1016 \n",
      "error : 0.905854, accuarcy : 0.743872\n",
      "total step : 1017 \n",
      "error : 0.905303, accuarcy : 0.743872\n",
      "total step : 1018 \n",
      "error : 0.904753, accuarcy : 0.743872\n",
      "total step : 1019 \n",
      "error : 0.904203, accuarcy : 0.743872\n",
      "total step : 1020 \n",
      "error : 0.903654, accuarcy : 0.744372\n",
      "total step : 1021 \n",
      "error : 0.903107, accuarcy : 0.744372\n",
      "total step : 1022 \n",
      "error : 0.902560, accuarcy : 0.744372\n",
      "total step : 1023 \n",
      "error : 0.902013, accuarcy : 0.744372\n",
      "total step : 1024 \n",
      "error : 0.901468, accuarcy : 0.744872\n",
      "total step : 1025 \n",
      "error : 0.900924, accuarcy : 0.744872\n",
      "total step : 1026 \n",
      "error : 0.900380, accuarcy : 0.745373\n",
      "total step : 1027 \n",
      "error : 0.899837, accuarcy : 0.745373\n",
      "total step : 1028 \n",
      "error : 0.899295, accuarcy : 0.745373\n",
      "total step : 1029 \n",
      "error : 0.898754, accuarcy : 0.745373\n",
      "total step : 1030 \n",
      "error : 0.898214, accuarcy : 0.746373\n",
      "total step : 1031 \n",
      "error : 0.897674, accuarcy : 0.746373\n",
      "total step : 1032 \n",
      "error : 0.897135, accuarcy : 0.746873\n",
      "total step : 1033 \n",
      "error : 0.896597, accuarcy : 0.746873\n",
      "total step : 1034 \n",
      "error : 0.896060, accuarcy : 0.746873\n",
      "total step : 1035 \n",
      "error : 0.895524, accuarcy : 0.746873\n",
      "total step : 1036 \n",
      "error : 0.894989, accuarcy : 0.746873\n",
      "total step : 1037 \n",
      "error : 0.894454, accuarcy : 0.746873\n",
      "total step : 1038 \n",
      "error : 0.893920, accuarcy : 0.746873\n",
      "total step : 1039 \n",
      "error : 0.893387, accuarcy : 0.746873\n",
      "total step : 1040 \n",
      "error : 0.892855, accuarcy : 0.746873\n",
      "total step : 1041 \n",
      "error : 0.892324, accuarcy : 0.747874\n",
      "total step : 1042 \n",
      "error : 0.891793, accuarcy : 0.747874\n",
      "total step : 1043 \n",
      "error : 0.891263, accuarcy : 0.748374\n",
      "total step : 1044 \n",
      "error : 0.890735, accuarcy : 0.748374\n",
      "total step : 1045 \n",
      "error : 0.890206, accuarcy : 0.748874\n",
      "total step : 1046 \n",
      "error : 0.889679, accuarcy : 0.749375\n",
      "total step : 1047 \n",
      "error : 0.889152, accuarcy : 0.748874\n",
      "total step : 1048 \n",
      "error : 0.888627, accuarcy : 0.748874\n",
      "total step : 1049 \n",
      "error : 0.888102, accuarcy : 0.748874\n",
      "total step : 1050 \n",
      "error : 0.887577, accuarcy : 0.748874\n",
      "total step : 1051 \n",
      "error : 0.887054, accuarcy : 0.748874\n",
      "total step : 1052 \n",
      "error : 0.886531, accuarcy : 0.748874\n",
      "total step : 1053 \n",
      "error : 0.886010, accuarcy : 0.748874\n",
      "total step : 1054 \n",
      "error : 0.885488, accuarcy : 0.748374\n",
      "total step : 1055 \n",
      "error : 0.884968, accuarcy : 0.748374\n",
      "total step : 1056 \n",
      "error : 0.884449, accuarcy : 0.748374\n",
      "total step : 1057 \n",
      "error : 0.883930, accuarcy : 0.748874\n",
      "total step : 1058 \n",
      "error : 0.883412, accuarcy : 0.748874\n",
      "total step : 1059 \n",
      "error : 0.882895, accuarcy : 0.748874\n",
      "total step : 1060 \n",
      "error : 0.882378, accuarcy : 0.748874\n",
      "total step : 1061 \n",
      "error : 0.881863, accuarcy : 0.748874\n",
      "total step : 1062 \n",
      "error : 0.881348, accuarcy : 0.748874\n",
      "total step : 1063 \n",
      "error : 0.880834, accuarcy : 0.748874\n",
      "total step : 1064 \n",
      "error : 0.880321, accuarcy : 0.748874\n",
      "total step : 1065 \n",
      "error : 0.879808, accuarcy : 0.748874\n",
      "total step : 1066 \n",
      "error : 0.879296, accuarcy : 0.748374\n",
      "total step : 1067 \n",
      "error : 0.878785, accuarcy : 0.748374\n",
      "total step : 1068 \n",
      "error : 0.878275, accuarcy : 0.748374\n",
      "total step : 1069 \n",
      "error : 0.877765, accuarcy : 0.748374\n",
      "total step : 1070 \n",
      "error : 0.877257, accuarcy : 0.748374\n",
      "total step : 1071 \n",
      "error : 0.876748, accuarcy : 0.748374\n",
      "total step : 1072 \n",
      "error : 0.876241, accuarcy : 0.748374\n",
      "total step : 1073 \n",
      "error : 0.875735, accuarcy : 0.748374\n",
      "total step : 1074 \n",
      "error : 0.875229, accuarcy : 0.748374\n",
      "total step : 1075 \n",
      "error : 0.874724, accuarcy : 0.748374\n",
      "total step : 1076 \n",
      "error : 0.874220, accuarcy : 0.748374\n",
      "total step : 1077 \n",
      "error : 0.873716, accuarcy : 0.748874\n",
      "total step : 1078 \n",
      "error : 0.873213, accuarcy : 0.748874\n",
      "total step : 1079 \n",
      "error : 0.872711, accuarcy : 0.748874\n",
      "total step : 1080 \n",
      "error : 0.872210, accuarcy : 0.748874\n",
      "total step : 1081 \n",
      "error : 0.871709, accuarcy : 0.748874\n",
      "total step : 1082 \n",
      "error : 0.871209, accuarcy : 0.748874\n",
      "total step : 1083 \n",
      "error : 0.870710, accuarcy : 0.748874\n",
      "total step : 1084 \n",
      "error : 0.870212, accuarcy : 0.748874\n",
      "total step : 1085 \n",
      "error : 0.869714, accuarcy : 0.749375\n",
      "total step : 1086 \n",
      "error : 0.869217, accuarcy : 0.749375\n",
      "total step : 1087 \n",
      "error : 0.868721, accuarcy : 0.750375\n",
      "total step : 1088 \n",
      "error : 0.868226, accuarcy : 0.751376\n",
      "total step : 1089 \n",
      "error : 0.867731, accuarcy : 0.751376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 1090 \n",
      "error : 0.867237, accuarcy : 0.751376\n",
      "total step : 1091 \n",
      "error : 0.866744, accuarcy : 0.751376\n",
      "total step : 1092 \n",
      "error : 0.866251, accuarcy : 0.751376\n",
      "total step : 1093 \n",
      "error : 0.865759, accuarcy : 0.751376\n",
      "total step : 1094 \n",
      "error : 0.865268, accuarcy : 0.751376\n",
      "total step : 1095 \n",
      "error : 0.864778, accuarcy : 0.751876\n",
      "total step : 1096 \n",
      "error : 0.864288, accuarcy : 0.751876\n",
      "total step : 1097 \n",
      "error : 0.863799, accuarcy : 0.751876\n",
      "total step : 1098 \n",
      "error : 0.863311, accuarcy : 0.751876\n",
      "total step : 1099 \n",
      "error : 0.862823, accuarcy : 0.751876\n",
      "total step : 1100 \n",
      "error : 0.862336, accuarcy : 0.751876\n",
      "total step : 1101 \n",
      "error : 0.861850, accuarcy : 0.751876\n",
      "total step : 1102 \n",
      "error : 0.861364, accuarcy : 0.751876\n",
      "total step : 1103 \n",
      "error : 0.860880, accuarcy : 0.752376\n",
      "total step : 1104 \n",
      "error : 0.860396, accuarcy : 0.752376\n",
      "total step : 1105 \n",
      "error : 0.859912, accuarcy : 0.752376\n",
      "total step : 1106 \n",
      "error : 0.859430, accuarcy : 0.752376\n",
      "total step : 1107 \n",
      "error : 0.858948, accuarcy : 0.752376\n",
      "total step : 1108 \n",
      "error : 0.858466, accuarcy : 0.752376\n",
      "total step : 1109 \n",
      "error : 0.857986, accuarcy : 0.752876\n",
      "total step : 1110 \n",
      "error : 0.857506, accuarcy : 0.753377\n",
      "total step : 1111 \n",
      "error : 0.857027, accuarcy : 0.753377\n",
      "total step : 1112 \n",
      "error : 0.856548, accuarcy : 0.753877\n",
      "total step : 1113 \n",
      "error : 0.856070, accuarcy : 0.753877\n",
      "total step : 1114 \n",
      "error : 0.855593, accuarcy : 0.753877\n",
      "total step : 1115 \n",
      "error : 0.855117, accuarcy : 0.754377\n",
      "total step : 1116 \n",
      "error : 0.854641, accuarcy : 0.754377\n",
      "total step : 1117 \n",
      "error : 0.854166, accuarcy : 0.754377\n",
      "total step : 1118 \n",
      "error : 0.853691, accuarcy : 0.754377\n",
      "total step : 1119 \n",
      "error : 0.853218, accuarcy : 0.754377\n",
      "total step : 1120 \n",
      "error : 0.852744, accuarcy : 0.754377\n",
      "total step : 1121 \n",
      "error : 0.852272, accuarcy : 0.754877\n",
      "total step : 1122 \n",
      "error : 0.851800, accuarcy : 0.755378\n",
      "total step : 1123 \n",
      "error : 0.851329, accuarcy : 0.755378\n",
      "total step : 1124 \n",
      "error : 0.850859, accuarcy : 0.755378\n",
      "total step : 1125 \n",
      "error : 0.850389, accuarcy : 0.755378\n",
      "total step : 1126 \n",
      "error : 0.849920, accuarcy : 0.755378\n",
      "total step : 1127 \n",
      "error : 0.849452, accuarcy : 0.755378\n",
      "total step : 1128 \n",
      "error : 0.848984, accuarcy : 0.755378\n",
      "total step : 1129 \n",
      "error : 0.848517, accuarcy : 0.755378\n",
      "total step : 1130 \n",
      "error : 0.848051, accuarcy : 0.755878\n",
      "total step : 1131 \n",
      "error : 0.847585, accuarcy : 0.755878\n",
      "total step : 1132 \n",
      "error : 0.847120, accuarcy : 0.756378\n",
      "total step : 1133 \n",
      "error : 0.846656, accuarcy : 0.756378\n",
      "total step : 1134 \n",
      "error : 0.846192, accuarcy : 0.755878\n",
      "total step : 1135 \n",
      "error : 0.845729, accuarcy : 0.756378\n",
      "total step : 1136 \n",
      "error : 0.845266, accuarcy : 0.756378\n",
      "total step : 1137 \n",
      "error : 0.844805, accuarcy : 0.756378\n",
      "total step : 1138 \n",
      "error : 0.844343, accuarcy : 0.756378\n",
      "total step : 1139 \n",
      "error : 0.843883, accuarcy : 0.756378\n",
      "total step : 1140 \n",
      "error : 0.843423, accuarcy : 0.756378\n",
      "total step : 1141 \n",
      "error : 0.842964, accuarcy : 0.756378\n",
      "total step : 1142 \n",
      "error : 0.842506, accuarcy : 0.756378\n",
      "total step : 1143 \n",
      "error : 0.842048, accuarcy : 0.756378\n",
      "total step : 1144 \n",
      "error : 0.841590, accuarcy : 0.756378\n",
      "total step : 1145 \n",
      "error : 0.841134, accuarcy : 0.756378\n",
      "total step : 1146 \n",
      "error : 0.840678, accuarcy : 0.756378\n",
      "total step : 1147 \n",
      "error : 0.840223, accuarcy : 0.756378\n",
      "total step : 1148 \n",
      "error : 0.839768, accuarcy : 0.757379\n",
      "total step : 1149 \n",
      "error : 0.839314, accuarcy : 0.757379\n",
      "total step : 1150 \n",
      "error : 0.838860, accuarcy : 0.757379\n",
      "total step : 1151 \n",
      "error : 0.838408, accuarcy : 0.757379\n",
      "total step : 1152 \n",
      "error : 0.837956, accuarcy : 0.757879\n",
      "total step : 1153 \n",
      "error : 0.837504, accuarcy : 0.758379\n",
      "total step : 1154 \n",
      "error : 0.837053, accuarcy : 0.758379\n",
      "total step : 1155 \n",
      "error : 0.836603, accuarcy : 0.758379\n",
      "total step : 1156 \n",
      "error : 0.836153, accuarcy : 0.758379\n",
      "total step : 1157 \n",
      "error : 0.835704, accuarcy : 0.758379\n",
      "total step : 1158 \n",
      "error : 0.835256, accuarcy : 0.758379\n",
      "total step : 1159 \n",
      "error : 0.834808, accuarcy : 0.758379\n",
      "total step : 1160 \n",
      "error : 0.834361, accuarcy : 0.758379\n",
      "total step : 1161 \n",
      "error : 0.833915, accuarcy : 0.758379\n",
      "total step : 1162 \n",
      "error : 0.833469, accuarcy : 0.758379\n",
      "total step : 1163 \n",
      "error : 0.833024, accuarcy : 0.758379\n",
      "total step : 1164 \n",
      "error : 0.832579, accuarcy : 0.758379\n",
      "total step : 1165 \n",
      "error : 0.832135, accuarcy : 0.758379\n",
      "total step : 1166 \n",
      "error : 0.831692, accuarcy : 0.758379\n",
      "total step : 1167 \n",
      "error : 0.831249, accuarcy : 0.758379\n",
      "total step : 1168 \n",
      "error : 0.830807, accuarcy : 0.758379\n",
      "total step : 1169 \n",
      "error : 0.830365, accuarcy : 0.758379\n",
      "total step : 1170 \n",
      "error : 0.829924, accuarcy : 0.758879\n",
      "total step : 1171 \n",
      "error : 0.829484, accuarcy : 0.758879\n",
      "total step : 1172 \n",
      "error : 0.829044, accuarcy : 0.758879\n",
      "total step : 1173 \n",
      "error : 0.828605, accuarcy : 0.758879\n",
      "total step : 1174 \n",
      "error : 0.828166, accuarcy : 0.758879\n",
      "total step : 1175 \n",
      "error : 0.827729, accuarcy : 0.758879\n",
      "total step : 1176 \n",
      "error : 0.827291, accuarcy : 0.758879\n",
      "total step : 1177 \n",
      "error : 0.826855, accuarcy : 0.758879\n",
      "total step : 1178 \n",
      "error : 0.826418, accuarcy : 0.758879\n",
      "total step : 1179 \n",
      "error : 0.825983, accuarcy : 0.758879\n",
      "total step : 1180 \n",
      "error : 0.825548, accuarcy : 0.758879\n",
      "total step : 1181 \n",
      "error : 0.825114, accuarcy : 0.759380\n",
      "total step : 1182 \n",
      "error : 0.824680, accuarcy : 0.759380\n",
      "total step : 1183 \n",
      "error : 0.824247, accuarcy : 0.759380\n",
      "total step : 1184 \n",
      "error : 0.823814, accuarcy : 0.759380\n",
      "total step : 1185 \n",
      "error : 0.823382, accuarcy : 0.759380\n",
      "total step : 1186 \n",
      "error : 0.822951, accuarcy : 0.759380\n",
      "total step : 1187 \n",
      "error : 0.822520, accuarcy : 0.759880\n",
      "total step : 1188 \n",
      "error : 0.822090, accuarcy : 0.759880\n",
      "total step : 1189 \n",
      "error : 0.821661, accuarcy : 0.759880\n",
      "total step : 1190 \n",
      "error : 0.821232, accuarcy : 0.759880\n",
      "total step : 1191 \n",
      "error : 0.820803, accuarcy : 0.759880\n",
      "total step : 1192 \n",
      "error : 0.820375, accuarcy : 0.760380\n",
      "total step : 1193 \n",
      "error : 0.819948, accuarcy : 0.760380\n",
      "total step : 1194 \n",
      "error : 0.819522, accuarcy : 0.760880\n",
      "total step : 1195 \n",
      "error : 0.819095, accuarcy : 0.760880\n",
      "total step : 1196 \n",
      "error : 0.818670, accuarcy : 0.760880\n",
      "total step : 1197 \n",
      "error : 0.818245, accuarcy : 0.760880\n",
      "total step : 1198 \n",
      "error : 0.817821, accuarcy : 0.761381\n",
      "total step : 1199 \n",
      "error : 0.817397, accuarcy : 0.761881\n",
      "total step : 1200 \n",
      "error : 0.816974, accuarcy : 0.761881\n",
      "total step : 1201 \n",
      "error : 0.816551, accuarcy : 0.761881\n",
      "total step : 1202 \n",
      "error : 0.816129, accuarcy : 0.761881\n",
      "total step : 1203 \n",
      "error : 0.815708, accuarcy : 0.761881\n",
      "total step : 1204 \n",
      "error : 0.815287, accuarcy : 0.761881\n",
      "total step : 1205 \n",
      "error : 0.814866, accuarcy : 0.761881\n",
      "total step : 1206 \n",
      "error : 0.814447, accuarcy : 0.762381\n",
      "total step : 1207 \n",
      "error : 0.814028, accuarcy : 0.762381\n",
      "total step : 1208 \n",
      "error : 0.813609, accuarcy : 0.762381\n",
      "total step : 1209 \n",
      "error : 0.813191, accuarcy : 0.762381\n",
      "total step : 1210 \n",
      "error : 0.812773, accuarcy : 0.762381\n",
      "total step : 1211 \n",
      "error : 0.812356, accuarcy : 0.762381\n",
      "total step : 1212 \n",
      "error : 0.811940, accuarcy : 0.762381\n",
      "total step : 1213 \n",
      "error : 0.811524, accuarcy : 0.762881\n",
      "total step : 1214 \n",
      "error : 0.811109, accuarcy : 0.762881\n",
      "total step : 1215 \n",
      "error : 0.810694, accuarcy : 0.763382\n",
      "total step : 1216 \n",
      "error : 0.810280, accuarcy : 0.763382\n",
      "total step : 1217 \n",
      "error : 0.809867, accuarcy : 0.763382\n",
      "total step : 1218 \n",
      "error : 0.809454, accuarcy : 0.763382\n",
      "total step : 1219 \n",
      "error : 0.809041, accuarcy : 0.763882\n",
      "total step : 1220 \n",
      "error : 0.808629, accuarcy : 0.763882\n",
      "total step : 1221 \n",
      "error : 0.808218, accuarcy : 0.763882\n",
      "total step : 1222 \n",
      "error : 0.807807, accuarcy : 0.763882\n",
      "total step : 1223 \n",
      "error : 0.807397, accuarcy : 0.764382\n",
      "total step : 1224 \n",
      "error : 0.806987, accuarcy : 0.764382\n",
      "total step : 1225 \n",
      "error : 0.806578, accuarcy : 0.764882\n",
      "total step : 1226 \n",
      "error : 0.806169, accuarcy : 0.765383\n",
      "total step : 1227 \n",
      "error : 0.805761, accuarcy : 0.765383\n",
      "total step : 1228 \n",
      "error : 0.805354, accuarcy : 0.765383\n",
      "total step : 1229 \n",
      "error : 0.804947, accuarcy : 0.766383\n",
      "total step : 1230 \n",
      "error : 0.804540, accuarcy : 0.766383\n",
      "total step : 1231 \n",
      "error : 0.804134, accuarcy : 0.766383\n",
      "total step : 1232 \n",
      "error : 0.803729, accuarcy : 0.766383\n",
      "total step : 1233 \n",
      "error : 0.803324, accuarcy : 0.766383\n",
      "total step : 1234 \n",
      "error : 0.802920, accuarcy : 0.766383\n",
      "total step : 1235 \n",
      "error : 0.802516, accuarcy : 0.766383\n",
      "total step : 1236 \n",
      "error : 0.802113, accuarcy : 0.766383\n",
      "total step : 1237 \n",
      "error : 0.801710, accuarcy : 0.766383\n",
      "total step : 1238 \n",
      "error : 0.801308, accuarcy : 0.766383\n",
      "total step : 1239 \n",
      "error : 0.800906, accuarcy : 0.766383\n",
      "total step : 1240 \n",
      "error : 0.800505, accuarcy : 0.766883\n",
      "total step : 1241 \n",
      "error : 0.800105, accuarcy : 0.766883\n",
      "total step : 1242 \n",
      "error : 0.799705, accuarcy : 0.766883\n",
      "total step : 1243 \n",
      "error : 0.799305, accuarcy : 0.766883\n",
      "total step : 1244 \n",
      "error : 0.798906, accuarcy : 0.766883\n",
      "total step : 1245 \n",
      "error : 0.798508, accuarcy : 0.766883\n",
      "total step : 1246 \n",
      "error : 0.798110, accuarcy : 0.766883\n",
      "total step : 1247 \n",
      "error : 0.797712, accuarcy : 0.766883\n",
      "total step : 1248 \n",
      "error : 0.797316, accuarcy : 0.766883\n",
      "total step : 1249 \n",
      "error : 0.796919, accuarcy : 0.766883\n",
      "total step : 1250 \n",
      "error : 0.796523, accuarcy : 0.766883\n",
      "total step : 1251 \n",
      "error : 0.796128, accuarcy : 0.766883\n",
      "total step : 1252 \n",
      "error : 0.795733, accuarcy : 0.766883\n",
      "total step : 1253 \n",
      "error : 0.795339, accuarcy : 0.767384\n",
      "total step : 1254 \n",
      "error : 0.794945, accuarcy : 0.767384\n",
      "total step : 1255 \n",
      "error : 0.794552, accuarcy : 0.768884\n",
      "total step : 1256 \n",
      "error : 0.794159, accuarcy : 0.769385\n",
      "total step : 1257 \n",
      "error : 0.793767, accuarcy : 0.769385\n",
      "total step : 1258 \n",
      "error : 0.793375, accuarcy : 0.769385\n",
      "total step : 1259 \n",
      "error : 0.792984, accuarcy : 0.769385\n",
      "total step : 1260 \n",
      "error : 0.792594, accuarcy : 0.769385\n",
      "total step : 1261 \n",
      "error : 0.792203, accuarcy : 0.769385\n",
      "total step : 1262 \n",
      "error : 0.791814, accuarcy : 0.769885\n",
      "total step : 1263 \n",
      "error : 0.791425, accuarcy : 0.769885\n",
      "total step : 1264 \n",
      "error : 0.791036, accuarcy : 0.769885\n",
      "total step : 1265 \n",
      "error : 0.790648, accuarcy : 0.769885\n",
      "total step : 1266 \n",
      "error : 0.790260, accuarcy : 0.769885\n",
      "total step : 1267 \n",
      "error : 0.789873, accuarcy : 0.769885\n",
      "total step : 1268 \n",
      "error : 0.789487, accuarcy : 0.770385\n",
      "total step : 1269 \n",
      "error : 0.789101, accuarcy : 0.770885\n",
      "total step : 1270 \n",
      "error : 0.788715, accuarcy : 0.770885\n",
      "total step : 1271 \n",
      "error : 0.788330, accuarcy : 0.770885\n",
      "total step : 1272 \n",
      "error : 0.787945, accuarcy : 0.770885\n",
      "total step : 1273 \n",
      "error : 0.787561, accuarcy : 0.771386\n",
      "total step : 1274 \n",
      "error : 0.787178, accuarcy : 0.771386\n",
      "total step : 1275 \n",
      "error : 0.786794, accuarcy : 0.771386\n",
      "total step : 1276 \n",
      "error : 0.786412, accuarcy : 0.771386\n",
      "total step : 1277 \n",
      "error : 0.786030, accuarcy : 0.771386\n",
      "total step : 1278 \n",
      "error : 0.785648, accuarcy : 0.771386\n",
      "total step : 1279 \n",
      "error : 0.785267, accuarcy : 0.771386\n",
      "total step : 1280 \n",
      "error : 0.784886, accuarcy : 0.771386\n",
      "total step : 1281 \n",
      "error : 0.784506, accuarcy : 0.771386\n",
      "total step : 1282 \n",
      "error : 0.784127, accuarcy : 0.771386\n",
      "total step : 1283 \n",
      "error : 0.783747, accuarcy : 0.771386\n",
      "total step : 1284 \n",
      "error : 0.783369, accuarcy : 0.771886\n",
      "total step : 1285 \n",
      "error : 0.782991, accuarcy : 0.771886\n",
      "total step : 1286 \n",
      "error : 0.782613, accuarcy : 0.771886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 1287 \n",
      "error : 0.782236, accuarcy : 0.771886\n",
      "total step : 1288 \n",
      "error : 0.781859, accuarcy : 0.771886\n",
      "total step : 1289 \n",
      "error : 0.781483, accuarcy : 0.771886\n",
      "total step : 1290 \n",
      "error : 0.781107, accuarcy : 0.771886\n",
      "total step : 1291 \n",
      "error : 0.780732, accuarcy : 0.771886\n",
      "total step : 1292 \n",
      "error : 0.780357, accuarcy : 0.771886\n",
      "total step : 1293 \n",
      "error : 0.779983, accuarcy : 0.771886\n",
      "total step : 1294 \n",
      "error : 0.779609, accuarcy : 0.772386\n",
      "total step : 1295 \n",
      "error : 0.779236, accuarcy : 0.772386\n",
      "total step : 1296 \n",
      "error : 0.778863, accuarcy : 0.772386\n",
      "total step : 1297 \n",
      "error : 0.778490, accuarcy : 0.772386\n",
      "total step : 1298 \n",
      "error : 0.778118, accuarcy : 0.772386\n",
      "total step : 1299 \n",
      "error : 0.777747, accuarcy : 0.772386\n",
      "total step : 1300 \n",
      "error : 0.777376, accuarcy : 0.772386\n",
      "total step : 1301 \n",
      "error : 0.777005, accuarcy : 0.772386\n",
      "total step : 1302 \n",
      "error : 0.776635, accuarcy : 0.772386\n",
      "total step : 1303 \n",
      "error : 0.776266, accuarcy : 0.772386\n",
      "total step : 1304 \n",
      "error : 0.775897, accuarcy : 0.772886\n",
      "total step : 1305 \n",
      "error : 0.775528, accuarcy : 0.772886\n",
      "total step : 1306 \n",
      "error : 0.775160, accuarcy : 0.773887\n",
      "total step : 1307 \n",
      "error : 0.774792, accuarcy : 0.774887\n",
      "total step : 1308 \n",
      "error : 0.774425, accuarcy : 0.774887\n",
      "total step : 1309 \n",
      "error : 0.774058, accuarcy : 0.774887\n",
      "total step : 1310 \n",
      "error : 0.773692, accuarcy : 0.775388\n",
      "total step : 1311 \n",
      "error : 0.773326, accuarcy : 0.775388\n",
      "total step : 1312 \n",
      "error : 0.772961, accuarcy : 0.775888\n",
      "total step : 1313 \n",
      "error : 0.772596, accuarcy : 0.775888\n",
      "total step : 1314 \n",
      "error : 0.772232, accuarcy : 0.775888\n",
      "total step : 1315 \n",
      "error : 0.771868, accuarcy : 0.776388\n",
      "total step : 1316 \n",
      "error : 0.771504, accuarcy : 0.776388\n",
      "total step : 1317 \n",
      "error : 0.771141, accuarcy : 0.776388\n",
      "total step : 1318 \n",
      "error : 0.770779, accuarcy : 0.776888\n",
      "total step : 1319 \n",
      "error : 0.770417, accuarcy : 0.776888\n",
      "total step : 1320 \n",
      "error : 0.770055, accuarcy : 0.776888\n",
      "total step : 1321 \n",
      "error : 0.769694, accuarcy : 0.776888\n",
      "total step : 1322 \n",
      "error : 0.769333, accuarcy : 0.776888\n",
      "total step : 1323 \n",
      "error : 0.768973, accuarcy : 0.776888\n",
      "total step : 1324 \n",
      "error : 0.768613, accuarcy : 0.776888\n",
      "total step : 1325 \n",
      "error : 0.768254, accuarcy : 0.776888\n",
      "total step : 1326 \n",
      "error : 0.767895, accuarcy : 0.776888\n",
      "total step : 1327 \n",
      "error : 0.767536, accuarcy : 0.776888\n",
      "total step : 1328 \n",
      "error : 0.767178, accuarcy : 0.776888\n",
      "total step : 1329 \n",
      "error : 0.766821, accuarcy : 0.776888\n",
      "total step : 1330 \n",
      "error : 0.766464, accuarcy : 0.776888\n",
      "total step : 1331 \n",
      "error : 0.766107, accuarcy : 0.777389\n",
      "total step : 1332 \n",
      "error : 0.765751, accuarcy : 0.777389\n",
      "total step : 1333 \n",
      "error : 0.765395, accuarcy : 0.777389\n",
      "total step : 1334 \n",
      "error : 0.765040, accuarcy : 0.777389\n",
      "total step : 1335 \n",
      "error : 0.764685, accuarcy : 0.777389\n",
      "total step : 1336 \n",
      "error : 0.764330, accuarcy : 0.777389\n",
      "total step : 1337 \n",
      "error : 0.763976, accuarcy : 0.777889\n",
      "total step : 1338 \n",
      "error : 0.763623, accuarcy : 0.778389\n",
      "total step : 1339 \n",
      "error : 0.763270, accuarcy : 0.778389\n",
      "total step : 1340 \n",
      "error : 0.762917, accuarcy : 0.778389\n",
      "total step : 1341 \n",
      "error : 0.762565, accuarcy : 0.778389\n",
      "total step : 1342 \n",
      "error : 0.762213, accuarcy : 0.778389\n",
      "total step : 1343 \n",
      "error : 0.761861, accuarcy : 0.778389\n",
      "total step : 1344 \n",
      "error : 0.761511, accuarcy : 0.778389\n",
      "total step : 1345 \n",
      "error : 0.761160, accuarcy : 0.778389\n",
      "total step : 1346 \n",
      "error : 0.760810, accuarcy : 0.779390\n",
      "total step : 1347 \n",
      "error : 0.760460, accuarcy : 0.779390\n",
      "total step : 1348 \n",
      "error : 0.760111, accuarcy : 0.779390\n",
      "total step : 1349 \n",
      "error : 0.759763, accuarcy : 0.779890\n",
      "total step : 1350 \n",
      "error : 0.759414, accuarcy : 0.779890\n",
      "total step : 1351 \n",
      "error : 0.759066, accuarcy : 0.779890\n",
      "total step : 1352 \n",
      "error : 0.758719, accuarcy : 0.779890\n",
      "total step : 1353 \n",
      "error : 0.758372, accuarcy : 0.779890\n",
      "total step : 1354 \n",
      "error : 0.758025, accuarcy : 0.779890\n",
      "total step : 1355 \n",
      "error : 0.757679, accuarcy : 0.779890\n",
      "total step : 1356 \n",
      "error : 0.757333, accuarcy : 0.779890\n",
      "total step : 1357 \n",
      "error : 0.756988, accuarcy : 0.779890\n",
      "total step : 1358 \n",
      "error : 0.756643, accuarcy : 0.779890\n",
      "total step : 1359 \n",
      "error : 0.756299, accuarcy : 0.779890\n",
      "total step : 1360 \n",
      "error : 0.755955, accuarcy : 0.779890\n",
      "total step : 1361 \n",
      "error : 0.755611, accuarcy : 0.779890\n",
      "total step : 1362 \n",
      "error : 0.755268, accuarcy : 0.780890\n",
      "total step : 1363 \n",
      "error : 0.754925, accuarcy : 0.780890\n",
      "total step : 1364 \n",
      "error : 0.754583, accuarcy : 0.780890\n",
      "total step : 1365 \n",
      "error : 0.754241, accuarcy : 0.781391\n",
      "total step : 1366 \n",
      "error : 0.753900, accuarcy : 0.781391\n",
      "total step : 1367 \n",
      "error : 0.753559, accuarcy : 0.781391\n",
      "total step : 1368 \n",
      "error : 0.753218, accuarcy : 0.781391\n",
      "total step : 1369 \n",
      "error : 0.752878, accuarcy : 0.781391\n",
      "total step : 1370 \n",
      "error : 0.752538, accuarcy : 0.781891\n",
      "total step : 1371 \n",
      "error : 0.752199, accuarcy : 0.781891\n",
      "total step : 1372 \n",
      "error : 0.751860, accuarcy : 0.781891\n",
      "total step : 1373 \n",
      "error : 0.751521, accuarcy : 0.781891\n",
      "total step : 1374 \n",
      "error : 0.751183, accuarcy : 0.781891\n",
      "total step : 1375 \n",
      "error : 0.750845, accuarcy : 0.781891\n",
      "total step : 1376 \n",
      "error : 0.750508, accuarcy : 0.781891\n",
      "total step : 1377 \n",
      "error : 0.750171, accuarcy : 0.781891\n",
      "total step : 1378 \n",
      "error : 0.749834, accuarcy : 0.782391\n",
      "total step : 1379 \n",
      "error : 0.749498, accuarcy : 0.782391\n",
      "total step : 1380 \n",
      "error : 0.749163, accuarcy : 0.782391\n",
      "total step : 1381 \n",
      "error : 0.748827, accuarcy : 0.782391\n",
      "total step : 1382 \n",
      "error : 0.748493, accuarcy : 0.782391\n",
      "total step : 1383 \n",
      "error : 0.748158, accuarcy : 0.782891\n",
      "total step : 1384 \n",
      "error : 0.747824, accuarcy : 0.782891\n",
      "total step : 1385 \n",
      "error : 0.747491, accuarcy : 0.782891\n",
      "total step : 1386 \n",
      "error : 0.747157, accuarcy : 0.782891\n",
      "total step : 1387 \n",
      "error : 0.746825, accuarcy : 0.782891\n",
      "total step : 1388 \n",
      "error : 0.746492, accuarcy : 0.782891\n",
      "total step : 1389 \n",
      "error : 0.746160, accuarcy : 0.782891\n",
      "total step : 1390 \n",
      "error : 0.745829, accuarcy : 0.782891\n",
      "total step : 1391 \n",
      "error : 0.745497, accuarcy : 0.783392\n",
      "total step : 1392 \n",
      "error : 0.745167, accuarcy : 0.783392\n",
      "total step : 1393 \n",
      "error : 0.744836, accuarcy : 0.783392\n",
      "total step : 1394 \n",
      "error : 0.744506, accuarcy : 0.783392\n",
      "total step : 1395 \n",
      "error : 0.744177, accuarcy : 0.783392\n",
      "total step : 1396 \n",
      "error : 0.743847, accuarcy : 0.783392\n",
      "total step : 1397 \n",
      "error : 0.743519, accuarcy : 0.783392\n",
      "total step : 1398 \n",
      "error : 0.743190, accuarcy : 0.783392\n",
      "total step : 1399 \n",
      "error : 0.742862, accuarcy : 0.783392\n",
      "total step : 1400 \n",
      "error : 0.742535, accuarcy : 0.783392\n",
      "total step : 1401 \n",
      "error : 0.742207, accuarcy : 0.783392\n",
      "total step : 1402 \n",
      "error : 0.741881, accuarcy : 0.784392\n",
      "total step : 1403 \n",
      "error : 0.741554, accuarcy : 0.784392\n",
      "total step : 1404 \n",
      "error : 0.741228, accuarcy : 0.784392\n",
      "total step : 1405 \n",
      "error : 0.740903, accuarcy : 0.784392\n",
      "total step : 1406 \n",
      "error : 0.740577, accuarcy : 0.784392\n",
      "total step : 1407 \n",
      "error : 0.740252, accuarcy : 0.784392\n",
      "total step : 1408 \n",
      "error : 0.739928, accuarcy : 0.784392\n",
      "total step : 1409 \n",
      "error : 0.739604, accuarcy : 0.784392\n",
      "total step : 1410 \n",
      "error : 0.739280, accuarcy : 0.784892\n",
      "total step : 1411 \n",
      "error : 0.738957, accuarcy : 0.784892\n",
      "total step : 1412 \n",
      "error : 0.738634, accuarcy : 0.784892\n",
      "total step : 1413 \n",
      "error : 0.738312, accuarcy : 0.784892\n",
      "total step : 1414 \n",
      "error : 0.737989, accuarcy : 0.784892\n",
      "total step : 1415 \n",
      "error : 0.737668, accuarcy : 0.784892\n",
      "total step : 1416 \n",
      "error : 0.737346, accuarcy : 0.784892\n",
      "total step : 1417 \n",
      "error : 0.737025, accuarcy : 0.784892\n",
      "total step : 1418 \n",
      "error : 0.736705, accuarcy : 0.784892\n",
      "total step : 1419 \n",
      "error : 0.736385, accuarcy : 0.784892\n",
      "total step : 1420 \n",
      "error : 0.736065, accuarcy : 0.784892\n",
      "total step : 1421 \n",
      "error : 0.735745, accuarcy : 0.784892\n",
      "total step : 1422 \n",
      "error : 0.735426, accuarcy : 0.784892\n",
      "total step : 1423 \n",
      "error : 0.735108, accuarcy : 0.784892\n",
      "total step : 1424 \n",
      "error : 0.734789, accuarcy : 0.784892\n",
      "total step : 1425 \n",
      "error : 0.734471, accuarcy : 0.784892\n",
      "total step : 1426 \n",
      "error : 0.734154, accuarcy : 0.785393\n",
      "total step : 1427 \n",
      "error : 0.733837, accuarcy : 0.785393\n",
      "total step : 1428 \n",
      "error : 0.733520, accuarcy : 0.785393\n",
      "total step : 1429 \n",
      "error : 0.733204, accuarcy : 0.785393\n",
      "total step : 1430 \n",
      "error : 0.732888, accuarcy : 0.785393\n",
      "total step : 1431 \n",
      "error : 0.732572, accuarcy : 0.785893\n",
      "total step : 1432 \n",
      "error : 0.732257, accuarcy : 0.785893\n",
      "total step : 1433 \n",
      "error : 0.731942, accuarcy : 0.785893\n",
      "total step : 1434 \n",
      "error : 0.731627, accuarcy : 0.786393\n",
      "total step : 1435 \n",
      "error : 0.731313, accuarcy : 0.786393\n",
      "total step : 1436 \n",
      "error : 0.730999, accuarcy : 0.786393\n",
      "total step : 1437 \n",
      "error : 0.730686, accuarcy : 0.786393\n",
      "total step : 1438 \n",
      "error : 0.730373, accuarcy : 0.786393\n",
      "total step : 1439 \n",
      "error : 0.730060, accuarcy : 0.786893\n",
      "total step : 1440 \n",
      "error : 0.729748, accuarcy : 0.786893\n",
      "total step : 1441 \n",
      "error : 0.729436, accuarcy : 0.786893\n",
      "total step : 1442 \n",
      "error : 0.729125, accuarcy : 0.786893\n",
      "total step : 1443 \n",
      "error : 0.728813, accuarcy : 0.786893\n",
      "total step : 1444 \n",
      "error : 0.728503, accuarcy : 0.786893\n",
      "total step : 1445 \n",
      "error : 0.728192, accuarcy : 0.787394\n",
      "total step : 1446 \n",
      "error : 0.727882, accuarcy : 0.787394\n",
      "total step : 1447 \n",
      "error : 0.727572, accuarcy : 0.787894\n",
      "total step : 1448 \n",
      "error : 0.727263, accuarcy : 0.787894\n",
      "total step : 1449 \n",
      "error : 0.726954, accuarcy : 0.788394\n",
      "total step : 1450 \n",
      "error : 0.726645, accuarcy : 0.788394\n",
      "total step : 1451 \n",
      "error : 0.726337, accuarcy : 0.788894\n",
      "total step : 1452 \n",
      "error : 0.726029, accuarcy : 0.788894\n",
      "total step : 1453 \n",
      "error : 0.725722, accuarcy : 0.788894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 1454 \n",
      "error : 0.725414, accuarcy : 0.788894\n",
      "total step : 1455 \n",
      "error : 0.725108, accuarcy : 0.788894\n",
      "total step : 1456 \n",
      "error : 0.724801, accuarcy : 0.788894\n",
      "total step : 1457 \n",
      "error : 0.724495, accuarcy : 0.788894\n",
      "total step : 1458 \n",
      "error : 0.724189, accuarcy : 0.788894\n",
      "total step : 1459 \n",
      "error : 0.723884, accuarcy : 0.788894\n",
      "total step : 1460 \n",
      "error : 0.723579, accuarcy : 0.788894\n",
      "total step : 1461 \n",
      "error : 0.723274, accuarcy : 0.788894\n",
      "total step : 1462 \n",
      "error : 0.722970, accuarcy : 0.788894\n",
      "total step : 1463 \n",
      "error : 0.722666, accuarcy : 0.789395\n",
      "total step : 1464 \n",
      "error : 0.722362, accuarcy : 0.789395\n",
      "total step : 1465 \n",
      "error : 0.722059, accuarcy : 0.789395\n",
      "total step : 1466 \n",
      "error : 0.721756, accuarcy : 0.789395\n",
      "total step : 1467 \n",
      "error : 0.721454, accuarcy : 0.789395\n",
      "total step : 1468 \n",
      "error : 0.721152, accuarcy : 0.789395\n",
      "total step : 1469 \n",
      "error : 0.720850, accuarcy : 0.789395\n",
      "total step : 1470 \n",
      "error : 0.720548, accuarcy : 0.789395\n",
      "total step : 1471 \n",
      "error : 0.720247, accuarcy : 0.789395\n",
      "total step : 1472 \n",
      "error : 0.719946, accuarcy : 0.789395\n",
      "total step : 1473 \n",
      "error : 0.719646, accuarcy : 0.789395\n",
      "total step : 1474 \n",
      "error : 0.719346, accuarcy : 0.789895\n",
      "total step : 1475 \n",
      "error : 0.719046, accuarcy : 0.789895\n",
      "total step : 1476 \n",
      "error : 0.718747, accuarcy : 0.789895\n",
      "total step : 1477 \n",
      "error : 0.718448, accuarcy : 0.790395\n",
      "total step : 1478 \n",
      "error : 0.718149, accuarcy : 0.790395\n",
      "total step : 1479 \n",
      "error : 0.717851, accuarcy : 0.790395\n",
      "total step : 1480 \n",
      "error : 0.717553, accuarcy : 0.790395\n",
      "total step : 1481 \n",
      "error : 0.717255, accuarcy : 0.790395\n",
      "total step : 1482 \n",
      "error : 0.716958, accuarcy : 0.790395\n",
      "total step : 1483 \n",
      "error : 0.716661, accuarcy : 0.790395\n",
      "total step : 1484 \n",
      "error : 0.716364, accuarcy : 0.790395\n",
      "total step : 1485 \n",
      "error : 0.716068, accuarcy : 0.790395\n",
      "total step : 1486 \n",
      "error : 0.715772, accuarcy : 0.790395\n",
      "total step : 1487 \n",
      "error : 0.715476, accuarcy : 0.790395\n",
      "total step : 1488 \n",
      "error : 0.715181, accuarcy : 0.790395\n",
      "total step : 1489 \n",
      "error : 0.714886, accuarcy : 0.790395\n",
      "total step : 1490 \n",
      "error : 0.714592, accuarcy : 0.790395\n",
      "total step : 1491 \n",
      "error : 0.714297, accuarcy : 0.790395\n",
      "total step : 1492 \n",
      "error : 0.714004, accuarcy : 0.790395\n",
      "total step : 1493 \n",
      "error : 0.713710, accuarcy : 0.790895\n",
      "total step : 1494 \n",
      "error : 0.713417, accuarcy : 0.790895\n",
      "total step : 1495 \n",
      "error : 0.713124, accuarcy : 0.790895\n",
      "total step : 1496 \n",
      "error : 0.712831, accuarcy : 0.790895\n",
      "total step : 1497 \n",
      "error : 0.712539, accuarcy : 0.790895\n",
      "total step : 1498 \n",
      "error : 0.712247, accuarcy : 0.790895\n",
      "total step : 1499 \n",
      "error : 0.711956, accuarcy : 0.790395\n",
      "total step : 1500 \n",
      "error : 0.711665, accuarcy : 0.790395\n",
      "total step : 1501 \n",
      "error : 0.711374, accuarcy : 0.790395\n",
      "total step : 1502 \n",
      "error : 0.711083, accuarcy : 0.790395\n",
      "total step : 1503 \n",
      "error : 0.710793, accuarcy : 0.790395\n",
      "total step : 1504 \n",
      "error : 0.710503, accuarcy : 0.790395\n",
      "total step : 1505 \n",
      "error : 0.710214, accuarcy : 0.790395\n",
      "total step : 1506 \n",
      "error : 0.709924, accuarcy : 0.790395\n",
      "total step : 1507 \n",
      "error : 0.709636, accuarcy : 0.790395\n",
      "total step : 1508 \n",
      "error : 0.709347, accuarcy : 0.790395\n",
      "total step : 1509 \n",
      "error : 0.709059, accuarcy : 0.790395\n",
      "total step : 1510 \n",
      "error : 0.708771, accuarcy : 0.790395\n",
      "total step : 1511 \n",
      "error : 0.708483, accuarcy : 0.790395\n",
      "total step : 1512 \n",
      "error : 0.708196, accuarcy : 0.790395\n",
      "total step : 1513 \n",
      "error : 0.707909, accuarcy : 0.790395\n",
      "total step : 1514 \n",
      "error : 0.707623, accuarcy : 0.790395\n",
      "total step : 1515 \n",
      "error : 0.707337, accuarcy : 0.790395\n",
      "total step : 1516 \n",
      "error : 0.707051, accuarcy : 0.790395\n",
      "total step : 1517 \n",
      "error : 0.706765, accuarcy : 0.790395\n",
      "total step : 1518 \n",
      "error : 0.706480, accuarcy : 0.790395\n",
      "total step : 1519 \n",
      "error : 0.706195, accuarcy : 0.790395\n",
      "total step : 1520 \n",
      "error : 0.705910, accuarcy : 0.790395\n",
      "total step : 1521 \n",
      "error : 0.705626, accuarcy : 0.790895\n",
      "total step : 1522 \n",
      "error : 0.705342, accuarcy : 0.790895\n",
      "total step : 1523 \n",
      "error : 0.705058, accuarcy : 0.790895\n",
      "total step : 1524 \n",
      "error : 0.704775, accuarcy : 0.790895\n",
      "total step : 1525 \n",
      "error : 0.704492, accuarcy : 0.790895\n",
      "total step : 1526 \n",
      "error : 0.704209, accuarcy : 0.791396\n",
      "total step : 1527 \n",
      "error : 0.703927, accuarcy : 0.791396\n",
      "total step : 1528 \n",
      "error : 0.703645, accuarcy : 0.791396\n",
      "total step : 1529 \n",
      "error : 0.703363, accuarcy : 0.791396\n",
      "total step : 1530 \n",
      "error : 0.703082, accuarcy : 0.791896\n",
      "total step : 1531 \n",
      "error : 0.702801, accuarcy : 0.791896\n",
      "total step : 1532 \n",
      "error : 0.702520, accuarcy : 0.791896\n",
      "total step : 1533 \n",
      "error : 0.702239, accuarcy : 0.791896\n",
      "total step : 1534 \n",
      "error : 0.701959, accuarcy : 0.791896\n",
      "total step : 1535 \n",
      "error : 0.701679, accuarcy : 0.791896\n",
      "total step : 1536 \n",
      "error : 0.701400, accuarcy : 0.791896\n",
      "total step : 1537 \n",
      "error : 0.701121, accuarcy : 0.791896\n",
      "total step : 1538 \n",
      "error : 0.700842, accuarcy : 0.791896\n",
      "total step : 1539 \n",
      "error : 0.700563, accuarcy : 0.791896\n",
      "total step : 1540 \n",
      "error : 0.700285, accuarcy : 0.792396\n",
      "total step : 1541 \n",
      "error : 0.700007, accuarcy : 0.792396\n",
      "total step : 1542 \n",
      "error : 0.699730, accuarcy : 0.792396\n",
      "total step : 1543 \n",
      "error : 0.699452, accuarcy : 0.792396\n",
      "total step : 1544 \n",
      "error : 0.699175, accuarcy : 0.792396\n",
      "total step : 1545 \n",
      "error : 0.698899, accuarcy : 0.792396\n",
      "total step : 1546 \n",
      "error : 0.698622, accuarcy : 0.792396\n",
      "total step : 1547 \n",
      "error : 0.698346, accuarcy : 0.792396\n",
      "total step : 1548 \n",
      "error : 0.698070, accuarcy : 0.792396\n",
      "total step : 1549 \n",
      "error : 0.697795, accuarcy : 0.792396\n",
      "total step : 1550 \n",
      "error : 0.697520, accuarcy : 0.792396\n",
      "total step : 1551 \n",
      "error : 0.697245, accuarcy : 0.792396\n",
      "total step : 1552 \n",
      "error : 0.696970, accuarcy : 0.792396\n",
      "total step : 1553 \n",
      "error : 0.696696, accuarcy : 0.792396\n",
      "total step : 1554 \n",
      "error : 0.696422, accuarcy : 0.792396\n",
      "total step : 1555 \n",
      "error : 0.696149, accuarcy : 0.792396\n",
      "total step : 1556 \n",
      "error : 0.695875, accuarcy : 0.792396\n",
      "total step : 1557 \n",
      "error : 0.695602, accuarcy : 0.792396\n",
      "total step : 1558 \n",
      "error : 0.695330, accuarcy : 0.792396\n",
      "total step : 1559 \n",
      "error : 0.695057, accuarcy : 0.792396\n",
      "total step : 1560 \n",
      "error : 0.694785, accuarcy : 0.792396\n",
      "total step : 1561 \n",
      "error : 0.694513, accuarcy : 0.792896\n",
      "total step : 1562 \n",
      "error : 0.694242, accuarcy : 0.792896\n",
      "total step : 1563 \n",
      "error : 0.693971, accuarcy : 0.792896\n",
      "total step : 1564 \n",
      "error : 0.693700, accuarcy : 0.792896\n",
      "total step : 1565 \n",
      "error : 0.693429, accuarcy : 0.792896\n",
      "total step : 1566 \n",
      "error : 0.693159, accuarcy : 0.792896\n",
      "total step : 1567 \n",
      "error : 0.692889, accuarcy : 0.792896\n",
      "total step : 1568 \n",
      "error : 0.692619, accuarcy : 0.793397\n",
      "total step : 1569 \n",
      "error : 0.692350, accuarcy : 0.793397\n",
      "total step : 1570 \n",
      "error : 0.692081, accuarcy : 0.793897\n",
      "total step : 1571 \n",
      "error : 0.691812, accuarcy : 0.793897\n",
      "total step : 1572 \n",
      "error : 0.691544, accuarcy : 0.793897\n",
      "total step : 1573 \n",
      "error : 0.691276, accuarcy : 0.793897\n",
      "total step : 1574 \n",
      "error : 0.691008, accuarcy : 0.793897\n",
      "total step : 1575 \n",
      "error : 0.690740, accuarcy : 0.793897\n",
      "total step : 1576 \n",
      "error : 0.690473, accuarcy : 0.793897\n",
      "total step : 1577 \n",
      "error : 0.690206, accuarcy : 0.793897\n",
      "total step : 1578 \n",
      "error : 0.689939, accuarcy : 0.793897\n",
      "total step : 1579 \n",
      "error : 0.689673, accuarcy : 0.794397\n",
      "total step : 1580 \n",
      "error : 0.689407, accuarcy : 0.794397\n",
      "total step : 1581 \n",
      "error : 0.689141, accuarcy : 0.794397\n",
      "total step : 1582 \n",
      "error : 0.688875, accuarcy : 0.794397\n",
      "total step : 1583 \n",
      "error : 0.688610, accuarcy : 0.794397\n",
      "total step : 1584 \n",
      "error : 0.688345, accuarcy : 0.793897\n",
      "total step : 1585 \n",
      "error : 0.688080, accuarcy : 0.793897\n",
      "total step : 1586 \n",
      "error : 0.687816, accuarcy : 0.793897\n",
      "total step : 1587 \n",
      "error : 0.687552, accuarcy : 0.794397\n",
      "total step : 1588 \n",
      "error : 0.687288, accuarcy : 0.794397\n",
      "total step : 1589 \n",
      "error : 0.687025, accuarcy : 0.794397\n",
      "total step : 1590 \n",
      "error : 0.686762, accuarcy : 0.794397\n",
      "total step : 1591 \n",
      "error : 0.686499, accuarcy : 0.794397\n",
      "total step : 1592 \n",
      "error : 0.686236, accuarcy : 0.794397\n",
      "total step : 1593 \n",
      "error : 0.685974, accuarcy : 0.794397\n",
      "total step : 1594 \n",
      "error : 0.685712, accuarcy : 0.794397\n",
      "total step : 1595 \n",
      "error : 0.685450, accuarcy : 0.794397\n",
      "total step : 1596 \n",
      "error : 0.685188, accuarcy : 0.794397\n",
      "total step : 1597 \n",
      "error : 0.684927, accuarcy : 0.794397\n",
      "total step : 1598 \n",
      "error : 0.684666, accuarcy : 0.794397\n",
      "total step : 1599 \n",
      "error : 0.684406, accuarcy : 0.794397\n",
      "total step : 1600 \n",
      "error : 0.684145, accuarcy : 0.794397\n",
      "total step : 1601 \n",
      "error : 0.683885, accuarcy : 0.794397\n",
      "total step : 1602 \n",
      "error : 0.683626, accuarcy : 0.794397\n",
      "total step : 1603 \n",
      "error : 0.683366, accuarcy : 0.794397\n",
      "total step : 1604 \n",
      "error : 0.683107, accuarcy : 0.794397\n",
      "total step : 1605 \n",
      "error : 0.682848, accuarcy : 0.794397\n",
      "total step : 1606 \n",
      "error : 0.682589, accuarcy : 0.794397\n",
      "total step : 1607 \n",
      "error : 0.682331, accuarcy : 0.794897\n",
      "total step : 1608 \n",
      "error : 0.682073, accuarcy : 0.794897\n",
      "total step : 1609 \n",
      "error : 0.681815, accuarcy : 0.794897\n",
      "total step : 1610 \n",
      "error : 0.681558, accuarcy : 0.794897\n",
      "total step : 1611 \n",
      "error : 0.681301, accuarcy : 0.795398\n",
      "total step : 1612 \n",
      "error : 0.681044, accuarcy : 0.795398\n",
      "total step : 1613 \n",
      "error : 0.680787, accuarcy : 0.795398\n",
      "total step : 1614 \n",
      "error : 0.680531, accuarcy : 0.795898\n",
      "total step : 1615 \n",
      "error : 0.680274, accuarcy : 0.795898\n",
      "total step : 1616 \n",
      "error : 0.680019, accuarcy : 0.795898\n",
      "total step : 1617 \n",
      "error : 0.679763, accuarcy : 0.795898\n",
      "total step : 1618 \n",
      "error : 0.679508, accuarcy : 0.796398\n",
      "total step : 1619 \n",
      "error : 0.679253, accuarcy : 0.796398\n",
      "total step : 1620 \n",
      "error : 0.678998, accuarcy : 0.796398\n",
      "total step : 1621 \n",
      "error : 0.678744, accuarcy : 0.796398\n",
      "total step : 1622 \n",
      "error : 0.678489, accuarcy : 0.796398\n",
      "total step : 1623 \n",
      "error : 0.678236, accuarcy : 0.796398\n",
      "total step : 1624 \n",
      "error : 0.677982, accuarcy : 0.796898\n",
      "total step : 1625 \n",
      "error : 0.677729, accuarcy : 0.796898\n",
      "total step : 1626 \n",
      "error : 0.677476, accuarcy : 0.796898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 1627 \n",
      "error : 0.677223, accuarcy : 0.796898\n",
      "total step : 1628 \n",
      "error : 0.676970, accuarcy : 0.796898\n",
      "total step : 1629 \n",
      "error : 0.676718, accuarcy : 0.797399\n",
      "total step : 1630 \n",
      "error : 0.676466, accuarcy : 0.797399\n",
      "total step : 1631 \n",
      "error : 0.676214, accuarcy : 0.797399\n",
      "total step : 1632 \n",
      "error : 0.675963, accuarcy : 0.797399\n",
      "total step : 1633 \n",
      "error : 0.675712, accuarcy : 0.797399\n",
      "total step : 1634 \n",
      "error : 0.675461, accuarcy : 0.797399\n",
      "total step : 1635 \n",
      "error : 0.675210, accuarcy : 0.797399\n",
      "total step : 1636 \n",
      "error : 0.674960, accuarcy : 0.797399\n",
      "total step : 1637 \n",
      "error : 0.674710, accuarcy : 0.797399\n",
      "total step : 1638 \n",
      "error : 0.674460, accuarcy : 0.797399\n",
      "total step : 1639 \n",
      "error : 0.674210, accuarcy : 0.797399\n",
      "total step : 1640 \n",
      "error : 0.673961, accuarcy : 0.797399\n",
      "total step : 1641 \n",
      "error : 0.673712, accuarcy : 0.797399\n",
      "total step : 1642 \n",
      "error : 0.673463, accuarcy : 0.797399\n",
      "total step : 1643 \n",
      "error : 0.673215, accuarcy : 0.797899\n",
      "total step : 1644 \n",
      "error : 0.672967, accuarcy : 0.797899\n",
      "total step : 1645 \n",
      "error : 0.672719, accuarcy : 0.797899\n",
      "total step : 1646 \n",
      "error : 0.672471, accuarcy : 0.797899\n",
      "total step : 1647 \n",
      "error : 0.672223, accuarcy : 0.797899\n",
      "total step : 1648 \n",
      "error : 0.671976, accuarcy : 0.797899\n",
      "total step : 1649 \n",
      "error : 0.671729, accuarcy : 0.797899\n",
      "total step : 1650 \n",
      "error : 0.671483, accuarcy : 0.797899\n",
      "total step : 1651 \n",
      "error : 0.671236, accuarcy : 0.798399\n",
      "total step : 1652 \n",
      "error : 0.670990, accuarcy : 0.798399\n",
      "total step : 1653 \n",
      "error : 0.670744, accuarcy : 0.798899\n",
      "total step : 1654 \n",
      "error : 0.670499, accuarcy : 0.799400\n",
      "total step : 1655 \n",
      "error : 0.670254, accuarcy : 0.799400\n",
      "total step : 1656 \n",
      "error : 0.670008, accuarcy : 0.799400\n",
      "total step : 1657 \n",
      "error : 0.669764, accuarcy : 0.799400\n",
      "total step : 1658 \n",
      "error : 0.669519, accuarcy : 0.799400\n",
      "total step : 1659 \n",
      "error : 0.669275, accuarcy : 0.799400\n",
      "total step : 1660 \n",
      "error : 0.669031, accuarcy : 0.799400\n",
      "total step : 1661 \n",
      "error : 0.668787, accuarcy : 0.799400\n",
      "total step : 1662 \n",
      "error : 0.668544, accuarcy : 0.799400\n",
      "total step : 1663 \n",
      "error : 0.668300, accuarcy : 0.799400\n",
      "total step : 1664 \n",
      "error : 0.668057, accuarcy : 0.799400\n",
      "total step : 1665 \n",
      "error : 0.667815, accuarcy : 0.799400\n",
      "total step : 1666 \n",
      "error : 0.667572, accuarcy : 0.798899\n",
      "total step : 1667 \n",
      "error : 0.667330, accuarcy : 0.799400\n",
      "total step : 1668 \n",
      "error : 0.667088, accuarcy : 0.799400\n",
      "total step : 1669 \n",
      "error : 0.666846, accuarcy : 0.799400\n",
      "total step : 1670 \n",
      "error : 0.666605, accuarcy : 0.799400\n",
      "total step : 1671 \n",
      "error : 0.666364, accuarcy : 0.799400\n",
      "total step : 1672 \n",
      "error : 0.666123, accuarcy : 0.799400\n",
      "total step : 1673 \n",
      "error : 0.665882, accuarcy : 0.799400\n",
      "total step : 1674 \n",
      "error : 0.665642, accuarcy : 0.799400\n",
      "total step : 1675 \n",
      "error : 0.665401, accuarcy : 0.799400\n",
      "total step : 1676 \n",
      "error : 0.665161, accuarcy : 0.799400\n",
      "total step : 1677 \n",
      "error : 0.664922, accuarcy : 0.799400\n",
      "total step : 1678 \n",
      "error : 0.664682, accuarcy : 0.798899\n",
      "total step : 1679 \n",
      "error : 0.664443, accuarcy : 0.798899\n",
      "total step : 1680 \n",
      "error : 0.664204, accuarcy : 0.798899\n",
      "total step : 1681 \n",
      "error : 0.663966, accuarcy : 0.798899\n",
      "total step : 1682 \n",
      "error : 0.663727, accuarcy : 0.798899\n",
      "total step : 1683 \n",
      "error : 0.663489, accuarcy : 0.798899\n",
      "total step : 1684 \n",
      "error : 0.663251, accuarcy : 0.799400\n",
      "total step : 1685 \n",
      "error : 0.663014, accuarcy : 0.799900\n",
      "total step : 1686 \n",
      "error : 0.662776, accuarcy : 0.800400\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for number 6\n",
      "total step : 1 \n",
      "error : 3.230331, accuarcy : 0.393697\n",
      "total step : 2 \n",
      "error : 3.209706, accuarcy : 0.396198\n",
      "total step : 3 \n",
      "error : 3.189469, accuarcy : 0.398199\n",
      "total step : 4 \n",
      "error : 3.169607, accuarcy : 0.401201\n",
      "total step : 5 \n",
      "error : 3.150108, accuarcy : 0.403202\n",
      "total step : 6 \n",
      "error : 3.130962, accuarcy : 0.405703\n",
      "total step : 7 \n",
      "error : 3.112155, accuarcy : 0.406203\n",
      "total step : 8 \n",
      "error : 3.093678, accuarcy : 0.410205\n",
      "total step : 9 \n",
      "error : 3.075518, accuarcy : 0.412706\n",
      "total step : 10 \n",
      "error : 3.057665, accuarcy : 0.413707\n",
      "total step : 11 \n",
      "error : 3.040109, accuarcy : 0.415208\n",
      "total step : 12 \n",
      "error : 3.022838, accuarcy : 0.416208\n",
      "total step : 13 \n",
      "error : 3.005843, accuarcy : 0.418209\n",
      "total step : 14 \n",
      "error : 2.989114, accuarcy : 0.422211\n",
      "total step : 15 \n",
      "error : 2.972642, accuarcy : 0.423712\n",
      "total step : 16 \n",
      "error : 2.956417, accuarcy : 0.423712\n",
      "total step : 17 \n",
      "error : 2.940431, accuarcy : 0.424712\n",
      "total step : 18 \n",
      "error : 2.924674, accuarcy : 0.426713\n",
      "total step : 19 \n",
      "error : 2.909140, accuarcy : 0.428714\n",
      "total step : 20 \n",
      "error : 2.893820, accuarcy : 0.431716\n",
      "total step : 21 \n",
      "error : 2.878706, accuarcy : 0.435218\n",
      "total step : 22 \n",
      "error : 2.863792, accuarcy : 0.436718\n",
      "total step : 23 \n",
      "error : 2.849069, accuarcy : 0.438719\n",
      "total step : 24 \n",
      "error : 2.834532, accuarcy : 0.440220\n",
      "total step : 25 \n",
      "error : 2.820174, accuarcy : 0.441721\n",
      "total step : 26 \n",
      "error : 2.805988, accuarcy : 0.444722\n",
      "total step : 27 \n",
      "error : 2.791970, accuarcy : 0.447224\n",
      "total step : 28 \n",
      "error : 2.778113, accuarcy : 0.451726\n",
      "total step : 29 \n",
      "error : 2.764411, accuarcy : 0.451726\n",
      "total step : 30 \n",
      "error : 2.750861, accuarcy : 0.453227\n",
      "total step : 31 \n",
      "error : 2.737456, accuarcy : 0.452726\n",
      "total step : 32 \n",
      "error : 2.724193, accuarcy : 0.455728\n",
      "total step : 33 \n",
      "error : 2.711066, accuarcy : 0.456228\n",
      "total step : 34 \n",
      "error : 2.698071, accuarcy : 0.457729\n",
      "total step : 35 \n",
      "error : 2.685205, accuarcy : 0.459730\n",
      "total step : 36 \n",
      "error : 2.672463, accuarcy : 0.460730\n",
      "total step : 37 \n",
      "error : 2.659842, accuarcy : 0.461731\n",
      "total step : 38 \n",
      "error : 2.647338, accuarcy : 0.463232\n",
      "total step : 39 \n",
      "error : 2.634948, accuarcy : 0.464232\n",
      "total step : 40 \n",
      "error : 2.622669, accuarcy : 0.467234\n",
      "total step : 41 \n",
      "error : 2.610498, accuarcy : 0.468734\n",
      "total step : 42 \n",
      "error : 2.598431, accuarcy : 0.469235\n",
      "total step : 43 \n",
      "error : 2.586466, accuarcy : 0.470735\n",
      "total step : 44 \n",
      "error : 2.574601, accuarcy : 0.471736\n",
      "total step : 45 \n",
      "error : 2.562832, accuarcy : 0.473237\n",
      "total step : 46 \n",
      "error : 2.551158, accuarcy : 0.474237\n",
      "total step : 47 \n",
      "error : 2.539576, accuarcy : 0.475238\n",
      "total step : 48 \n",
      "error : 2.528084, accuarcy : 0.477239\n",
      "total step : 49 \n",
      "error : 2.516679, accuarcy : 0.479240\n",
      "total step : 50 \n",
      "error : 2.505361, accuarcy : 0.480240\n",
      "total step : 51 \n",
      "error : 2.494127, accuarcy : 0.481241\n",
      "total step : 52 \n",
      "error : 2.482976, accuarcy : 0.483742\n",
      "total step : 53 \n",
      "error : 2.471905, accuarcy : 0.484242\n",
      "total step : 54 \n",
      "error : 2.460913, accuarcy : 0.485743\n",
      "total step : 55 \n",
      "error : 2.449998, accuarcy : 0.486243\n",
      "total step : 56 \n",
      "error : 2.439159, accuarcy : 0.487244\n",
      "total step : 57 \n",
      "error : 2.428395, accuarcy : 0.488244\n",
      "total step : 58 \n",
      "error : 2.417704, accuarcy : 0.489245\n",
      "total step : 59 \n",
      "error : 2.407085, accuarcy : 0.491246\n",
      "total step : 60 \n",
      "error : 2.396537, accuarcy : 0.492246\n",
      "total step : 61 \n",
      "error : 2.386058, accuarcy : 0.493247\n",
      "total step : 62 \n",
      "error : 2.375647, accuarcy : 0.494247\n",
      "total step : 63 \n",
      "error : 2.365304, accuarcy : 0.496248\n",
      "total step : 64 \n",
      "error : 2.355027, accuarcy : 0.497249\n",
      "total step : 65 \n",
      "error : 2.344815, accuarcy : 0.498749\n",
      "total step : 66 \n",
      "error : 2.334668, accuarcy : 0.499750\n",
      "total step : 67 \n",
      "error : 2.324584, accuarcy : 0.501251\n",
      "total step : 68 \n",
      "error : 2.314563, accuarcy : 0.502251\n",
      "total step : 69 \n",
      "error : 2.304603, accuarcy : 0.504252\n",
      "total step : 70 \n",
      "error : 2.294705, accuarcy : 0.505753\n",
      "total step : 71 \n",
      "error : 2.284866, accuarcy : 0.507754\n",
      "total step : 72 \n",
      "error : 2.275087, accuarcy : 0.508754\n",
      "total step : 73 \n",
      "error : 2.265367, accuarcy : 0.511256\n",
      "total step : 74 \n",
      "error : 2.255705, accuarcy : 0.513257\n",
      "total step : 75 \n",
      "error : 2.246101, accuarcy : 0.514757\n",
      "total step : 76 \n",
      "error : 2.236553, accuarcy : 0.515758\n",
      "total step : 77 \n",
      "error : 2.227062, accuarcy : 0.516758\n",
      "total step : 78 \n",
      "error : 2.217626, accuarcy : 0.517259\n",
      "total step : 79 \n",
      "error : 2.208246, accuarcy : 0.520760\n",
      "total step : 80 \n",
      "error : 2.198920, accuarcy : 0.522761\n",
      "total step : 81 \n",
      "error : 2.189648, accuarcy : 0.524762\n",
      "total step : 82 \n",
      "error : 2.180430, accuarcy : 0.526263\n",
      "total step : 83 \n",
      "error : 2.171265, accuarcy : 0.527264\n",
      "total step : 84 \n",
      "error : 2.162153, accuarcy : 0.528764\n",
      "total step : 85 \n",
      "error : 2.153093, accuarcy : 0.530765\n",
      "total step : 86 \n",
      "error : 2.144085, accuarcy : 0.531766\n",
      "total step : 87 \n",
      "error : 2.135128, accuarcy : 0.532266\n",
      "total step : 88 \n",
      "error : 2.126222, accuarcy : 0.533767\n",
      "total step : 89 \n",
      "error : 2.117367, accuarcy : 0.536268\n",
      "total step : 90 \n",
      "error : 2.108562, accuarcy : 0.537769\n",
      "total step : 91 \n",
      "error : 2.099807, accuarcy : 0.539270\n",
      "total step : 92 \n",
      "error : 2.091102, accuarcy : 0.539770\n",
      "total step : 93 \n",
      "error : 2.082446, accuarcy : 0.539770\n",
      "total step : 94 \n",
      "error : 2.073839, accuarcy : 0.539770\n",
      "total step : 95 \n",
      "error : 2.065281, accuarcy : 0.540770\n",
      "total step : 96 \n",
      "error : 2.056771, accuarcy : 0.540770\n",
      "total step : 97 \n",
      "error : 2.048308, accuarcy : 0.542271\n",
      "total step : 98 \n",
      "error : 2.039894, accuarcy : 0.543272\n",
      "total step : 99 \n",
      "error : 2.031527, accuarcy : 0.544272\n",
      "total step : 100 \n",
      "error : 2.023207, accuarcy : 0.544272\n",
      "total step : 101 \n",
      "error : 2.014935, accuarcy : 0.545273\n",
      "total step : 102 \n",
      "error : 2.006708, accuarcy : 0.547274\n",
      "total step : 103 \n",
      "error : 1.998529, accuarcy : 0.547274\n",
      "total step : 104 \n",
      "error : 1.990395, accuarcy : 0.549275\n",
      "total step : 105 \n",
      "error : 1.982307, accuarcy : 0.549775\n",
      "total step : 106 \n",
      "error : 1.974265, accuarcy : 0.551776\n",
      "total step : 107 \n",
      "error : 1.966268, accuarcy : 0.553277\n",
      "total step : 108 \n",
      "error : 1.958317, accuarcy : 0.553777\n",
      "total step : 109 \n",
      "error : 1.950411, accuarcy : 0.553777\n",
      "total step : 110 \n",
      "error : 1.942549, accuarcy : 0.554277\n",
      "total step : 111 \n",
      "error : 1.934732, accuarcy : 0.555778\n",
      "total step : 112 \n",
      "error : 1.926959, accuarcy : 0.556778\n",
      "total step : 113 \n",
      "error : 1.919230, accuarcy : 0.557779\n",
      "total step : 114 \n",
      "error : 1.911545, accuarcy : 0.558279\n",
      "total step : 115 \n",
      "error : 1.903904, accuarcy : 0.559780\n",
      "total step : 116 \n",
      "error : 1.896307, accuarcy : 0.561281\n",
      "total step : 117 \n",
      "error : 1.888753, accuarcy : 0.562781\n",
      "total step : 118 \n",
      "error : 1.881241, accuarcy : 0.564282\n",
      "total step : 119 \n",
      "error : 1.873773, accuarcy : 0.564782\n",
      "total step : 120 \n",
      "error : 1.866348, accuarcy : 0.564782\n",
      "total step : 121 \n",
      "error : 1.858965, accuarcy : 0.564282\n",
      "total step : 122 \n",
      "error : 1.851624, accuarcy : 0.565783\n",
      "total step : 123 \n",
      "error : 1.844326, accuarcy : 0.566283\n",
      "total step : 124 \n",
      "error : 1.837069, accuarcy : 0.567784\n",
      "total step : 125 \n",
      "error : 1.829854, accuarcy : 0.568784\n",
      "total step : 126 \n",
      "error : 1.822681, accuarcy : 0.569785\n",
      "total step : 127 \n",
      "error : 1.815549, accuarcy : 0.571286\n",
      "total step : 128 \n",
      "error : 1.808459, accuarcy : 0.572286\n",
      "total step : 129 \n",
      "error : 1.801409, accuarcy : 0.572786\n",
      "total step : 130 \n",
      "error : 1.794400, accuarcy : 0.574287\n",
      "total step : 131 \n",
      "error : 1.787432, accuarcy : 0.574287\n",
      "total step : 132 \n",
      "error : 1.780504, accuarcy : 0.575288\n",
      "total step : 133 \n",
      "error : 1.773617, accuarcy : 0.575788\n",
      "total step : 134 \n",
      "error : 1.766770, accuarcy : 0.576788\n",
      "total step : 135 \n",
      "error : 1.759962, accuarcy : 0.579290\n",
      "total step : 136 \n",
      "error : 1.753195, accuarcy : 0.579790\n",
      "total step : 137 \n",
      "error : 1.746467, accuarcy : 0.581791\n",
      "total step : 138 \n",
      "error : 1.739778, accuarcy : 0.581791\n",
      "total step : 139 \n",
      "error : 1.733128, accuarcy : 0.582791\n",
      "total step : 140 \n",
      "error : 1.726518, accuarcy : 0.584292\n",
      "total step : 141 \n",
      "error : 1.719946, accuarcy : 0.585293\n",
      "total step : 142 \n",
      "error : 1.713413, accuarcy : 0.586293\n",
      "total step : 143 \n",
      "error : 1.706919, accuarcy : 0.587794\n",
      "total step : 144 \n",
      "error : 1.700462, accuarcy : 0.588294\n",
      "total step : 145 \n",
      "error : 1.694044, accuarcy : 0.589295\n",
      "total step : 146 \n",
      "error : 1.687664, accuarcy : 0.589295\n",
      "total step : 147 \n",
      "error : 1.681322, accuarcy : 0.590295\n",
      "total step : 148 \n",
      "error : 1.675017, accuarcy : 0.591796\n",
      "total step : 149 \n",
      "error : 1.668749, accuarcy : 0.592796\n",
      "total step : 150 \n",
      "error : 1.662519, accuarcy : 0.594297\n",
      "total step : 151 \n",
      "error : 1.656326, accuarcy : 0.594297\n",
      "total step : 152 \n",
      "error : 1.650169, accuarcy : 0.595298\n",
      "total step : 153 \n",
      "error : 1.644050, accuarcy : 0.595798\n",
      "total step : 154 \n",
      "error : 1.637966, accuarcy : 0.596798\n",
      "total step : 155 \n",
      "error : 1.631920, accuarcy : 0.597799\n",
      "total step : 156 \n",
      "error : 1.625909, accuarcy : 0.599800\n",
      "total step : 157 \n",
      "error : 1.619934, accuarcy : 0.601301\n",
      "total step : 158 \n",
      "error : 1.613995, accuarcy : 0.601801\n",
      "total step : 159 \n",
      "error : 1.608091, accuarcy : 0.602301\n",
      "total step : 160 \n",
      "error : 1.602223, accuarcy : 0.602801\n",
      "total step : 161 \n",
      "error : 1.596390, accuarcy : 0.603802\n",
      "total step : 162 \n",
      "error : 1.590592, accuarcy : 0.604802\n",
      "total step : 163 \n",
      "error : 1.584829, accuarcy : 0.605303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 164 \n",
      "error : 1.579100, accuarcy : 0.606303\n",
      "total step : 165 \n",
      "error : 1.573406, accuarcy : 0.607304\n",
      "total step : 166 \n",
      "error : 1.567746, accuarcy : 0.607304\n",
      "total step : 167 \n",
      "error : 1.562120, accuarcy : 0.607804\n",
      "total step : 168 \n",
      "error : 1.556528, accuarcy : 0.608804\n",
      "total step : 169 \n",
      "error : 1.550970, accuarcy : 0.609805\n",
      "total step : 170 \n",
      "error : 1.545446, accuarcy : 0.610805\n",
      "total step : 171 \n",
      "error : 1.539954, accuarcy : 0.612306\n",
      "total step : 172 \n",
      "error : 1.534496, accuarcy : 0.613807\n",
      "total step : 173 \n",
      "error : 1.529071, accuarcy : 0.613807\n",
      "total step : 174 \n",
      "error : 1.523678, accuarcy : 0.613307\n",
      "total step : 175 \n",
      "error : 1.518318, accuarcy : 0.614807\n",
      "total step : 176 \n",
      "error : 1.512990, accuarcy : 0.615308\n",
      "total step : 177 \n",
      "error : 1.507695, accuarcy : 0.615808\n",
      "total step : 178 \n",
      "error : 1.502431, accuarcy : 0.616808\n",
      "total step : 179 \n",
      "error : 1.497199, accuarcy : 0.619310\n",
      "total step : 180 \n",
      "error : 1.491999, accuarcy : 0.619810\n",
      "total step : 181 \n",
      "error : 1.486830, accuarcy : 0.619810\n",
      "total step : 182 \n",
      "error : 1.481693, accuarcy : 0.621811\n",
      "total step : 183 \n",
      "error : 1.476586, accuarcy : 0.622811\n",
      "total step : 184 \n",
      "error : 1.471511, accuarcy : 0.622811\n",
      "total step : 185 \n",
      "error : 1.466466, accuarcy : 0.624312\n",
      "total step : 186 \n",
      "error : 1.461451, accuarcy : 0.625813\n",
      "total step : 187 \n",
      "error : 1.456466, accuarcy : 0.626313\n",
      "total step : 188 \n",
      "error : 1.451512, accuarcy : 0.626813\n",
      "total step : 189 \n",
      "error : 1.446588, accuarcy : 0.627814\n",
      "total step : 190 \n",
      "error : 1.441693, accuarcy : 0.628814\n",
      "total step : 191 \n",
      "error : 1.436827, accuarcy : 0.629315\n",
      "total step : 192 \n",
      "error : 1.431991, accuarcy : 0.631316\n",
      "total step : 193 \n",
      "error : 1.427184, accuarcy : 0.631316\n",
      "total step : 194 \n",
      "error : 1.422406, accuarcy : 0.632816\n",
      "total step : 195 \n",
      "error : 1.417657, accuarcy : 0.634317\n",
      "total step : 196 \n",
      "error : 1.412936, accuarcy : 0.636318\n",
      "total step : 197 \n",
      "error : 1.408243, accuarcy : 0.637319\n",
      "total step : 198 \n",
      "error : 1.403579, accuarcy : 0.638319\n",
      "total step : 199 \n",
      "error : 1.398943, accuarcy : 0.638319\n",
      "total step : 200 \n",
      "error : 1.394334, accuarcy : 0.639820\n",
      "total step : 201 \n",
      "error : 1.389753, accuarcy : 0.639820\n",
      "total step : 202 \n",
      "error : 1.385199, accuarcy : 0.640320\n",
      "total step : 203 \n",
      "error : 1.380673, accuarcy : 0.640820\n",
      "total step : 204 \n",
      "error : 1.376173, accuarcy : 0.642321\n",
      "total step : 205 \n",
      "error : 1.371700, accuarcy : 0.642321\n",
      "total step : 206 \n",
      "error : 1.367254, accuarcy : 0.643822\n",
      "total step : 207 \n",
      "error : 1.362835, accuarcy : 0.645323\n",
      "total step : 208 \n",
      "error : 1.358441, accuarcy : 0.645823\n",
      "total step : 209 \n",
      "error : 1.354074, accuarcy : 0.646823\n",
      "total step : 210 \n",
      "error : 1.349732, accuarcy : 0.648324\n",
      "total step : 211 \n",
      "error : 1.345417, accuarcy : 0.648824\n",
      "total step : 212 \n",
      "error : 1.341127, accuarcy : 0.650325\n",
      "total step : 213 \n",
      "error : 1.336862, accuarcy : 0.652326\n",
      "total step : 214 \n",
      "error : 1.332622, accuarcy : 0.653827\n",
      "total step : 215 \n",
      "error : 1.328407, accuarcy : 0.653827\n",
      "total step : 216 \n",
      "error : 1.324218, accuarcy : 0.654827\n",
      "total step : 217 \n",
      "error : 1.320052, accuarcy : 0.656328\n",
      "total step : 218 \n",
      "error : 1.315912, accuarcy : 0.657829\n",
      "total step : 219 \n",
      "error : 1.311795, accuarcy : 0.658329\n",
      "total step : 220 \n",
      "error : 1.307703, accuarcy : 0.658329\n",
      "total step : 221 \n",
      "error : 1.303634, accuarcy : 0.659830\n",
      "total step : 222 \n",
      "error : 1.299590, accuarcy : 0.660330\n",
      "total step : 223 \n",
      "error : 1.295569, accuarcy : 0.661331\n",
      "total step : 224 \n",
      "error : 1.291571, accuarcy : 0.661831\n",
      "total step : 225 \n",
      "error : 1.287596, accuarcy : 0.662831\n",
      "total step : 226 \n",
      "error : 1.283645, accuarcy : 0.664832\n",
      "total step : 227 \n",
      "error : 1.279717, accuarcy : 0.665333\n",
      "total step : 228 \n",
      "error : 1.275811, accuarcy : 0.665333\n",
      "total step : 229 \n",
      "error : 1.271928, accuarcy : 0.665833\n",
      "total step : 230 \n",
      "error : 1.268067, accuarcy : 0.666833\n",
      "total step : 231 \n",
      "error : 1.264228, accuarcy : 0.668334\n",
      "total step : 232 \n",
      "error : 1.260412, accuarcy : 0.668834\n",
      "total step : 233 \n",
      "error : 1.256617, accuarcy : 0.670835\n",
      "total step : 234 \n",
      "error : 1.252844, accuarcy : 0.671836\n",
      "total step : 235 \n",
      "error : 1.249093, accuarcy : 0.672336\n",
      "total step : 236 \n",
      "error : 1.245363, accuarcy : 0.672836\n",
      "total step : 237 \n",
      "error : 1.241654, accuarcy : 0.674337\n",
      "total step : 238 \n",
      "error : 1.237967, accuarcy : 0.674837\n",
      "total step : 239 \n",
      "error : 1.234300, accuarcy : 0.677339\n",
      "total step : 240 \n",
      "error : 1.230655, accuarcy : 0.678839\n",
      "total step : 241 \n",
      "error : 1.227029, accuarcy : 0.679840\n",
      "total step : 242 \n",
      "error : 1.223425, accuarcy : 0.679840\n",
      "total step : 243 \n",
      "error : 1.219840, accuarcy : 0.682341\n",
      "total step : 244 \n",
      "error : 1.216276, accuarcy : 0.682341\n",
      "total step : 245 \n",
      "error : 1.212732, accuarcy : 0.685343\n",
      "total step : 246 \n",
      "error : 1.209207, accuarcy : 0.685343\n",
      "total step : 247 \n",
      "error : 1.205703, accuarcy : 0.685343\n",
      "total step : 248 \n",
      "error : 1.202218, accuarcy : 0.685843\n",
      "total step : 249 \n",
      "error : 1.198752, accuarcy : 0.686343\n",
      "total step : 250 \n",
      "error : 1.195306, accuarcy : 0.686843\n",
      "total step : 251 \n",
      "error : 1.191878, accuarcy : 0.687844\n",
      "total step : 252 \n",
      "error : 1.188470, accuarcy : 0.689345\n",
      "total step : 253 \n",
      "error : 1.185080, accuarcy : 0.689845\n",
      "total step : 254 \n",
      "error : 1.181710, accuarcy : 0.690845\n",
      "total step : 255 \n",
      "error : 1.178357, accuarcy : 0.691846\n",
      "total step : 256 \n",
      "error : 1.175024, accuarcy : 0.692346\n",
      "total step : 257 \n",
      "error : 1.171708, accuarcy : 0.692846\n",
      "total step : 258 \n",
      "error : 1.168411, accuarcy : 0.693347\n",
      "total step : 259 \n",
      "error : 1.165131, accuarcy : 0.694847\n",
      "total step : 260 \n",
      "error : 1.161870, accuarcy : 0.695348\n",
      "total step : 261 \n",
      "error : 1.158626, accuarcy : 0.696348\n",
      "total step : 262 \n",
      "error : 1.155400, accuarcy : 0.696848\n",
      "total step : 263 \n",
      "error : 1.152191, accuarcy : 0.697849\n",
      "total step : 264 \n",
      "error : 1.149000, accuarcy : 0.697849\n",
      "total step : 265 \n",
      "error : 1.145825, accuarcy : 0.698349\n",
      "total step : 266 \n",
      "error : 1.142668, accuarcy : 0.698349\n",
      "total step : 267 \n",
      "error : 1.139528, accuarcy : 0.700350\n",
      "total step : 268 \n",
      "error : 1.136405, accuarcy : 0.701351\n",
      "total step : 269 \n",
      "error : 1.133298, accuarcy : 0.702351\n",
      "total step : 270 \n",
      "error : 1.130208, accuarcy : 0.702851\n",
      "total step : 271 \n",
      "error : 1.127134, accuarcy : 0.703352\n",
      "total step : 272 \n",
      "error : 1.124077, accuarcy : 0.703352\n",
      "total step : 273 \n",
      "error : 1.121036, accuarcy : 0.703352\n",
      "total step : 274 \n",
      "error : 1.118011, accuarcy : 0.704352\n",
      "total step : 275 \n",
      "error : 1.115002, accuarcy : 0.704352\n",
      "total step : 276 \n",
      "error : 1.112009, accuarcy : 0.705353\n",
      "total step : 277 \n",
      "error : 1.109031, accuarcy : 0.705853\n",
      "total step : 278 \n",
      "error : 1.106069, accuarcy : 0.706353\n",
      "total step : 279 \n",
      "error : 1.103123, accuarcy : 0.706853\n",
      "total step : 280 \n",
      "error : 1.100192, accuarcy : 0.706853\n",
      "total step : 281 \n",
      "error : 1.097276, accuarcy : 0.707354\n",
      "total step : 282 \n",
      "error : 1.094376, accuarcy : 0.707354\n",
      "total step : 283 \n",
      "error : 1.091490, accuarcy : 0.707854\n",
      "total step : 284 \n",
      "error : 1.088620, accuarcy : 0.707854\n",
      "total step : 285 \n",
      "error : 1.085764, accuarcy : 0.709355\n",
      "total step : 286 \n",
      "error : 1.082923, accuarcy : 0.709855\n",
      "total step : 287 \n",
      "error : 1.080096, accuarcy : 0.709855\n",
      "total step : 288 \n",
      "error : 1.077285, accuarcy : 0.709855\n",
      "total step : 289 \n",
      "error : 1.074487, accuarcy : 0.709855\n",
      "total step : 290 \n",
      "error : 1.071704, accuarcy : 0.710355\n",
      "total step : 291 \n",
      "error : 1.068935, accuarcy : 0.711356\n",
      "total step : 292 \n",
      "error : 1.066180, accuarcy : 0.712856\n",
      "total step : 293 \n",
      "error : 1.063440, accuarcy : 0.713357\n",
      "total step : 294 \n",
      "error : 1.060713, accuarcy : 0.713357\n",
      "total step : 295 \n",
      "error : 1.058000, accuarcy : 0.713857\n",
      "total step : 296 \n",
      "error : 1.055300, accuarcy : 0.713857\n",
      "total step : 297 \n",
      "error : 1.052615, accuarcy : 0.714357\n",
      "total step : 298 \n",
      "error : 1.049943, accuarcy : 0.714357\n",
      "total step : 299 \n",
      "error : 1.047284, accuarcy : 0.714357\n",
      "total step : 300 \n",
      "error : 1.044638, accuarcy : 0.714357\n",
      "total step : 301 \n",
      "error : 1.042006, accuarcy : 0.714357\n",
      "total step : 302 \n",
      "error : 1.039387, accuarcy : 0.714357\n",
      "total step : 303 \n",
      "error : 1.036781, accuarcy : 0.714357\n",
      "total step : 304 \n",
      "error : 1.034188, accuarcy : 0.714857\n",
      "total step : 305 \n",
      "error : 1.031608, accuarcy : 0.715858\n",
      "total step : 306 \n",
      "error : 1.029041, accuarcy : 0.716358\n",
      "total step : 307 \n",
      "error : 1.026486, accuarcy : 0.715858\n",
      "total step : 308 \n",
      "error : 1.023944, accuarcy : 0.715858\n",
      "total step : 309 \n",
      "error : 1.021415, accuarcy : 0.717359\n",
      "total step : 310 \n",
      "error : 1.018898, accuarcy : 0.717859\n",
      "total step : 311 \n",
      "error : 1.016393, accuarcy : 0.719360\n",
      "total step : 312 \n",
      "error : 1.013901, accuarcy : 0.719360\n",
      "total step : 313 \n",
      "error : 1.011421, accuarcy : 0.719860\n",
      "total step : 314 \n",
      "error : 1.008953, accuarcy : 0.719860\n",
      "total step : 315 \n",
      "error : 1.006497, accuarcy : 0.720860\n",
      "total step : 316 \n",
      "error : 1.004053, accuarcy : 0.722361\n",
      "total step : 317 \n",
      "error : 1.001620, accuarcy : 0.722861\n",
      "total step : 318 \n",
      "error : 0.999200, accuarcy : 0.723862\n",
      "total step : 319 \n",
      "error : 0.996792, accuarcy : 0.724862\n",
      "total step : 320 \n",
      "error : 0.994395, accuarcy : 0.725863\n",
      "total step : 321 \n",
      "error : 0.992009, accuarcy : 0.726363\n",
      "total step : 322 \n",
      "error : 0.989635, accuarcy : 0.726863\n",
      "total step : 323 \n",
      "error : 0.987273, accuarcy : 0.726863\n",
      "total step : 324 \n",
      "error : 0.984922, accuarcy : 0.727364\n",
      "total step : 325 \n",
      "error : 0.982582, accuarcy : 0.727364\n",
      "total step : 326 \n",
      "error : 0.980254, accuarcy : 0.727864\n",
      "total step : 327 \n",
      "error : 0.977936, accuarcy : 0.727864\n",
      "total step : 328 \n",
      "error : 0.975630, accuarcy : 0.727864\n",
      "total step : 329 \n",
      "error : 0.973334, accuarcy : 0.727864\n",
      "total step : 330 \n",
      "error : 0.971050, accuarcy : 0.728864\n",
      "total step : 331 \n",
      "error : 0.968776, accuarcy : 0.730365\n",
      "total step : 332 \n",
      "error : 0.966513, accuarcy : 0.730365\n",
      "total step : 333 \n",
      "error : 0.964261, accuarcy : 0.731866\n",
      "total step : 334 \n",
      "error : 0.962020, accuarcy : 0.731866\n",
      "total step : 335 \n",
      "error : 0.959789, accuarcy : 0.732866\n",
      "total step : 336 \n",
      "error : 0.957569, accuarcy : 0.734367\n",
      "total step : 337 \n",
      "error : 0.955359, accuarcy : 0.734367\n",
      "total step : 338 \n",
      "error : 0.953159, accuarcy : 0.734867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 339 \n",
      "error : 0.950970, accuarcy : 0.734867\n",
      "total step : 340 \n",
      "error : 0.948791, accuarcy : 0.734867\n",
      "total step : 341 \n",
      "error : 0.946622, accuarcy : 0.734867\n",
      "total step : 342 \n",
      "error : 0.944464, accuarcy : 0.735368\n",
      "total step : 343 \n",
      "error : 0.942315, accuarcy : 0.735868\n",
      "total step : 344 \n",
      "error : 0.940177, accuarcy : 0.735868\n",
      "total step : 345 \n",
      "error : 0.938048, accuarcy : 0.735868\n",
      "total step : 346 \n",
      "error : 0.935929, accuarcy : 0.735868\n",
      "total step : 347 \n",
      "error : 0.933821, accuarcy : 0.736868\n",
      "total step : 348 \n",
      "error : 0.931722, accuarcy : 0.736868\n",
      "total step : 349 \n",
      "error : 0.929632, accuarcy : 0.737369\n",
      "total step : 350 \n",
      "error : 0.927553, accuarcy : 0.737869\n",
      "total step : 351 \n",
      "error : 0.925482, accuarcy : 0.738869\n",
      "total step : 352 \n",
      "error : 0.923422, accuarcy : 0.738869\n",
      "total step : 353 \n",
      "error : 0.921371, accuarcy : 0.739370\n",
      "total step : 354 \n",
      "error : 0.919329, accuarcy : 0.739370\n",
      "total step : 355 \n",
      "error : 0.917297, accuarcy : 0.739870\n",
      "total step : 356 \n",
      "error : 0.915274, accuarcy : 0.740370\n",
      "total step : 357 \n",
      "error : 0.913260, accuarcy : 0.740370\n",
      "total step : 358 \n",
      "error : 0.911256, accuarcy : 0.740370\n",
      "total step : 359 \n",
      "error : 0.909261, accuarcy : 0.740370\n",
      "total step : 360 \n",
      "error : 0.907274, accuarcy : 0.741871\n",
      "total step : 361 \n",
      "error : 0.905297, accuarcy : 0.741871\n",
      "total step : 362 \n",
      "error : 0.903329, accuarcy : 0.742371\n",
      "total step : 363 \n",
      "error : 0.901370, accuarcy : 0.742871\n",
      "total step : 364 \n",
      "error : 0.899419, accuarcy : 0.742871\n",
      "total step : 365 \n",
      "error : 0.897478, accuarcy : 0.743372\n",
      "total step : 366 \n",
      "error : 0.895545, accuarcy : 0.743372\n",
      "total step : 367 \n",
      "error : 0.893621, accuarcy : 0.743872\n",
      "total step : 368 \n",
      "error : 0.891706, accuarcy : 0.743872\n",
      "total step : 369 \n",
      "error : 0.889799, accuarcy : 0.744872\n",
      "total step : 370 \n",
      "error : 0.887901, accuarcy : 0.745873\n",
      "total step : 371 \n",
      "error : 0.886011, accuarcy : 0.746373\n",
      "total step : 372 \n",
      "error : 0.884130, accuarcy : 0.747874\n",
      "total step : 373 \n",
      "error : 0.882258, accuarcy : 0.748874\n",
      "total step : 374 \n",
      "error : 0.880393, accuarcy : 0.749875\n",
      "total step : 375 \n",
      "error : 0.878538, accuarcy : 0.749875\n",
      "total step : 376 \n",
      "error : 0.876690, accuarcy : 0.749875\n",
      "total step : 377 \n",
      "error : 0.874851, accuarcy : 0.750375\n",
      "total step : 378 \n",
      "error : 0.873019, accuarcy : 0.751376\n",
      "total step : 379 \n",
      "error : 0.871196, accuarcy : 0.751876\n",
      "total step : 380 \n",
      "error : 0.869381, accuarcy : 0.752876\n",
      "total step : 381 \n",
      "error : 0.867575, accuarcy : 0.754377\n",
      "total step : 382 \n",
      "error : 0.865776, accuarcy : 0.754877\n",
      "total step : 383 \n",
      "error : 0.863985, accuarcy : 0.755378\n",
      "total step : 384 \n",
      "error : 0.862202, accuarcy : 0.755878\n",
      "total step : 385 \n",
      "error : 0.860427, accuarcy : 0.756378\n",
      "total step : 386 \n",
      "error : 0.858659, accuarcy : 0.756878\n",
      "total step : 387 \n",
      "error : 0.856900, accuarcy : 0.756878\n",
      "total step : 388 \n",
      "error : 0.855148, accuarcy : 0.757379\n",
      "total step : 389 \n",
      "error : 0.853404, accuarcy : 0.757879\n",
      "total step : 390 \n",
      "error : 0.851668, accuarcy : 0.758379\n",
      "total step : 391 \n",
      "error : 0.849939, accuarcy : 0.758379\n",
      "total step : 392 \n",
      "error : 0.848218, accuarcy : 0.758379\n",
      "total step : 393 \n",
      "error : 0.846504, accuarcy : 0.759380\n",
      "total step : 394 \n",
      "error : 0.844798, accuarcy : 0.759380\n",
      "total step : 395 \n",
      "error : 0.843099, accuarcy : 0.759380\n",
      "total step : 396 \n",
      "error : 0.841408, accuarcy : 0.759380\n",
      "total step : 397 \n",
      "error : 0.839724, accuarcy : 0.759880\n",
      "total step : 398 \n",
      "error : 0.838048, accuarcy : 0.759880\n",
      "total step : 399 \n",
      "error : 0.836378, accuarcy : 0.759880\n",
      "total step : 400 \n",
      "error : 0.834716, accuarcy : 0.760380\n",
      "total step : 401 \n",
      "error : 0.833061, accuarcy : 0.760880\n",
      "total step : 402 \n",
      "error : 0.831414, accuarcy : 0.761381\n",
      "total step : 403 \n",
      "error : 0.829773, accuarcy : 0.761881\n",
      "total step : 404 \n",
      "error : 0.828140, accuarcy : 0.761881\n",
      "total step : 405 \n",
      "error : 0.826513, accuarcy : 0.761881\n",
      "total step : 406 \n",
      "error : 0.824894, accuarcy : 0.761881\n",
      "total step : 407 \n",
      "error : 0.823281, accuarcy : 0.763382\n",
      "total step : 408 \n",
      "error : 0.821675, accuarcy : 0.763882\n",
      "total step : 409 \n",
      "error : 0.820077, accuarcy : 0.764882\n",
      "total step : 410 \n",
      "error : 0.818485, accuarcy : 0.764882\n",
      "total step : 411 \n",
      "error : 0.816900, accuarcy : 0.765383\n",
      "total step : 412 \n",
      "error : 0.815322, accuarcy : 0.765883\n",
      "total step : 413 \n",
      "error : 0.813750, accuarcy : 0.765883\n",
      "total step : 414 \n",
      "error : 0.812185, accuarcy : 0.766883\n",
      "total step : 415 \n",
      "error : 0.810627, accuarcy : 0.767384\n",
      "total step : 416 \n",
      "error : 0.809076, accuarcy : 0.767884\n",
      "total step : 417 \n",
      "error : 0.807531, accuarcy : 0.767884\n",
      "total step : 418 \n",
      "error : 0.805992, accuarcy : 0.767884\n",
      "total step : 419 \n",
      "error : 0.804461, accuarcy : 0.767884\n",
      "total step : 420 \n",
      "error : 0.802935, accuarcy : 0.767884\n",
      "total step : 421 \n",
      "error : 0.801416, accuarcy : 0.767384\n",
      "total step : 422 \n",
      "error : 0.799904, accuarcy : 0.767884\n",
      "total step : 423 \n",
      "error : 0.798398, accuarcy : 0.768384\n",
      "total step : 424 \n",
      "error : 0.796898, accuarcy : 0.768384\n",
      "total step : 425 \n",
      "error : 0.795404, accuarcy : 0.768384\n",
      "total step : 426 \n",
      "error : 0.793917, accuarcy : 0.768384\n",
      "total step : 427 \n",
      "error : 0.792436, accuarcy : 0.768384\n",
      "total step : 428 \n",
      "error : 0.790962, accuarcy : 0.770385\n",
      "total step : 429 \n",
      "error : 0.789493, accuarcy : 0.770385\n",
      "total step : 430 \n",
      "error : 0.788031, accuarcy : 0.770385\n",
      "total step : 431 \n",
      "error : 0.786574, accuarcy : 0.770385\n",
      "total step : 432 \n",
      "error : 0.785124, accuarcy : 0.770385\n",
      "total step : 433 \n",
      "error : 0.783680, accuarcy : 0.770385\n",
      "total step : 434 \n",
      "error : 0.782242, accuarcy : 0.770385\n",
      "total step : 435 \n",
      "error : 0.780809, accuarcy : 0.770385\n",
      "total step : 436 \n",
      "error : 0.779383, accuarcy : 0.770385\n",
      "total step : 437 \n",
      "error : 0.777963, accuarcy : 0.770385\n",
      "total step : 438 \n",
      "error : 0.776548, accuarcy : 0.770385\n",
      "total step : 439 \n",
      "error : 0.775140, accuarcy : 0.770385\n",
      "total step : 440 \n",
      "error : 0.773737, accuarcy : 0.771886\n",
      "total step : 441 \n",
      "error : 0.772340, accuarcy : 0.771886\n",
      "total step : 442 \n",
      "error : 0.770949, accuarcy : 0.771886\n",
      "total step : 443 \n",
      "error : 0.769563, accuarcy : 0.771886\n",
      "total step : 444 \n",
      "error : 0.768183, accuarcy : 0.773387\n",
      "total step : 445 \n",
      "error : 0.766809, accuarcy : 0.774887\n",
      "total step : 446 \n",
      "error : 0.765441, accuarcy : 0.774887\n",
      "total step : 447 \n",
      "error : 0.764078, accuarcy : 0.774887\n",
      "total step : 448 \n",
      "error : 0.762720, accuarcy : 0.775388\n",
      "total step : 449 \n",
      "error : 0.761369, accuarcy : 0.776388\n",
      "total step : 450 \n",
      "error : 0.760022, accuarcy : 0.776888\n",
      "total step : 451 \n",
      "error : 0.758681, accuarcy : 0.777389\n",
      "total step : 452 \n",
      "error : 0.757346, accuarcy : 0.777389\n",
      "total step : 453 \n",
      "error : 0.756016, accuarcy : 0.777889\n",
      "total step : 454 \n",
      "error : 0.754692, accuarcy : 0.777889\n",
      "total step : 455 \n",
      "error : 0.753372, accuarcy : 0.777889\n",
      "total step : 456 \n",
      "error : 0.752059, accuarcy : 0.778889\n",
      "total step : 457 \n",
      "error : 0.750750, accuarcy : 0.778889\n",
      "total step : 458 \n",
      "error : 0.749447, accuarcy : 0.778889\n",
      "total step : 459 \n",
      "error : 0.748149, accuarcy : 0.779390\n",
      "total step : 460 \n",
      "error : 0.746856, accuarcy : 0.779890\n",
      "total step : 461 \n",
      "error : 0.745569, accuarcy : 0.780390\n",
      "total step : 462 \n",
      "error : 0.744286, accuarcy : 0.780390\n",
      "total step : 463 \n",
      "error : 0.743009, accuarcy : 0.780890\n",
      "total step : 464 \n",
      "error : 0.741737, accuarcy : 0.781391\n",
      "total step : 465 \n",
      "error : 0.740470, accuarcy : 0.781391\n",
      "total step : 466 \n",
      "error : 0.739208, accuarcy : 0.781391\n",
      "total step : 467 \n",
      "error : 0.737951, accuarcy : 0.781391\n",
      "total step : 468 \n",
      "error : 0.736700, accuarcy : 0.781391\n",
      "total step : 469 \n",
      "error : 0.735453, accuarcy : 0.781391\n",
      "total step : 470 \n",
      "error : 0.734211, accuarcy : 0.781891\n",
      "total step : 471 \n",
      "error : 0.732974, accuarcy : 0.781891\n",
      "total step : 472 \n",
      "error : 0.731742, accuarcy : 0.782391\n",
      "total step : 473 \n",
      "error : 0.730515, accuarcy : 0.782891\n",
      "total step : 474 \n",
      "error : 0.729292, accuarcy : 0.784392\n",
      "total step : 475 \n",
      "error : 0.728075, accuarcy : 0.784392\n",
      "total step : 476 \n",
      "error : 0.726862, accuarcy : 0.784392\n",
      "total step : 477 \n",
      "error : 0.725654, accuarcy : 0.784392\n",
      "total step : 478 \n",
      "error : 0.724451, accuarcy : 0.784392\n",
      "total step : 479 \n",
      "error : 0.723253, accuarcy : 0.784892\n",
      "total step : 480 \n",
      "error : 0.722059, accuarcy : 0.785393\n",
      "total step : 481 \n",
      "error : 0.720870, accuarcy : 0.785893\n",
      "total step : 482 \n",
      "error : 0.719686, accuarcy : 0.785893\n",
      "total step : 483 \n",
      "error : 0.718506, accuarcy : 0.785893\n",
      "total step : 484 \n",
      "error : 0.717331, accuarcy : 0.786893\n",
      "total step : 485 \n",
      "error : 0.716161, accuarcy : 0.786893\n",
      "total step : 486 \n",
      "error : 0.714995, accuarcy : 0.787394\n",
      "total step : 487 \n",
      "error : 0.713833, accuarcy : 0.788394\n",
      "total step : 488 \n",
      "error : 0.712676, accuarcy : 0.788894\n",
      "total step : 489 \n",
      "error : 0.711524, accuarcy : 0.788894\n",
      "total step : 490 \n",
      "error : 0.710376, accuarcy : 0.788894\n",
      "total step : 491 \n",
      "error : 0.709233, accuarcy : 0.788894\n",
      "total step : 492 \n",
      "error : 0.708094, accuarcy : 0.788894\n",
      "total step : 493 \n",
      "error : 0.706959, accuarcy : 0.789395\n",
      "total step : 494 \n",
      "error : 0.705829, accuarcy : 0.789395\n",
      "total step : 495 \n",
      "error : 0.704703, accuarcy : 0.789895\n",
      "total step : 496 \n",
      "error : 0.703581, accuarcy : 0.790395\n",
      "total step : 497 \n",
      "error : 0.702464, accuarcy : 0.790395\n",
      "total step : 498 \n",
      "error : 0.701351, accuarcy : 0.790395\n",
      "total step : 499 \n",
      "error : 0.700243, accuarcy : 0.790395\n",
      "total step : 500 \n",
      "error : 0.699138, accuarcy : 0.790395\n",
      "total step : 501 \n",
      "error : 0.698038, accuarcy : 0.790395\n",
      "total step : 502 \n",
      "error : 0.696942, accuarcy : 0.791396\n",
      "total step : 503 \n",
      "error : 0.695850, accuarcy : 0.791896\n",
      "total step : 504 \n",
      "error : 0.694763, accuarcy : 0.792896\n",
      "total step : 505 \n",
      "error : 0.693679, accuarcy : 0.792896\n",
      "total step : 506 \n",
      "error : 0.692600, accuarcy : 0.792896\n",
      "total step : 507 \n",
      "error : 0.691525, accuarcy : 0.792896\n",
      "total step : 508 \n",
      "error : 0.690453, accuarcy : 0.792896\n",
      "total step : 509 \n",
      "error : 0.689386, accuarcy : 0.793397\n",
      "total step : 510 \n",
      "error : 0.688323, accuarcy : 0.793397\n",
      "total step : 511 \n",
      "error : 0.687264, accuarcy : 0.793897\n",
      "total step : 512 \n",
      "error : 0.686209, accuarcy : 0.793897\n",
      "total step : 513 \n",
      "error : 0.685158, accuarcy : 0.794397\n",
      "total step : 514 \n",
      "error : 0.684111, accuarcy : 0.794897\n",
      "total step : 515 \n",
      "error : 0.683068, accuarcy : 0.795398\n",
      "total step : 516 \n",
      "error : 0.682028, accuarcy : 0.795398\n",
      "total step : 517 \n",
      "error : 0.680993, accuarcy : 0.795398\n",
      "total step : 518 \n",
      "error : 0.679961, accuarcy : 0.795398\n",
      "total step : 519 \n",
      "error : 0.678934, accuarcy : 0.795398\n",
      "total step : 520 \n",
      "error : 0.677910, accuarcy : 0.795398\n",
      "total step : 521 \n",
      "error : 0.676890, accuarcy : 0.796398\n",
      "total step : 522 \n",
      "error : 0.675873, accuarcy : 0.796398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 523 \n",
      "error : 0.674861, accuarcy : 0.796398\n",
      "total step : 524 \n",
      "error : 0.673852, accuarcy : 0.796398\n",
      "total step : 525 \n",
      "error : 0.672847, accuarcy : 0.796398\n",
      "total step : 526 \n",
      "error : 0.671846, accuarcy : 0.796398\n",
      "total step : 527 \n",
      "error : 0.670849, accuarcy : 0.796898\n",
      "total step : 528 \n",
      "error : 0.669855, accuarcy : 0.797399\n",
      "total step : 529 \n",
      "error : 0.668864, accuarcy : 0.798399\n",
      "total step : 530 \n",
      "error : 0.667878, accuarcy : 0.798899\n",
      "total step : 531 \n",
      "error : 0.666895, accuarcy : 0.798899\n",
      "total step : 532 \n",
      "error : 0.665916, accuarcy : 0.798899\n",
      "total step : 533 \n",
      "error : 0.664940, accuarcy : 0.799400\n",
      "total step : 534 \n",
      "error : 0.663968, accuarcy : 0.799400\n",
      "total step : 535 \n",
      "error : 0.662999, accuarcy : 0.799900\n",
      "total step : 536 \n",
      "error : 0.662034, accuarcy : 0.799900\n",
      "total step : 537 \n",
      "error : 0.661072, accuarcy : 0.800900\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAEWCAYAAABPDqCoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABOJ0lEQVR4nO3dd3zV1f3H8dcni5DBCCTMsPceYSgO3Dipe2vVlp/bVmurHVrtclTrFq1abEWpiigq7irgYoPsvcLem5Dx+f1xLzakARJM8r335v18PO4j9/v9nnPv50By8sm553uOuTsiIiIiIlI2cUEHICIiIiISTZRAi4iIiIiUgxJoEREREZFyUAItIiIiIlIOSqBFRERERMpBCbSIiIiISDkogZZqzcyGmdkfD3F9p5m1qsqYRESkYpnZQDPLPcT1oWb2u6qMSaKbEmiJCGa2zMxODjqOktw9zd2XHKrM4TpmEZFIZ2ZfmNkWM6sRdCxBcPfr3f0PhysXqb+rpOopgRYJmJklBB2DiFRfZtYCOBZw4Jwqfu9q0/9Vp7ZWB0qgJaKZWQ0ze8zMVocfj+0fITGz+mb2npltNbPNZjbezOLC135lZqvMbIeZzTezkw7xNnXN7P1w2Qlm1rrY+7uZtQk/P8PM5oTLrTKzX5hZKvAB0Dg83WOnmTU+TNwDzSw3HONa4B9mNsvMzi72volmttHMelT4P6qIyIGuAr4FhgFXF79gZtlm9paZbTCzTWb2VLFrPzWzueE+cY6Z9Qqf/77fDB9/P1XuIP1f3XBfviE8Cv6emTUtVj/DzP4R7ku3mNnb4fPl7jfN7A4zW29ma8zsmoPEWOrvFjP7F9AMeDfc1/8yXP4cM5sdLv+FmXUs9rrLwm39DthlZnea2cgSMT1pZo8d5v9IIowSaIl0vwH6Az2A7kBf4Lfha3cAuUAm0AD4NeBm1h64Gejj7unAacCyQ7zHpcB9QF1gEfCng5R7Efi/8Gt2Af7j7ruA04HV4ekeae6++jBxAzQEMoDmwBDgn8AVxa6fAaxx9+mHiFtEpCJcBQwPP04zswYAZhYPvAcsB1oATYAR4WsXAr8P161FaOR6Uxnfr2T/Fwf8I3zcDNgDPFWs/L+AFKAzkAX8LXy+vP1mQ6B2uB3XAU+bWd1SypX6u8XdrwRWAGeH+/qHzKwd8Brws3D5MYQS7KRir3cpcCZQB3gFGGRmdeD7UemLw22UKKIEWiLd5cD97r7e3TcQSnSvDF/LBxoBzd09393Hu7sDhUANoJOZJbr7MndffIj3eMvdJ7p7AaFfID0OUi4//Jq13H2Lu089wrgBioB73T3P3fcQ6lTPMLNa4etXog5VRCqZmR1DKHF93d2nAIuBy8KX+wKNgTvdfZe773X3L8PXfgI85O6TPGSRuy8v49se0P+5+yZ3H+nuu919B6FBjOPD8TUiNEhxfbjfzXf3seHXKW+/mU+oX8539zHATqD9QcqV9rulNBcD77v7J+6eD/wVqAkcXazME+6+MtzWNcA44MLwtUHAxvC/vUQRJdAS6RoTGv3Yb3n4HMDDhEaMPzazJWZ2F4C7LyI0GvB7YL2ZjTCzxhzc2mLPdwNpByl3PqERjuVmNtbMjjrCuAE2uPve/QfhUeuvgPPDIxOnE0rmRUQq09XAx+6+MXz8Kv+dxpENLA8PLpSUTSjZPhIH9H9mlmJmz5nZcjPbTijBrBMeAc8GNrv7lpIvcgT95qYSbTlYf1/q75aDOKCvd/ciYCWhUe79Vpao8zL/HTm/Ag2WRCUl0BLpVhMaHdmvWfgc7r7D3e9w91bA2cDt++c6u/ur7r5/ZMWBB39oIOGRlsGEPkJ8G3h9/6XyxH2IOvs71QuBb9x91Q+NWUTkYMysJnARcLyZrQ3PSf450N3MuhNK/JpZ6Te/rQRal3IeQolpSrHjhiWul+z/7iA0EtzP3WsBx+0PMfw+GfunPJSiwvvNQ/1uKSX2A/p6MzNCSX/xOErWeRvoZmZdgLPQYElUUgItkSTRzJKLPRIIzS37rZllmll94B5CH9thZmeZWZtwh7Wd0NSNQjNrb2Ynhm/a20toPl3hDwnMzJLM7HIzqx3+mG7/+wGsA+qZWe1iVQ4a9yG8DfQCbiM0t09EpDL9iFA/1onQ1LUeQEdgPKG5zROBNcADZpYa7pcHhOu+APzCzHpbSBsz259ITgcuM7N4MxtEeDrGIaQT6qe3mlkGcO/+C+EpDx8Az4RvNkw0s+OK1X2bCu43D/a7JXx5HVB8b4DXgTPN7CQzSyT0x0Ae8PXBXj88+v4modH+ie6+oiLilqqlBFoiyRhCnej+x++BPwKTge+AmcDU8DmAtsCnhOaxfQM84+5fEJr//ACwkdD0jCxCN4H8UFcCy8IfMV5P+CM4d59HKGFeEr4Lu/Fh4i5VeC70SKAl8FYFxCsicihXA/9w9xXuvnb/g9ANfJcTGgE+G2hD6Oa5XEJzfnH3NwjNVX4V2EEokc0Iv+5t4Xpbw6/z9mHieIzQvOGNhFYD+bDE9SsJzUueB6wnNEWPcByV0W8e7HcLwF8IDY5sNbNfuPt8Qr8LngzHfzahmwz3HeY9Xga6oukbUcsOPi9eRKqamd0DtHP3Kw5bWEREorLfNLNmhP4gaOju24OOR8pPi3qLRIjwR5fXceBqHSIichDR2G9aaL+C24ERSp6jl6ZwiEQAM/spoZtlPnD3cUHHIyIS6aKx37TQ5lvbgVMoNtdboo+mcIiIiIiIlINGoEVEREREyiHq5kDXr1/fW7RoEXQYIiJHZMqUKRvdPTPoOKqK+mwRiWYH67OjLoFu0aIFkydPDjoMEZEjYmZl3e44JqjPFpFodrA+W1M4RERERETKQQm0iIiIiEg5KIEWERERESkHJdAiIiIiIuWgBFpEREREpByUQIuIiIiIlIMSaBGRasbMBpnZfDNbZGZ3lXK9tpm9a2YzzGy2mV1T1roiItVBtUigR89YzasTVgQdhohI4MwsHngaOB3oBFxqZp1KFLsJmOPu3YGBwCNmllTGuiIiEcPdGTkll49mr63Q1426jVSOxAcz1zB5+RYu7pNNfJwFHY6ISJD6AovcfQmAmY0ABgNzipVxIN3MDEgDNgMFQL8y1BURCdTM3G18MGsNyzftZtKyzazfkcfx7TI5tVMDQt3aD1ctEugzuzXig1lrmbh0M0e1rhd0OCIiQWoCrCx2nEsoMS7uKWA0sBpIBy529yIzK0tdzGwIMASgWbNmFRe5iEgppq3YwrrtewGYs3o7z45dTGGRUzclid7N63JU63pc2rdZhSXPUE0S6BM7ZFEzMZ73Z65WAi0i1V1pv0G8xPFpwHTgRKA18ImZjS9jXdz9eeB5gJycnP+5LiJypDbtzOOj2ev49+SV7MorIK+gkJWb9xxQ5vQuDXngvG7UTkmstDiqRQKdkpTAiR2z+GDmWn5/dmcS4qvF1G8RkdLkAtnFjpsSGmku7hrgAXd3YJGZLQU6lLGuiEiFcHeGT1jB9JVbKSxyZuRuZcmGXQB0blyL9g3SATirW2PO7NqI+Dijds1EGtepWemxVYsEGuDsbo14/7s1TFi6mQFt6gcdjohIUCYBbc2sJbAKuAS4rESZFcBJwHgzawC0B5YAW8tQV0TkiBQUFvGfeetZvyMPd2f0jNVMWraF2jUTSauRQLOMFM7q2oij29SnX8uMCp2SUV7VJoEe2D6L1KR43vtutRJoEam23L3AzG4GPgLigZfcfbaZXR++PhT4AzDMzGYSmrbxK3ffCFBa3SDaISLRb9H6nTw/bjHb9uSzK6+QaSu2sGtf4ffXkxLiuO+czlzZvzlxEbYIRLVJoJMT4zm5UwM+nLWW+wd3IVHTOESkmnL3McCYEueGFnu+Gji1rHVFRMpqy659zFy1jbem5vLZ3PUUuZOdkQLA6V0bcXLHBvRqXgeA1KQEUmtEZqoamVFVkjO7NuKd6av5evEmjm+XGXQ4IiIiIjFtZ14BIyauYFdeIWu372XUtFz25hcRH2ec0rEBvzmz4/cJdDSpVgn08e0zqZWcwKipuUqgRURERCrY3vzQFIy8giKmLt/C38cv4evFmwBIjDd+1KMJZ3RtRNsGaTStG32J837VKoGukRDP2d0bM3JqLjv25pOeXHnLm4iIiIhUBxt25PH65JWMnb+Bics2H3AtNSmeP53bhUv7hNaEj7S5zEeqWiXQAOf1asrwCSv4YNZaLsrJPnwFEREREfnett35TF2xhW+XbGLsgg0s3biLvIIimmWkcMuJbUhJCqWX7Rum0bt5BrVrxt6AZbVLoHs1q0PL+qmMnJKrBFpERESkjDbtzOOed2YzZtYaPLxFUpcmtbikTzZXH92CVplpwQZYhapdAm1mnNezCY98soCVm3dH5cR1ERERkaoyM3cbv3hjBiu37KagyPnpsa04qlU9WtZPpXm9lEDXYw5KtUugAc7tFUqgR01bxa0ntQ06HBEREZGIM33lVmas3Mqwr5exK6+AH/VswmV9m9GlSe2gQwtctUygm9ZNoX+rDEZOzeWWE9tUy7+cRERERAoKi7h39GzWbc8rccUZt2Aj+wqLqJkYzz+u6UP/VvUCiTESVVoCbWbJwDigRvh93nT3e0uUMeBx4AxgN/Bjd59aWTEVd1FONre/PoNvFm/iaO1MKCIiIjFs9dY9vP/dGvIKCjEztu7ex7y1O9ixt4DpK7fSrkEaCXEHbjJ3dJt63HdOZzLTa3x/Y6CEVOa/Rh5worvvNLNE4Esz+8Ddvy1W5nSgbfjRD3g2/LXSndG1Efe9O4fhE1cogRYREZGY4+4sWr+T4RNWMHzCcvIL/ftrZtClcW0S4o1L+2bz53O76hP5cqi0BNrdHdgZPkwMP7xEscHAP8NlvzWzOmbWyN3XVFZc+yUnxnN+r6b869tlbNiRR2Z6jcp+SxEREZEqMXv1Nm7/9wzmr9sBwAntM7l/cBca1k5my6595Bc5TerUDDjK6FWp4/FmFg9MAdoAT7v7hBJFmgArix3nhs8dkECb2RBgCECzZs0qLL7L+jXjpa+W8saUldw4sE2Fva6IiIhIVdu0M49/T17J+u2hjU0S4+O487T2XNInm3pp/x0ozKqVHGCUsaFSE2h3LwR6mFkdYJSZdXH3WcWKlPZZQclRatz9eeB5gJycnP+5fqTaZKXRr2UGIyau5PrjWsfM7jgiIiIS29yd8Qs3snnXPuav28GarXv4cPZa9uYXUSs5gWYZKbz44z4aZa4kVTIj3N23mtkXwCCgeAKdCxTfzaQpsLoqYtrvsn7NuG3EdL5ctJHj2mVW5VuLiIiIlNmefYV8l7uVl79ZxsSlW9i4M7RyRnycUS81ibO6Neb641vRJis94EhjX2WuwpEJ5IeT55rAycCDJYqNBm42sxGEbh7cVhXzn4sb1KUhGalJDJ+wXAm0iIiIRJzNu/Yx7KulvPzNcrbtySc9OYGB7bPo0DCdQV0aUi81iTopSUGHWa1U5gh0I+Dl8DzoOOB1d3/PzK4HcPehwBhCS9gtIrSM3TWVGE+paiTEc2Hvprzw5VLWbNtDo9r6qENEYpuZDSK0hGg88IK7P1Di+p3A5eHDBKAjkOnum81sGbADKAQK3D2nygIXqYa+WbyJa4dNYk9+Iad2asCZ3RpxYocs0pMTgw6tWqvMVTi+A3qWcn5osecO3FRZMZTVFf2b8/fxS3j56+XcdXqHoMMREak04UGNp4FTCE2jm2Rmo919zv4y7v4w8HC4/NnAz919c7GXOcHdN1Zh2CLV0oez1nD3WzNpVCeZ56/srakZEUSrYgPZGSmc1rkhr01cwa0ntdFi4SISy/oCi9x9CUB4Ct1gYM5Byl8KvFZFsYlUe3vzC/nzmLnMWrWNqSu20qFhOkOv6E2L+qlBhybFxB2+SPVw3TEt2bYnn5FTcoMORUSkMh1s+dD/YWYphG7+HlnstAMfm9mU8BKjpdUbYmaTzWzyhg0bKihskdhXUFjEdS9P4l/fLifOjBsHtmb0zccoeY5AGmoN6928Lt2b1ualr5Zxeb/mWtJORGJVmZYPDTsb+KrE9I0B7r7azLKAT8xsnruPO+DFKmnpUZFYtnX3Pq5/ZQrfLtnMg+d35eI+FbfvhVQ8JdBhZsa1x7TkthHT+Xz+ek7q2CDokEREKkN5lg+9hBLTN9x9dfjrejMbRWhKyLhS6orIYazcvJtxCzcwedkWPp27jryCIv4wuLOS5yigBLqYM7o24oEP5vHil0uVQItIrJoEtDWzlsAqQknyZSULmVlt4HjgimLnUoE4d98Rfn4qcH+VRC0SIzbsyGPd9r3cO3o201ZsocghPTmBPi0y+L/jWtGvVb2gQ5QyUAJdTGJ8HFcd1YIHP5zHrFXb6NKkdtAhiYhUKHcvMLObgY8ILWP3krvPLrHEKMC5wMfuvqtY9QaEdpWF0O+PV939w6qLXiQ6zVq1jY/nrGPCkk1MWPrfGVFDjmvFuT2b0L5BuqaORhkl0CVc1q8Zz3y+iGfHLubpy3oFHY6ISIVz9zGE1uEvfm5oieNhwLAS55YA3Ss5PJGYMWHJJj6YtZbhE5aTX+i0ykzl5ye3o1GdZHpk16FdAy1LF62UQJdQu2YiVxzVnKFjF7Nkw05aZaYFHZKIiIhEsMIiZ+LSzcxatY09+YWMnrGatdv2sjOvAIDzejbh56e0IzsjJeBIpaIogS7FtQNa8tKXS3lu7BIevKBb0OGIiIhIhCoqcm4cPoWPZq/7/ly3prW5MKcpGSlJXN6/ORmp2mY71iiBLkVmeg0u6ZPNqxNXcNvJbWlcR9t7i4iIyIHGL9zAz0ZMZ9OufVx1VHNuPrENaTUStCFbNaCNVA7ip8e1wh3+Pn5J0KGIiIhIhCgqcnbmFfDO9FVc+eJEduQV8PAF3bh/cBey0pOVPFcT+l8+iKZ1UxjcowmvTVzBzSe0oV5ajaBDEhERkQDs2VfIm1Nz+WbxRiYv28L6HXkAtG+QzrBr+9Cotj6prm6UQB/CDQNbM2paLs+PW8LdZ3QMOhwRERGpYrlbdnPlixNZunEX9VKTyGlRl+7ZdUirkcDgHk2oXTMx6BAlAEqgD6FNVhqDezTh5W+Wcd2xLclKTw46JBEREakiE5du5rYR09iVV8C/ruvLsW0zgw5JIoTmQB/GrSe1Jb/QefaLxUGHIiIiIlWgsMj5fN56rnhxAgAjhhyl5FkOoBHow2hZP5XzejZh+IQVDDmuleY5iYiIxLC3p63i3tGz2bYnn8a1k3nn5mPITNd9UHIgjUCXwa0ntaWoyHnmc41Ci4iIxJo9+woZNS2Xe96Zxc/+PZ3aNRN54LyuvHuLkmcpnUagyyA7I4WL+mQzYtIK/u/4VjStq52EREREoo27s35HHhOXbmbxhp3hc/DR7LXMW7sDgGPb1uepy3rp5kA5JCXQZXTzCW14c3IuT/1nEQ+cr90JRUREosmKTbu59uVJLFq/83+uNahVg79e2J3GtZPJaZFBUoI+oJdDUwJdRo3r1OSyfs3417fL+cmxLWmTlR50SCIiInIIu/IKmLB0E3vzi3jis4Ws27aXX5zajj4tMujVvC4JcfZ9WTM7xCuJHEgJdDncfGIb3pySy4MfzufvV+UEHY6IiIiUYs++QqYs38Iv35zB6m17AUivkcCzV/TmmLb1A45OYoES6HKon1aD649vxV8/XsCkZZvp0yIj6JBERESE0NJzH85ay7Cvl/Jd7jbyCopIio/j8Ut60Dozjeb1UkhP1rxmqRhKoMvpumNa8a9vl/PnMXN564aj9ZGPiEQdMxsEPA7EAy+4+wMlrt8JXB4+TAA6ApnuvvlwdUWq2rrte5m0bDNDxy5m1qrttKiXwkU52eS0qMvRretrFQ2pFEqgy6lmUjy3n9KOX42cyYez1nJ610ZBhyQiUmZmFg88DZwC5AKTzGy0u8/ZX8bdHwYeDpc/G/h5OHk+bF2RqrJl1z7ueus7Ppq9DoD05ATuO6czV/RvTnycBrekcimBPgLn92rKi18u5cEP53FypwYkxutuXRGJGn2BRe6+BMDMRgCDgYMlwZcCrx1hXZFKsWNvPhc//w3LNu3m1hPb0KFRLY5vl0lqDaU1UjWU+R2BhPg47jq9A8s27Wb4t8uDDkdEpDyaACuLHeeGz/0PM0sBBgEjy1PXzIaY2WQzm7xhw4YKCVoEYPe+Al78cilnPfklC9bt5IWrcrj91Pac0bWRkmepUkqgj9AJ7bMY0KYef/t0IVt27Qs6HBGRsirts20/SNmzga/cfXN56rr78+6e4+45mZmZRximyIFmrdrG2U9+yR/em0NefhE3n9CG49rp+0uCoQT6CJkZ95zVmZ15BTz6yYKgwxERKatcILvYcVNg9UHKXsJ/p2+Ut65Ihflw1loGP/0VO/MKeOnHOXxz94n84rT2QYcl1ZgS6B+gfcN0rujXjOETljN3zfagwxERKYtJQFsza2lmSYSS5NElC5lZbeB44J3y1hWpKLlbdnPtsElc/8oU2jdI571bjuXEDg20ApYErtISaDPLNrPPzWyumc02s9tKKTPQzLaZ2fTw457Kiqey/PyUdtSumch9787G/WCfgoqIRAZ3LwBuBj4C5gKvu/tsM7vezK4vVvRc4GN333W4ulUXvVQnE5du5sKh3zBp6WZuP6Udrw3pryXpJGJU5oz7AuAOd59qZunAFDP7pJTljsa7+1mVGEelqpOSxO2ntud3b8/SsnYiEhXcfQwwpsS5oSWOhwHDylJXpKLNWLmVK16YQL20JF6//ig6NqoVdEgiB6i0BNrd1wBrws93mNlcQndrx9xyR5f1bcbwb5fzx/fnckKHLJIT44MOSUREJKqs37GXT+asY2buNkZOzSUrPZnRNw+gXppGnSXyVMkcaDNrAfQEJpRy+Sgzm2FmH5hZ54PUj+glkeLjjHvP7syqrXt4buySoMMRERGJOg9+MJ/fjJrFW9NWcXGfbEbdeLSSZ4lYlb5oopmlEVpD9GfuXvJOu6lAc3ffaWZnAG8DbUu+hrs/DzwPkJOTE5ETjY9qXY+zujXi6S8WMbhHY1rUTw06JBERkagw7KuljJyay3k9m/DHc7uQkqQ1nSWyVeoItJklEkqeh7v7WyWvu/t2d98Zfj4GSDSz+pUZU2W656xO1IiP43fvzNINhSIiImUwckouv393DhmpSfz8lHZKniUqVOYqHAa8CMx190cPUqZhuBxm1jccz6bKiqmyZdVK5s5B7Rm/cCPvfrcm6HBEREQi2tgFG7jjjRk0r5fCi1fnkJ2REnRIImVSmX/mDQCuBGaa2fTwuV8DzeD7O74vAG4wswJgD3CJR/nQ7eX9mvPmlFz+8N4cjm+XSe2aiUGHJCIiEnEKi5w/vT+HVvVTGXPbsboBX6JKZa7C8SWlb/tavMxTwFOVFUMQ4uOMP5/blXOe+pK/fjSfP/yoS9AhiYiIRJR9BUXc/OpUFqzbyYPnd1XyLFFHOxFWgi5NanP10S14ZcJypq/cGnQ4IiIiEWNm7jYGPvw5H89Zx+2ntOOinOzDVxKJMEqgK8ntp7QjK70Gv35rJgWFRUGHIyIiErhtu/O5YfgUzIyXr+3LLSe20bbcEpWUQFeS9ORE7j27M3PWbOfFL5cGHY6IiEig9uYXcsnfv2XV1j385byuHN8uU8mzRC2tFVOJTu/SkNM6N+CRTxZwcqcGtM5MCzokERGRKrV7XwGPfryA/8xfz9KNu3j28l4c1y4z6LBEfhCNQFciM+MPg7tQMzGeX735HUVFUb3AiIiISLk9+vECXvhyKbVrJjL0it4M6tIo6JBEfjAl0JUsq1Yy95zVicnLt/DPb5YFHY6IiEiV2LOvkF++OYMXvlzKxTnZjLpxAKd1bhh0WCIVQgl0FTivVxMGts/kwQ/ns3Lz7qDDERERqVSTlm3m5EfH8vrkXIYc10pLukrMUQJdBcxCa0PHxxm/GvmdtvkWEZGY9fTni7hw6Des276XF67K4ddndCQpQemGxBZ9R1eRxnVq8uszOvL14k28NnFl0OGISDVmZoPMbL6ZLTKzuw5SZqCZTTez2WY2ttj5ZWY2M3xtctVFLdHg83nrefSTBQzq3JDxvzqBkzs1CDokkUqhVTiq0KV9s3nvu9X86f05HNu2PtkZKUGHJCLVjJnFA08DpwC5wCQzG+3uc4qVqQM8Awxy9xVmllXiZU5w941VFbNEh9+Pns2wr5fRol4KD13YjVrJiUGHJFJpNAJdhcyMhy7oRpwZd7w+g0KtyiEiVa8vsMjdl7j7PmAEMLhEmcuAt9x9BYC7r6/iGCXKTFuxhWFfL+OUTg0YecPRSp4l5imBrmJN66bw+3M6M3HZZv4+fknQ4YhI9dMEKD6PLDd8rrh2QF0z+8LMppjZVcWuOfBx+PyQ0t7AzIaY2WQzm7xhw4YKDV4iz9KNu/jFGzOon1aDxy7uQb20GkGHJFLplEAH4LxeTTi9S0Me+Xg+c1ZvDzocEaleStv6reTHYQlAb+BM4DTgd2bWLnxtgLv3Ak4HbjKz4/7nxdyfd/ccd8/JzNSGGbFsw448rh02ibXb9vK3i7uTWkMzQ6V6UAIdADPjT+d2pU5KEre/Pp29+YVBhyQi1UcukF3suCmwupQyH7r7rvBc53FAdwB3Xx3+uh4YRWhKiFRDi9bv4KRHvmD11j3845q+HNtWfyxJ9aEEOiAZqUk8dEE35q3dwaOfLAg6HBGpPiYBbc2spZklAZcAo0uUeQc41swSzCwF6AfMNbNUM0sHMLNU4FRgVhXGLhFi+sqtXPHCRBLi4xh14wD6tswIOiSRKqXPWgJ0QvssLu/XjL+PX8IJ7bM4qnW9oEMSkRjn7gVmdjPwERAPvOTus83s+vD1oe4+18w+BL4DioAX3H2WmbUCRpkZhH5/vOruHwbTEgnC+u17+XD2Wh7+aD479hbwjx/3oVPjWkGHJVLlLNo29cjJyfHJk2Nn6dHd+wo484kv2bOvkA9uO5a6qUlBhyQilcjMprh7TtBxVJVY67Ors3emr+Lut2aye18hXZrU4tnLe2s5Vol5B+uzNYUjYClJCTx5aU8279rHnW/O0C6FIlImZnaWmakPlyrx/LjF3DZiOmk1Enjuyt68e/MxSp6lWlPnGwG6NKnN3Wd04NO56xn29bKgwxGR6HAJsNDMHjKzjkEHI7Fr2cZdPPThfAZ1bsjXd53IaZ0bEp7GI1JtKYGOED8+ugUnd8ziL2PmMWvVtqDDEZEI5+5XAD2BxcA/zOyb8PrL6QGHJjFkw448bhw+lZqJ8dw3uDMJ8UobREAJdMQwMx6+oDsZqUnc8to0duYVBB2SiEQ4d98OjCS0m2Aj4FxgqpndEmhgEhN27M3n9MfHM2fNdv54bhca1EoOOiSRiKEEOoLUTU3i8Ut6sHzTLu55WytDicjBmdnZZjYK+A+QCPR199MJrdf8i0CDk6i3auseBj/9FRt35vGX87oyuEfJzSpFqjctYxdh+rWqx60nteWxTxdydJv6XNC7adAhiUhkuhD4m7uPK37S3Xeb2bUBxSQx4OWvl/HEZwvZV1jEi1fncFLHBkGHJBJxNAIdgW45sS39W2Xw27dnMneNtvoWkVLdC0zcf2BmNc2sBYC7fxZUUBLdRk7J5d7Rs2nXIJ03rj9KybPIQSiBjkDxccYTl/akVnIiN7wyhW178oMOSUQizxuENjnZrzB8TqTc3J3bX5/OHW/MoEd2Hf51XV86NNQGKSIHowQ6QmWlJ/P05b3I3bKHX7wxg6IirQ8tIgdIcPd9+w/Cz7UTk5Tb+h17ueGVqbw1dRUX9G7KC1fnaLUNkcPQT0gE69Mig7vP6Mgnc9YxdNzioMMRkciywczO2X9gZoOBjQHGI1Hq7pEz+XD2Wga0qccD53WlflqNoEMSiXi6iTDCXTugBdNWbOGvH82ne9M6DGhTP+iQRCQyXA8MN7OnAANWAlcFG5JEk937Chj29TI+m7eeW09sw+2ntg86JJGooQQ6wpkZD57fjflrd3Dra9N479ZjaFS7ZtBhiUjA3H0x0N/M0gBz9x1BxyTRYcuufTzwwTz+PXklACd1yOL/jm8dcFQi0aVMUzjMLNXM4sLP25nZOWaWWLmhyX6pNRJ49ore7M0v5PpXprI3vzDokEQkApjZmcCNwM/N7B4zuyfomCSyuTu/HPkd/568koQ4468XdufFH/chtYbG00TKo6xzoMcByWbWBPgMuAYYdqgKZpZtZp+b2Vwzm21mt5VSxszsCTNbZGbfmVmv8jagumiTlcYjF/Vgxsqt/PqtmbjrpkKR6szMhgIXA7cQmsJxIdA80KAkork7f3hvLp/MWcdvzujIgj+err0GRI5QWRNoc/fdwHnAk+5+LtDpMHUKgDvcvSPQH7jJzErWOR1oG34MAZ4tc+TV0KAuDfn5ye14a9oqXhi/NOhwRCRYR7v7VcAWd78POArIDjgmiVAfzV7LtcMm8dJXS/nx0S247piWxMVZ0GGJRK2yfmZjZnYUcDlwXVnquvsaYE34+Q4zmws0AeYUKzYY+KeHhlO/NbM6ZtYoXFdKccuJbZi/bjt/+WAubRqkcUL7rKBDEpFg7A1/3W1mjYFNQMsA45EIlFdQyJOfLeKpzxeRlBDHLwe154bjW2Om5FnkhyjrCPTPgLuBUe4+28xaAZ+X9U3Cu2P1BCaUuNSE0J3j++WGz5WsP8TMJpvZ5A0bNpT1bWNSXHjOWoeGtbj11WksWr8z6JBEJBjvmlkd4GFgKrAMeK0sFc1skJnND0+fu+sgZQaa2fTwFLyx5akrkWHLrn1c849JPPX5Is7r2YTZ953GjQPbKHkWqQBlSqDdfay7n+PuD4ZvJtzo7reWpW74DvGRwM/cveS+1KX9FP/P5F53f97dc9w9JzMzsyxvG9NSkhL4+9U51EiM46f/nMy23dqpUKQ6CffDn7n7VncfSWjucwd3P+xNhGYWDzxNaApdJ+DSktPrwon5M8A57t6Z0PzqMtWV4Lk7f/tkAQMe/A8Tlm7mkQu78+jFPUjU5igiFaasq3C8ama1zCyV0BSM+WZ2ZxnqJRJKnoe7+1ulFMnlwDl7TYHVZYmpumtSpyZDr+hN7pbd3PTqVPILiw5fSURigrsXAY8UO85z921lrN4XWOTuS8K7F44gNJ2uuMuAt9x9Rfj115ejrgRs9IzVPP7ZQo5vl8kHtx3L+bpRUKTClfXP0U7h0eMfAWOAZsCVh6pgoc+IXgTmuvujByk2GrgqvBpHf2Cb5j+XXU6LDP50ble+XLSR3709SytziFQvH5vZ+Vb+z+PLMnWuHVDXzL4wsylmdlU56mraXYByt+zmL2Pm0bVJbZ66rBftGqQHHZJITCrrTYSJ4dHkHwFPuXu+mR0uWxtAKMmeaWbTw+d+TSj5xt2HEkrGzwAWAbsJLY8n5XBRTjYrNu3mqc8XkZ2Rwk0ntAk6JBGpGrcDqUCBme0lNCXO3b3WYeqVZepcAtAbOAmoCXxjZt+WsS7u/jzwPEBOTo7+sq8CBYVFvDkll79+PJ99BUX8+dwc4rXKhkilKWsC/RyhG1RmAOPMrDlQcj7zAdz9S0rvbIuXceCmMsYgB3HHqe3I3bKbhz+aT9O6NRnc438GhEQkxrj7kQ4tlmXqXC6he112AbvMbBzQvYx1pYoVFjlD/jWF/8xbT+fGtfjrhd3p2Ohwf0eJyA9RpgTa3Z8Anih2armZnVA5IUl5mRkPXtCNtdv3cucb39GgVjL9W9ULOiwRqURmdlxp59193GGqTgLamllLYBVwCaE5z8W9AzxlZglAEtAP+Bswrwx1pYrd9+5s/jNvPT85piW/OK09yYnxQYckEvPKlECbWW3gXmB/hz0WuB8o600rUslqJMTz3BU5nPfsVwz552TeuvFo2mRp7ptIDCt+I3cyoRv8pgAnHqqSuxeY2c3AR0A88FJ4edLrw9eHuvtcM/sQ+A4oAl5w91kApdWt4HZJGS3esJO735rJxKWbue6Ylvz2LC2IIlJVrCw3npnZSGAW8HL41JVAd3c/rxJjK1VOTo5Pnjy5qt82aqzcvJtzn/mK5MR43rrhaLJqJQcdkogUY2ZT3D2nEl43G3jI3S+t6Nf+IdRnV44pyzdz6fMTiI8zrj66Bb84tR0JWqZOpMIdrM8u609ba3e/N7x00ZLwtrGtKjZEqQjZGSm89OM+bN61j6temsi2PVojWqSayAW6BB2EVL6Zudu47uXJZNWqwad3HM9dp3dQ8ixSxcr6E7fHzI7Zf2BmA4A9lROS/FDdmtbhuSt7s3jDTq4bNok9+wqDDklEKpiZPWlmT4QfTwHjCd3oLTFs7prtXPfyJFKTEhj+k340qVMz6JBEqqWyJtDXA0+b2TIzWwY8BfxfpUUlP9ixbTN57OKeTFmxhRuHT9FGKyKxZzKhOc9TgG+AX7n7FcGGJJXp49lrOf3x8ezMK+D5q3rTvF5q0CGJVFtlXYVjBtDdzGqFj7eb2c8I3WAiEerMbo3YuqcLvxk1izvfmMGjF/UgTuuCisSKN4G97l4IoW22zSzF3XcHHJdUgpm527j5tWnUTUnkjeuPpk1WWtAhiVRr5Zo05e7bwzsSQmgRf4lwl/drzp2nteft6au5/7052q1QJHZ8RmiTk/1qAp8GFItUooLCIh75ZD6JccaoGwcoeRaJAGXdSKU0GsqMEjcObM3mXft48culpCcncMep7YMOSUR+uGR337n/wN13mllKkAFJxfts7jr+PGYuizfs4tdndKBFfU3bEIkEPySB1lBmlDAzfnNGR3bszefJ/ywiMT6OW09qG3RYIvLD7DKzXu4+FcDMeqObu2OGu/OPr5Zx/3tzSIgznrm8F2d0bRR0WCISdsgE2sx2UHqibBz40aFEuLg44y/ndaOg0Hn0kwUkxsdxw8DWQYclIkfuZ8AbZrZ/K+1GwMXBhSMV6e/jl/DnMfM4pk19nruyN6k1fsh4l4hUtEP+RLq7trKLIfFxxsMXdie/yHnww3kkxhs/OVbLeYtEI3efZGYdgPaEBjXmubsWfo8Bb0xeyQMfzGNQ54Y8c3kv3fwtEoG08no1Ex9nPHpRd07v0pA/vj+Xf36zLOiQROQImNlNQKq7z3L3mUCamd0YdFzyw7wzfRW/GvkdA9rU59GLuyt5FolQ+kyoGkqMj+PxS3qSP3wq97wzGzPjyv7Ngw5LRMrnp+7+9P4Dd99iZj8FngkwJjkC+wqK+M2omYyesZq8giI6NqrFs1f0JiVJv6JFIpVGoKuppIQ4nr68Jyd1yOJ3b8/ixS+XBh2SiJRPnJl9PzxpZvFAUoDxyBH626cLeGNKLn1bZnByxyxG3Xg0aZrzLBLR9BNajdVIiOfZK3pz24hp/OG9OezNL+SmE9oEHZaIlM1HwOtmNpTQzd7XAx8EG5KU1zeLNzF07GIu7ZvNX87rFnQ4IlJGGoGu5pIS4njy0p4M7tGYhz+az6Mfz9dmKyLR4VeENlO5AbiJ0M6wWh0pikxdsYXbRkyjZb1UfndWp6DDEZFy0Ai0kBAfx6MX9aBGQhxP/GcRewuKuPv0DhT7dFhEIoy7F5nZt0ArQsvXZQAjg41KyuqL+ev5v39NISM1iScu7an5ziJRRj+xAoRW53jgvG7USIjn+XFL2L2vgPvO6UK87gAXiShm1g64BLgU2AT8G8DdTyjHawwCHgfigRfc/YES1wcC7wD7b454y93vD19bBuwACoECd8858tZUP8s37eLfk1byz2+W0zozjVd+0o+MVE1dF4k2SqDle3Fxxv2DO5NSI57nxi5hy658Hr24OzUS4oMOTUT+ax4wHjjb3RcBmNnPy1o5fLPh08ApQC4wycxGu/ucEkXHu/tZB3mZE9x9Y/lDr9627t7HRc99w7rteXRtUptnLu+l5FkkSimBlgOYGXef3pF6qUn8ecw8tuzex3NX9iY9OTHo0EQk5HxCI9Cfm9mHwAhCG6mUVV9gkbsvATCzEcBgoGQCLRWooLCIe96ZzYYdeYy+eQDdmtYJOiQR+QF0E6GUashxrXnkwu5MWLqZS//+LRt25AUdkogA7j7K3S8GOgBfAD8HGpjZs2Z2ahleogmwsthxbvhcSUeZ2Qwz+8DMOhcPAfjYzKaY2ZDS3sDMhpjZZDObvGHDhrI0K6btzS/k5lenMXrGaq4d0FLJs0gMUAItB3V+76a8cFUOi9bv5MKhX7Ni0+6gQxKRMHff5e7Dw9MsmgLTgbvKULW00eqSS+9MBZq7e3fgSeDtYtcGuHsv4HTgJjM7rpTYnnf3HHfPyczMLENIse2Jzxby4ey1nNyxAb/VahsiMUEJtBzSCR2yGP6T/mzZnc95z37FtBVbgg5JREpw983u/py7n1iG4rlAdrHjpsDqEq+33d13hp+PARLNrH74eHX463pgFKEpIVKKjTvzuGjoNzzzxWJO6pDF0Ct6BR2SiFQQJdByWL2b12XkDUdTMymeS57/ljEz1wQdkogcuUlAWzNraWZJhOZTjy5ewMwa7t/l0Mz6EvpdscnMUs0sPXw+FTgVmFWl0UeJxRt2csGzXzMjdyvXH9+apy7rRUK8fuWKxArdRChl0iYrjbdvHMCQf03hxuFT+eWg9txwfGutFS0SZdy9wMxuJrSTYTzwkrvPNrPrw9eHAhcAN5hZAbAHuMTd3cwaAKPCP/cJwKvu/mEgDYlA7s5LXy3jo9lrmbRsMwAvX9OX49ppGotIrLFo23UuJyfHJ0+eHHQY1dbe/ELufPM73p2xmotzsvnDj7qQlKBRFZGyMrMp1Wnt5OrUZ785JZdfvDGDdg3SaN+wFjcc35pOjWsFHZaI/AAH67M1Ai3lkpwYzxOX9KBl/VSe+GwhK7fs5tnLe1M7RcvciUj1tXX3Pn4/ejZ9W2bw2k/7axMqkRinoUMpNzPj9lPa8ciF3Zm0bDPnPP0l89fuCDosEZHA/Oub5ezMK+C+czoreRapBiotgTazl8xsvZmVeoOJmQ00s21mNj38uKeyYpHKcX7vpowY0p/d+wo595mv+EA3F4pINePuPPrJAh75ZAGndW5Ax0aasiFSHVTmCPQwYNBhyox39x7hx/2VGItUkt7NM3jvlmNo3zCdG4ZP5aEP51FYFF3z6kVEjtTYBRt44rOFDO7RmIcv7B50OCJSRSotgXb3ccDmynp9iRwNaiUzYkh/Lu2bzTNfLObaYZPYtjs/6LBERCrVpp153PPObJrXS+HhC7pTK1n3gohUF0HPgT7YVrEH0Lawka9GQjx/Oa8bfz63K18v3sg5T3/JrFXbgg5LRKRSFBQW8ZN/Tmbd9r08elF3rUYkUs0E+RN/qK1iD6BtYaPHZf2aMWJIf/Lyizjv2a/517fLibalEkVEDufNKblMW7GVhy7oRu/mGUGHIyJVLLAE+lBbxUp06908g/dvPYajWtXjd2/P4ubXprFjr6Z0iEhs2JtfyGOfLqRXszqc071x0OGISAACS6APtlVsUPFIxaqXVoN//LgPvxrUgQ9nreWsJzWlQ0Riw2dz17N2+15+dnI77cYqUk1V5jJ2rwHfAO3NLNfMrjOz6/dvF0toq9hZZjYDeILwVrGVFY9Uvbg444aBrRkxpD/7Coo475mvGfbVUk3pEJGotXnXPp78z0LqpiRydOt6QYcjIgGptJ0I3f3Sw1x/Cniqst5fIkefFhm8f+ux/OKNGfz+3Tl8Pn8DD1/QjaxayUGHJiJSZpt25nHx89+yYvNu7junMwnxunFQpLrST79UiYzUJF68Oof7B3fm2yWbOO2xcXw4a23QYYmIlNnv3pnFis27efmavlzat1nQ4YhIgJRAS5UxM646qgXv33osTeumcP0rU7jzjRnszCsIOjQRkUOat3Y7Y2au5YbjW3OUpm6IVHtKoKXKtclK460bj+aWE9swcmoupz8+jglLdP+oiESmoiLn0Y8XkJoUzzUDWgQdjohEACXQEojE+DjuOLU9b1x/FIZx8fPf8ru3Z2k0WkQizvAJy/l4zjpuPKENdVKSgg5HRCKAEmgJVO/mGXz4s2O5ZkALXpmwnNP+No6xC7TbpEhlMrNBZjbfzBaZ2V2lXB9oZtvMbHr4cU9Z68aa9dv38ugnC+jXMoMbB7YOOhwRiRBKoCVwKUkJ3Ht2Z968/iiSE+O4+qWJ3PH6DLbu3hd0aCIxx8zigaeB04FOwKVm1qmUouPdvUf4cX8568YEd+eBD+exK6+QP53bRWs+i8j3lEBLxAjtYHgsN53Qmrenr+KUv43jg5lrtG60SMXqCyxy9yXuvg8YAQyugrpR5+Wvl/HW1FVce0xL2mSlBx2OiEQQJdASUZIT47nztA68c9MAMtNqcMPwqVwzbBLLN+0KOjSRWNEEWFnsODd8rqSjzGyGmX1gZp3LU9fMhpjZZDObvGFDdE7J+nTOOv7w/lxO6pDFL09rH3Q4IhJhlEBLROrSpDajbx7A787qxORlWzjlb+N47NMF7M0vDDo0kWhX2jyEkh/zTAWau3t34Eng7XLUxd2fd/ccd8/JzMz8IbEGYvbqbfz89el0bJTO45f2JC5OUzdE5EBKoCViJcTHcd0xLfnsjuM5rXNDHvt0Iac9ppsMRX6gXCC72HFTYHXxAu6+3d13hp+PARLNrH5Z6ka7vIJCfjNqFjUS4hl6RW/SalTahr0iEsWUQEvEa1ArmScv7ckr1/Uj3oyrX5rIDa9MYeXm3UGHJhKNJgFtzaylmSUBlwCjixcws4YWvmPOzPoS+l2xqSx1o9mefYX89J9TmL5yK787qyNN66YEHZKIRCj9aS1R45i29fngZ8fy93FLePrzxXw2dz3XHtOSm05oTXpyYtDhiUQFdy8ws5uBj4B44CV3n21m14evDwUuAG4wswJgD3CJh+7mLbVuIA2pBPe/N5svF27gofO7MbhHadPCRURCLNpWOMjJyfHJkycHHYYEbO22vTz80XxGTs2lfloSt5/Snov7ZBOvuYoS4cxsirvnBB1HVYmWPvv1ySu5a+R3XHVUC35/TufDVxCRauFgfbamcEhUalg7mUcu6s7omwfQsn4qvx41kzOfGM+XCzcGHZqIRJHCIucvY+byyze/Y0Cb+vxykFbcEJHDUwItUa1b0zq8/n9H8czlvdi1r4ArXpzAlS9O4LvcrUGHJiIRrqCwiF+++R3PjVvC+b2a8sLVOaQkaWajiByeEmiJembGGV0b8cnPj+e3Z3Zk9urtnPPUV9w4fAqL1u8MOjwRiVDvz1zDyKm5XHVUc/56YTdqJMQHHZKIRAkl0BIzkhPj+cmxrRh750BuO6ktY+dv4NS/jeWXb85g1dY9QYcnIhFk9upt3P/uHFrVT+X3Z3fWNt0iUi5KoCXmpCcn8vNT2jHulyfw46Nb8va01Zzw8Bfc9+5s1m3fG3R4IhKwgsIibnl1GkkJcbxwdY42ShGRclMCLTGrXloN7jm7E5/fOZAf9WzMP79ZzrEPfc6978xizTaNSItUV29MyWXJxl3cP7gLrTLTgg5HRKKQEmiJeU3q1OShC7rz+R0DOa9nE4ZPWMHxD33Bb0bNJHeLNmMRqU4+n7+e34+eTZ8WdTm5Y1bQ4YhIlFICLdVGs3opPHB+N764cyAX5jTl9ckrGfjwF/zqze9YtnFX0OGJSCVbsG4Ht7w6jTZZaTx3ZY7mPYvIEVMCLdVO07op/Oncroy98wQu79eMUdNXccIjX3DDK1OYtmJL0OGJSCVYunEXV704kZSkeJ67sjcZqUlBhyQiUUwLXkq11bhOTe4b3IWbTmzDy18v41/fLOeDWWvp2yKDnx7XipM6ZOnmIpEYsGFHHuc/+zV78wv595CjaFo3JeiQRCTKaQRaqr2s9GTuPK0DX999Er87qxOrtu7hp/+czCl/G8uIiSvYm18YdIgi8gMMn7CcLbv38daNR9O1ae2gwxGRGKAEWiQsrUYC1x3Tki/uHMjjl/SgRkI8d701k2Me/A+PfDxfK3eIRKGVm3fzyrfL6d+yHh0a1go6HBGJEZrCIVJCYnwcg3s04Zzujflq0SaGfb2Upz5fxDNfLGZQ54ZcdVRz+rbM0A1IIhFu2558bhw+lbyCIn53VqegwxGRGKIEWuQgzIxj2tbnmLb1vx/FGjFpJe/PXEOHhulcfXQLBvdoTEqSfoxEIk1hkXP1SxOZt3Y7z17em06NNfosIhVHUzhEyiA7I4W7z+jIt3efxIPnd8XMuPutmfT/82fc9+5s5q/dEXSIIlLMiEkrmL5yKw9d0I2TOzUIOhwRiTEaOhMph5pJ8VzcpxkX5WQzefkWXv56GcO/XcE/vlpGz2Z1uKRPNmd1a0xqDf1oSeQys0HA40A88IK7P3CQcn2Ab4GL3f3N8LllwA6gEChw95wqCboc3vtuNb8ZNYv+rTL4UY8mQYcjIjFIv+VFjoCZ0adFBn1aZLB51z7emprLiEkr+dXImdz/7hzO6dGYS/o0o1vT2porLRHFzOKBp4FTgFxgkpmNdvc5pZR7EPiolJc5wd03VnqwR+DLhRu5+dVpNKlTk+eu0GYpIlI5Ki2BNrOXgLOA9e7epZTrRmgE5AxgN/Bjd59aWfGIVJaM1CR+cmwrrjumJVNXbGHExJW8PW01r01cSYeG6VzQuynn9GhMVnpy0KGKAPQFFrn7EgAzGwEMBuaUKHcLMBLoU7XhHbl/frOMe96ZDcBNJ7ShdkpiwBGJSKyqzDnQw4BBh7h+OtA2/BgCPFuJsYhUOjOjd/MMHr6wOxN+cxJ/OrcLNRLi+OP7c+n/58+4+qWJvDN9FXv2aV1pCVQTYGWx49zwue+ZWRPgXGBoKfUd+NjMppjZkEqL8gh8MHMtABN+fRKX9WsWcDQiEssqbQTa3ceZWYtDFBkM/NPdHfjWzOqYWSN3X1NZMYlUlVrJiVzerzmX92vOovU7GTUtl7enrea2EdNJTYpnUJdGnNerCf1b1SNeux1K1SrtG85LHD8G/MrdC0uZAjHA3VebWRbwiZnNc/dxB7xBKLEeAtCsWdUksnkFhUxbuYVrBrSgQS192iMilSvIOdAHGwX5nwQ6iM5YpKK0yUrjztM6cMcp7Zm4bDOjpq5izMw1jJyaS8NayZzdvRFndmtMd82XlqqRC2QXO24KrC5RJgcYEf5+rA+cYWYF7v62u68GcPf1ZjaK0JSQAxJod38eeB4gJyenZHJeKT6ds569+UUMbJ9VFW8nItVckAl0WUZBQicD6IxFKlpcnNG/VT36t6rHfYM78+ncdbw9bRXDvl7G38cvpUmdmpzVrRFndmtE1yZKpqXSTALamllLYBVwCXBZ8QLu3nL/czMbBrzn7m+bWSoQ5+47ws9PBe6vssgP4d+TV9K4djLHtKkfdCgiUg0EmUCXZRREJCYlJ8ZzVrfGnNWtMdv25PPpnHW8P3MNL321lOfGLSE7oyZndm3MmV0b0aVJLSXTUmHcvcDMbia0ukY88JK7zzaz68PXS5v3vF8DYFT4+zEBeNXdP6zsmA9nxsqtjF+4gVtObKspUSJSJYJMoEcDN4fvAO8HbNP8Z6mOatdM5PzeTTm/d1O27c7n4zlreX/mGl4Yv4ShYxfTLCOF0zo34JRODendvK4SBPnB3H0MMKbEuVITZ3f/cbHnS4DulRpcOe3NL+Rn/55Oo1rJXDeg5eEriIhUgMpcxu41YCBQ38xygXuBRPi+ox5DaAm7RYSWsbumsmIRiRa1UxK5MCebC3Oy2bp7Hx/PDo1Mv/z1cv4+fin1UpM4sUMWp3RqwLFtM6mZFB90yCKBeumrpSzduItXruunZetEpMpU5ioclx7mugM3Vdb7i0S7OilJXNQnm4v6ZLNjbz5jF2zgkznr+HD2Wt6YkktyYhzHtMnk1E4NOKljFvXSagQdskiVWrNtD499upBTOzXgmLaa+ywiVUc7EYpEgfTkxO/nTOcXFjFx6WY+mbOOj2ev5dO56zCDntl1OL5dFgPbZ9K1SW3iNNVDYtyYmWvZV1DE3Wd0DDoUEalmlECLRJnE+DgGtKnPgDb1uffsTsxevZ1P5qzjiwUbeOyzBfzt0wXUS03iuHaZDGyfybFtM8lITQo6bJEK98HMNXRomE7L+qlBhyIi1YwSaJEoZmZ0aVKbLk1q8/NT2rFpZx7jF27ki/nrGbtgA6OmrcIMujWtw8BwQt2taR3diChRb/XWPUxevoXbT2kXdCgiUg0pgRaJIfXSavCjnk34Uc8mFBU5M1dt44v5G/hiwXqe+M9CHv9sIenJCfRvVY8BresxoE192mSlaZk8iTpP/mcRifHGuT2bHL6wiEgFUwItEqPi4ozu2XXonl2H205uy5Zd+xi/aCNfL9rIV4s38smcdQBkptfg6Nb1GNC6Pke3qUfTuikBRy5ycPmFRYyYuILXJ6/kyv7Nyc7Q96uIVD0l0CLVRN3UJM7p3phzujcGYOXm3Xy9eCNfLdrEV4s28c700D5GzeulcHTr0I6JfVtm0Kh2zSDDFjnA458u5KnPF9EmK41bT2obdDgiUk0pgRapprIzUrg4oxkX92mGu7Nw/U6+WhRKqN+bsYbXJq4EoGndmvRtkUGflhn0aZFB68xUTfmQQLg7Iyat4Ni29fnHj/uQEB8XdEgiUk0pgRYRzIx2DdJp1yCdawa0pKCwiHlrdzBx6WYmLdvM2AUbeGvaKgDqpSaR06IufVpk0LdlBp0a1VIiI1Vi7podbNy5j7O7N9b3nIgESgm0iPyPhPi471f3uPaYlrg7SzbuYtLSzUxcFkqqP5odmkOdmhRP9+w69GxWh57ZdenRrA71tamLVLDCIufRTxaQnBjHCe2zgg5HRKo5JdAiclhmRuvMNFpnpnFJ32YArN22N5RML93MtJVbGDp2CYVFDkB2Rk16ZtelZ7M69MiuQ6fGtaiRoG3H5cg98vF8Pp27jt+e2ZHMdP2BJiLBUgItIkekYe3kA25K3LOvkJmrtjF95RamrdjKxKWbGT0jdGNiUnwcnZvUokd2KKHu2qQ2LeqlardEKZOPZq/lmS8Wc3FONj85tlXQ4YiIKIEWkYpRMymevi1D86L3W7NtD9NXbGXayq1MX7GV1yau4B9fLQMgrUYCnRrXomuT2nQNTxdpWT9Vm7zI/3jpy6VkZ9TkT+d2CToUERFACbSIVKJGtWvSqGtNTu/aCAit4btg3Q5mr9rOzFXbmLlqG698u5y8giIgNJ+6U+NadCmWVLfOTFNSXY1t253PpGWbufmENrpxUEQihhJoEakyifFxdG5cm86Na3NRn2wACgqLWLRhJzNztzF7dSixHjFxJf/IXwZAcmIc7Rqk06FhOh0b1aJDw1p0aJhO3dSkAFsiVeXhj+dR5HBsu8ygQxER+Z4SaBEJVEJ8XDgprsWF4XOFRc7icFI9d8125q3dwWdz1/P65Nzv6zWslUyHRul0aFiLjo1CyXXL+qkkapTysMxsEPA4EA+84O4PHKRcH+Bb4GJ3f7M8dSvCovU7eOXbFZzaqQG9mtWtrLcRESk3JdAiEnHi4/67LnVx63fsZd6aHcxbu515a3Ywd+0Ovlq0hPzC0OofSfFxtMlKo33DdNpkpdE2K402WWk0y0jRx/9hZhYPPA2cAuQCk8xstLvPKaXcg8BH5a1bUV76ahlJCXH85byumsYjIhFFCbSIRI2s9GSy0pM5rtjH+fmFRSzZsIt5a7czd80O5q7ZzoQlmxgV3vgFQol1q8xU2oQT6rZZ6bRtkEaLeqkkJVS7xLovsMjdlwCY2QhgMFAyCb4FGAn0OYK6P9jmXfsYOSWX83o2oZ7WFReRCKMEWkSiWmJ8HO0bptO+YTqDe/z3/M68Ahav38nC9TtZuH4Hi9bt5Lvcbbw/cw0eGrAmPs5oXi/l+5HqVvXTaJmZSqv6qdRJidk51k2AlcWOc4F+xQuYWRPgXOBEDkygD1s3XH8IMASgWbNmRxTkqxNCN5dee0zLI6ovIlKZlECLSExKq5FA9+w6dM+uc8D5vfmFLN6wk0Xrd7JwXfjr+h18Onf99xvBANRNSaRVZhot66fSsn4oqW6ZmUqLeqkkJ0b1pjClzYXwEsePAb9y90KzA4qXpS7u/jzwPEBOTs7/XD+cvIJCXv5mOce1y/yfaTwiIpFACbSIVCvJifHfrwRSXH5hESs372bpxl0s3biLxRt2sXTjTsYv3MCbU/5786IZNK5dkxsGtuaK/s2rOvyKkAtkFztuCqwuUSYHGBFOnusDZ5hZQRnr/mDvzVjDhh15PHKhRp9FJDIpgRYRITQVpFVmGq0y0/7n2s68ApaFE+sl4cQ6ireTngS0NbOWwCrgEuCy4gXc/fvM1cyGAe+5+9tmlnC4uhUhPTmBQZ0bcmzb+hX90iIiFUIJtIjIYaTVSKBLeGOXaOfuBWZ2M6HVNeKBl9x9tpldH74+tLx1KzrGUzs35NTODSv6ZUVEKowSaBGRasbdxwBjSpwrNXF29x8frq6ISHVT7dZvEhERERH5IZRAi4iIiIiUgxJoEREREZFyUAItIiIiIlIOSqBFRERERMpBCbSIiIiISDkogRYRERERKQdz96BjKBcz2wAsP4Kq9YGNFRxOkGKpPbHUFlB7IlkktKW5u2cGHEOVUZ/9PbUncsVSWyC22hMJbSm1z466BPpImdlkd88JOo6KEkvtiaW2gNoTyWKpLbEu1v6v1J7IFUttgdhqTyS3RVM4RERERETKQQm0iIiIiEg5VKcE+vmgA6hgsdSeWGoLqD2RLJbaEuti7f9K7YlcsdQWiK32RGxbqs0caBERERGRilCdRqBFRERERH4wJdAiIiIiIuUQ8wm0mQ0ys/lmtsjM7go6nrIws5fMbL2ZzSp2LsPMPjGzheGvdYtduzvcvvlmdlowUZfOzLLN7HMzm2tms83stvD5aG1PsplNNLMZ4fbcFz4fle0BMLN4M5tmZu+Fj6O5LcvMbKaZTTezyeFzUdue6ira+u1Y6rMhtvrtWOyzQf12RLTH3WP2AcQDi4FWQBIwA+gUdFxliPs4oBcwq9i5h4C7ws/vAh4MP+8UblcNoGW4vfFBt6FY3I2AXuHn6cCCcMzR2h4D0sLPE4EJQP9obU84xtuBV4H3ovl7LRzjMqB+iXNR257q+IjGfjuW+uxwjDHTb8dinx2OU/12wO2J9RHovsAid1/i7vuAEcDggGM6LHcfB2wucXow8HL4+cvAj4qdH+Huee6+FFhEqN0Rwd3XuPvU8PMdwFygCdHbHnf3neHDxPDDidL2mFlT4EzghWKno7IthxBr7Yl1Uddvx1KfDbHVb8danw3qt4mQ9sR6At0EWFnsODd8Lho1cPc1EOrcgKzw+ahpo5m1AHoSGgGI2vaEPzqbDqwHPnH3aG7PY8AvgaJi56K1LRD6xfixmU0xsyHhc9HcnuooVv5fYuL7Lhb67Rjrs0H9dkS0JyGIN61CVsq5WFu3LyraaGZpwEjgZ+6+3ay0sENFSzkXUe1x90Kgh5nVAUaZWZdDFI/Y9pjZWcB6d59iZgPLUqWUcxHRlmIGuPtqM8sCPjGzeYcoGw3tqY5i/f8latoXK/12rPTZoH6bCGpPrI9A5wLZxY6bAqsDiuWHWmdmjQDCX9eHz0d8G80skVAnPNzd3wqfjtr27OfuW4EvgEFEZ3sGAOeY2TJCH5OfaGavEJ1tAcDdV4e/rgdGEfpoL2rbU03Fyv9LVH/fxWK/HQN9Nqjfjpj2xHoCPQloa2YtzSwJuAQYHXBMR2o0cHX4+dXAO8XOX2JmNcysJdAWmBhAfKWy0JDFi8Bcd3+02KVobU9meBQDM6sJnAzMIwrb4+53u3tTd29B6GfjP+5+BVHYFgAzSzWz9P3PgVOBWURpe6qxWOm3o/b7Lpb67Vjqs0H9NpHUniDuXKzKB3AGoTuIFwO/CTqeMsb8GrAGyCf019Z1QD3gM2Bh+GtGsfK/CbdvPnB60PGXaMsxhD5e+Q6YHn6cEcXt6QZMC7dnFnBP+HxUtqdYjAP5793cUdkWQqs2zAg/Zu//eY/W9lTnR7T127HUZ4fji5l+O1b77HCc6rcDbI+28hYRERERKYdYn8IhIiIiIlKhlECLiIiIiJSDEmgRERERkXJQAi0iIiIiUg5KoEVEREREykEJtMQ0M/uNmc02s+/MbLqZ9TOzn5lZStCxiYjIgdRnS7TQMnYSs8zsKOBRYKC755lZfSAJ+BrIcfeNgQYoIiLfU58t0UQj0BLLGgEb3T0PINz5XgA0Bj43s88BzOxUM/vGzKaa2RtmlhY+v8zMHjSzieFHm/D5C81slpnNMLNxwTRNRCTmqM+WqKERaIlZ4U71SyAF+BT4t7uPNbNlhEczwiMcbxHazWiXmf0KqOHu94fL/d3d/2RmVwEXuftZZjYTGOTuq8ysjrtvDaJ9IiKxRH22RBONQEvMcvedQG9gCLAB+LeZ/bhEsf5AJ+ArM5sOXA00L3b9tWJfjwo//woYZmY/BeIrJXgRkWpGfbZEk4SgAxCpTO5eCHwBfBEehbi6RBEDPnH3Sw/2EiWfu/v1ZtYPOBOYbmY93H1TxUYuIlL9qM+WaKERaIlZZtbezNoWO9UDWA7sANLD574FBhSbK5diZu2K1bm42NdvwmVau/sEd78H2AhkV14rRESqB/XZEk00Ai2xLA140szqAAXAIkIfDV4KfGBma9z9hPBHhK+ZWY1wvd8CC8LPa5jZBEJ/bO4f8Xg43Mkb8BkwoyoaIyIS49RnS9TQTYQiB1H8xpWgYxERkUNTny1VSVM4RERERETKQSPQIiIiIiLloBFoEREREZFyUAItIiIiIlIOSqBFRERERMpBCbSIiIiISDkogRYRERERKYf/BwWJaFHqqOYkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for number 7\n",
      "total step : 1 \n",
      "error : 4.436948, accuarcy : 0.340170\n",
      "total step : 2 \n",
      "error : 4.416498, accuarcy : 0.341171\n",
      "total step : 3 \n",
      "error : 4.396115, accuarcy : 0.341671\n",
      "total step : 4 \n",
      "error : 4.375801, accuarcy : 0.343172\n",
      "total step : 5 \n",
      "error : 4.355556, accuarcy : 0.343672\n",
      "total step : 6 \n",
      "error : 4.335379, accuarcy : 0.344172\n",
      "total step : 7 \n",
      "error : 4.315273, accuarcy : 0.345173\n",
      "total step : 8 \n",
      "error : 4.295236, accuarcy : 0.347174\n",
      "total step : 9 \n",
      "error : 4.275270, accuarcy : 0.348174\n",
      "total step : 10 \n",
      "error : 4.255374, accuarcy : 0.350675\n",
      "total step : 11 \n",
      "error : 4.235551, accuarcy : 0.351176\n",
      "total step : 12 \n",
      "error : 4.215798, accuarcy : 0.352676\n",
      "total step : 13 \n",
      "error : 4.196118, accuarcy : 0.355678\n",
      "total step : 14 \n",
      "error : 4.176511, accuarcy : 0.358179\n",
      "total step : 15 \n",
      "error : 4.156976, accuarcy : 0.359180\n",
      "total step : 16 \n",
      "error : 4.137515, accuarcy : 0.359680\n",
      "total step : 17 \n",
      "error : 4.118127, accuarcy : 0.359180\n",
      "total step : 18 \n",
      "error : 4.098813, accuarcy : 0.361181\n",
      "total step : 19 \n",
      "error : 4.079573, accuarcy : 0.360680\n",
      "total step : 20 \n",
      "error : 4.060408, accuarcy : 0.363182\n",
      "total step : 21 \n",
      "error : 4.041318, accuarcy : 0.364182\n",
      "total step : 22 \n",
      "error : 4.022302, accuarcy : 0.365183\n",
      "total step : 23 \n",
      "error : 4.003363, accuarcy : 0.366683\n",
      "total step : 24 \n",
      "error : 3.984499, accuarcy : 0.367684\n",
      "total step : 25 \n",
      "error : 3.965711, accuarcy : 0.367184\n",
      "total step : 26 \n",
      "error : 3.946999, accuarcy : 0.368184\n",
      "total step : 27 \n",
      "error : 3.928364, accuarcy : 0.369185\n",
      "total step : 28 \n",
      "error : 3.909806, accuarcy : 0.370685\n",
      "total step : 29 \n",
      "error : 3.891325, accuarcy : 0.373687\n",
      "total step : 30 \n",
      "error : 3.872921, accuarcy : 0.376688\n",
      "total step : 31 \n",
      "error : 3.854595, accuarcy : 0.378189\n",
      "total step : 32 \n",
      "error : 3.836347, accuarcy : 0.379190\n",
      "total step : 33 \n",
      "error : 3.818177, accuarcy : 0.380690\n",
      "total step : 34 \n",
      "error : 3.800085, accuarcy : 0.380190\n",
      "total step : 35 \n",
      "error : 3.782071, accuarcy : 0.381191\n",
      "total step : 36 \n",
      "error : 3.764136, accuarcy : 0.383192\n",
      "total step : 37 \n",
      "error : 3.746280, accuarcy : 0.383192\n",
      "total step : 38 \n",
      "error : 3.728503, accuarcy : 0.384692\n",
      "total step : 39 \n",
      "error : 3.710805, accuarcy : 0.387694\n",
      "total step : 40 \n",
      "error : 3.693187, accuarcy : 0.387694\n",
      "total step : 41 \n",
      "error : 3.675648, accuarcy : 0.390195\n",
      "total step : 42 \n",
      "error : 3.658188, accuarcy : 0.390695\n",
      "total step : 43 \n",
      "error : 3.640808, accuarcy : 0.391196\n",
      "total step : 44 \n",
      "error : 3.623508, accuarcy : 0.392696\n",
      "total step : 45 \n",
      "error : 3.606288, accuarcy : 0.393697\n",
      "total step : 46 \n",
      "error : 3.589148, accuarcy : 0.396198\n",
      "total step : 47 \n",
      "error : 3.572088, accuarcy : 0.397699\n",
      "total step : 48 \n",
      "error : 3.555107, accuarcy : 0.398199\n",
      "total step : 49 \n",
      "error : 3.538207, accuarcy : 0.399200\n",
      "total step : 50 \n",
      "error : 3.521387, accuarcy : 0.399200\n",
      "total step : 51 \n",
      "error : 3.504646, accuarcy : 0.400200\n",
      "total step : 52 \n",
      "error : 3.487986, accuarcy : 0.401701\n",
      "total step : 53 \n",
      "error : 3.471405, accuarcy : 0.402701\n",
      "total step : 54 \n",
      "error : 3.454905, accuarcy : 0.403702\n",
      "total step : 55 \n",
      "error : 3.438484, accuarcy : 0.406703\n",
      "total step : 56 \n",
      "error : 3.422142, accuarcy : 0.408704\n",
      "total step : 57 \n",
      "error : 3.405880, accuarcy : 0.409705\n",
      "total step : 58 \n",
      "error : 3.389698, accuarcy : 0.410705\n",
      "total step : 59 \n",
      "error : 3.373595, accuarcy : 0.411706\n",
      "total step : 60 \n",
      "error : 3.357570, accuarcy : 0.414207\n",
      "total step : 61 \n",
      "error : 3.341625, accuarcy : 0.414707\n",
      "total step : 62 \n",
      "error : 3.325758, accuarcy : 0.416708\n",
      "total step : 63 \n",
      "error : 3.309970, accuarcy : 0.418209\n",
      "total step : 64 \n",
      "error : 3.294261, accuarcy : 0.419210\n",
      "total step : 65 \n",
      "error : 3.278629, accuarcy : 0.420710\n",
      "total step : 66 \n",
      "error : 3.263075, accuarcy : 0.423712\n",
      "total step : 67 \n",
      "error : 3.247599, accuarcy : 0.424212\n",
      "total step : 68 \n",
      "error : 3.232201, accuarcy : 0.425713\n",
      "total step : 69 \n",
      "error : 3.216879, accuarcy : 0.426713\n",
      "total step : 70 \n",
      "error : 3.201635, accuarcy : 0.429215\n",
      "total step : 71 \n",
      "error : 3.186467, accuarcy : 0.429215\n",
      "total step : 72 \n",
      "error : 3.171376, accuarcy : 0.429715\n",
      "total step : 73 \n",
      "error : 3.156361, accuarcy : 0.432216\n",
      "total step : 74 \n",
      "error : 3.141422, accuarcy : 0.435218\n",
      "total step : 75 \n",
      "error : 3.126558, accuarcy : 0.436218\n",
      "total step : 76 \n",
      "error : 3.111771, accuarcy : 0.437719\n",
      "total step : 77 \n",
      "error : 3.097058, accuarcy : 0.438219\n",
      "total step : 78 \n",
      "error : 3.082420, accuarcy : 0.438719\n",
      "total step : 79 \n",
      "error : 3.067857, accuarcy : 0.439220\n",
      "total step : 80 \n",
      "error : 3.053368, accuarcy : 0.439720\n",
      "total step : 81 \n",
      "error : 3.038953, accuarcy : 0.442221\n",
      "total step : 82 \n",
      "error : 3.024612, accuarcy : 0.443222\n",
      "total step : 83 \n",
      "error : 3.010345, accuarcy : 0.444222\n",
      "total step : 84 \n",
      "error : 2.996151, accuarcy : 0.444722\n",
      "total step : 85 \n",
      "error : 2.982030, accuarcy : 0.446223\n",
      "total step : 86 \n",
      "error : 2.967982, accuarcy : 0.447724\n",
      "total step : 87 \n",
      "error : 2.954007, accuarcy : 0.448224\n",
      "total step : 88 \n",
      "error : 2.940104, accuarcy : 0.449225\n",
      "total step : 89 \n",
      "error : 2.926273, accuarcy : 0.451726\n",
      "total step : 90 \n",
      "error : 2.912514, accuarcy : 0.452726\n",
      "total step : 91 \n",
      "error : 2.898827, accuarcy : 0.454727\n",
      "total step : 92 \n",
      "error : 2.885211, accuarcy : 0.455728\n",
      "total step : 93 \n",
      "error : 2.871667, accuarcy : 0.457229\n",
      "total step : 94 \n",
      "error : 2.858193, accuarcy : 0.457229\n",
      "total step : 95 \n",
      "error : 2.844790, accuarcy : 0.458729\n",
      "total step : 96 \n",
      "error : 2.831458, accuarcy : 0.461231\n",
      "total step : 97 \n",
      "error : 2.818196, accuarcy : 0.461731\n",
      "total step : 98 \n",
      "error : 2.805004, accuarcy : 0.462231\n",
      "total step : 99 \n",
      "error : 2.791882, accuarcy : 0.462731\n",
      "total step : 100 \n",
      "error : 2.778829, accuarcy : 0.465733\n",
      "total step : 101 \n",
      "error : 2.765846, accuarcy : 0.466733\n",
      "total step : 102 \n",
      "error : 2.752933, accuarcy : 0.467734\n",
      "total step : 103 \n",
      "error : 2.740088, accuarcy : 0.468234\n",
      "total step : 104 \n",
      "error : 2.727313, accuarcy : 0.471236\n",
      "total step : 105 \n",
      "error : 2.714606, accuarcy : 0.474237\n",
      "total step : 106 \n",
      "error : 2.701967, accuarcy : 0.475238\n",
      "total step : 107 \n",
      "error : 2.689397, accuarcy : 0.476238\n",
      "total step : 108 \n",
      "error : 2.676895, accuarcy : 0.477739\n",
      "total step : 109 \n",
      "error : 2.664461, accuarcy : 0.478239\n",
      "total step : 110 \n",
      "error : 2.652095, accuarcy : 0.479740\n",
      "total step : 111 \n",
      "error : 2.639796, accuarcy : 0.481241\n",
      "total step : 112 \n",
      "error : 2.627565, accuarcy : 0.481741\n",
      "total step : 113 \n",
      "error : 2.615401, accuarcy : 0.482241\n",
      "total step : 114 \n",
      "error : 2.603304, accuarcy : 0.483242\n",
      "total step : 115 \n",
      "error : 2.591273, accuarcy : 0.484242\n",
      "total step : 116 \n",
      "error : 2.579310, accuarcy : 0.485243\n",
      "total step : 117 \n",
      "error : 2.567413, accuarcy : 0.485743\n",
      "total step : 118 \n",
      "error : 2.555582, accuarcy : 0.486743\n",
      "total step : 119 \n",
      "error : 2.543818, accuarcy : 0.487244\n",
      "total step : 120 \n",
      "error : 2.532119, accuarcy : 0.487244\n",
      "total step : 121 \n",
      "error : 2.520487, accuarcy : 0.487744\n",
      "total step : 122 \n",
      "error : 2.508919, accuarcy : 0.488244\n",
      "total step : 123 \n",
      "error : 2.497418, accuarcy : 0.490245\n",
      "total step : 124 \n",
      "error : 2.485982, accuarcy : 0.490245\n",
      "total step : 125 \n",
      "error : 2.474610, accuarcy : 0.491246\n",
      "total step : 126 \n",
      "error : 2.463304, accuarcy : 0.493247\n",
      "total step : 127 \n",
      "error : 2.452063, accuarcy : 0.493747\n",
      "total step : 128 \n",
      "error : 2.440886, accuarcy : 0.495248\n",
      "total step : 129 \n",
      "error : 2.429773, accuarcy : 0.498249\n",
      "total step : 130 \n",
      "error : 2.418725, accuarcy : 0.497749\n",
      "total step : 131 \n",
      "error : 2.407740, accuarcy : 0.498249\n",
      "total step : 132 \n",
      "error : 2.396820, accuarcy : 0.499750\n",
      "total step : 133 \n",
      "error : 2.385963, accuarcy : 0.501251\n",
      "total step : 134 \n",
      "error : 2.375169, accuarcy : 0.501251\n",
      "total step : 135 \n",
      "error : 2.364439, accuarcy : 0.501751\n",
      "total step : 136 \n",
      "error : 2.353772, accuarcy : 0.502751\n",
      "total step : 137 \n",
      "error : 2.343167, accuarcy : 0.503752\n",
      "total step : 138 \n",
      "error : 2.332626, accuarcy : 0.505753\n",
      "total step : 139 \n",
      "error : 2.322147, accuarcy : 0.506253\n",
      "total step : 140 \n",
      "error : 2.311729, accuarcy : 0.506753\n",
      "total step : 141 \n",
      "error : 2.301374, accuarcy : 0.508754\n",
      "total step : 142 \n",
      "error : 2.291081, accuarcy : 0.510255\n",
      "total step : 143 \n",
      "error : 2.280849, accuarcy : 0.511256\n",
      "total step : 144 \n",
      "error : 2.270679, accuarcy : 0.512256\n",
      "total step : 145 \n",
      "error : 2.260570, accuarcy : 0.514257\n",
      "total step : 146 \n",
      "error : 2.250522, accuarcy : 0.516758\n",
      "total step : 147 \n",
      "error : 2.240534, accuarcy : 0.518259\n",
      "total step : 148 \n",
      "error : 2.230607, accuarcy : 0.519260\n",
      "total step : 149 \n",
      "error : 2.220740, accuarcy : 0.519260\n",
      "total step : 150 \n",
      "error : 2.210933, accuarcy : 0.520260\n",
      "total step : 151 \n",
      "error : 2.201186, accuarcy : 0.522761\n",
      "total step : 152 \n",
      "error : 2.191498, accuarcy : 0.524762\n",
      "total step : 153 \n",
      "error : 2.181869, accuarcy : 0.526763\n",
      "total step : 154 \n",
      "error : 2.172299, accuarcy : 0.527264\n",
      "total step : 155 \n",
      "error : 2.162788, accuarcy : 0.528264\n",
      "total step : 156 \n",
      "error : 2.153336, accuarcy : 0.530765\n",
      "total step : 157 \n",
      "error : 2.143942, accuarcy : 0.531766\n",
      "total step : 158 \n",
      "error : 2.134605, accuarcy : 0.533767\n",
      "total step : 159 \n",
      "error : 2.125326, accuarcy : 0.534267\n",
      "total step : 160 \n",
      "error : 2.116105, accuarcy : 0.534767\n",
      "total step : 161 \n",
      "error : 2.106940, accuarcy : 0.538269\n",
      "total step : 162 \n",
      "error : 2.097832, accuarcy : 0.540270\n",
      "total step : 163 \n",
      "error : 2.088781, accuarcy : 0.540770\n",
      "total step : 164 \n",
      "error : 2.079786, accuarcy : 0.543272\n",
      "total step : 165 \n",
      "error : 2.070847, accuarcy : 0.545773\n",
      "total step : 166 \n",
      "error : 2.061964, accuarcy : 0.546273\n",
      "total step : 167 \n",
      "error : 2.053136, accuarcy : 0.548274\n",
      "total step : 168 \n",
      "error : 2.044363, accuarcy : 0.550275\n",
      "total step : 169 \n",
      "error : 2.035644, accuarcy : 0.550275\n",
      "total step : 170 \n",
      "error : 2.026981, accuarcy : 0.550775\n",
      "total step : 171 \n",
      "error : 2.018371, accuarcy : 0.550775\n",
      "total step : 172 \n",
      "error : 2.009815, accuarcy : 0.552776\n",
      "total step : 173 \n",
      "error : 2.001313, accuarcy : 0.552776\n",
      "total step : 174 \n",
      "error : 1.992864, accuarcy : 0.554777\n",
      "total step : 175 \n",
      "error : 1.984468, accuarcy : 0.555278\n",
      "total step : 176 \n",
      "error : 1.976125, accuarcy : 0.556278\n",
      "total step : 177 \n",
      "error : 1.967834, accuarcy : 0.557279\n",
      "total step : 178 \n",
      "error : 1.959595, accuarcy : 0.558279\n",
      "total step : 179 \n",
      "error : 1.951407, accuarcy : 0.558779\n",
      "total step : 180 \n",
      "error : 1.943271, accuarcy : 0.559280\n",
      "total step : 181 \n",
      "error : 1.935187, accuarcy : 0.560280\n",
      "total step : 182 \n",
      "error : 1.927153, accuarcy : 0.560780\n",
      "total step : 183 \n",
      "error : 1.919169, accuarcy : 0.560780\n",
      "total step : 184 \n",
      "error : 1.911236, accuarcy : 0.560280\n",
      "total step : 185 \n",
      "error : 1.903353, accuarcy : 0.561781\n",
      "total step : 186 \n",
      "error : 1.895519, accuarcy : 0.563282\n",
      "total step : 187 \n",
      "error : 1.887735, accuarcy : 0.564282\n",
      "total step : 188 \n",
      "error : 1.879999, accuarcy : 0.564782\n",
      "total step : 189 \n",
      "error : 1.872313, accuarcy : 0.565783\n",
      "total step : 190 \n",
      "error : 1.864675, accuarcy : 0.567284\n",
      "total step : 191 \n",
      "error : 1.857084, accuarcy : 0.568284\n",
      "total step : 192 \n",
      "error : 1.849542, accuarcy : 0.569285\n",
      "total step : 193 \n",
      "error : 1.842047, accuarcy : 0.570285\n",
      "total step : 194 \n",
      "error : 1.834600, accuarcy : 0.570785\n",
      "total step : 195 \n",
      "error : 1.827200, accuarcy : 0.572786\n",
      "total step : 196 \n",
      "error : 1.819846, accuarcy : 0.573787\n",
      "total step : 197 \n",
      "error : 1.812538, accuarcy : 0.574287\n",
      "total step : 198 \n",
      "error : 1.805277, accuarcy : 0.574287\n",
      "total step : 199 \n",
      "error : 1.798062, accuarcy : 0.574787\n",
      "total step : 200 \n",
      "error : 1.790892, accuarcy : 0.574787\n",
      "total step : 201 \n",
      "error : 1.783767, accuarcy : 0.576788\n",
      "total step : 202 \n",
      "error : 1.776688, accuarcy : 0.578289\n",
      "total step : 203 \n",
      "error : 1.769653, accuarcy : 0.579290\n",
      "total step : 204 \n",
      "error : 1.762663, accuarcy : 0.579790\n",
      "total step : 205 \n",
      "error : 1.755717, accuarcy : 0.579290\n",
      "total step : 206 \n",
      "error : 1.748815, accuarcy : 0.580290\n",
      "total step : 207 \n",
      "error : 1.741957, accuarcy : 0.581291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 208 \n",
      "error : 1.735142, accuarcy : 0.581791\n",
      "total step : 209 \n",
      "error : 1.728371, accuarcy : 0.583292\n",
      "total step : 210 \n",
      "error : 1.721642, accuarcy : 0.583292\n",
      "total step : 211 \n",
      "error : 1.714956, accuarcy : 0.585793\n",
      "total step : 212 \n",
      "error : 1.708313, accuarcy : 0.586293\n",
      "total step : 213 \n",
      "error : 1.701712, accuarcy : 0.586293\n",
      "total step : 214 \n",
      "error : 1.695153, accuarcy : 0.587794\n",
      "total step : 215 \n",
      "error : 1.688635, accuarcy : 0.589295\n",
      "total step : 216 \n",
      "error : 1.682159, accuarcy : 0.589295\n",
      "total step : 217 \n",
      "error : 1.675725, accuarcy : 0.589295\n",
      "total step : 218 \n",
      "error : 1.669331, accuarcy : 0.590295\n",
      "total step : 219 \n",
      "error : 1.662978, accuarcy : 0.590795\n",
      "total step : 220 \n",
      "error : 1.656666, accuarcy : 0.590795\n",
      "total step : 221 \n",
      "error : 1.650395, accuarcy : 0.592296\n",
      "total step : 222 \n",
      "error : 1.644163, accuarcy : 0.593297\n",
      "total step : 223 \n",
      "error : 1.637971, accuarcy : 0.595298\n",
      "total step : 224 \n",
      "error : 1.631819, accuarcy : 0.596798\n",
      "total step : 225 \n",
      "error : 1.625706, accuarcy : 0.596798\n",
      "total step : 226 \n",
      "error : 1.619633, accuarcy : 0.597799\n",
      "total step : 227 \n",
      "error : 1.613599, accuarcy : 0.598299\n",
      "total step : 228 \n",
      "error : 1.607603, accuarcy : 0.599800\n",
      "total step : 229 \n",
      "error : 1.601646, accuarcy : 0.599800\n",
      "total step : 230 \n",
      "error : 1.595727, accuarcy : 0.600800\n",
      "total step : 231 \n",
      "error : 1.589847, accuarcy : 0.601801\n",
      "total step : 232 \n",
      "error : 1.584004, accuarcy : 0.602801\n",
      "total step : 233 \n",
      "error : 1.578199, accuarcy : 0.603802\n",
      "total step : 234 \n",
      "error : 1.572432, accuarcy : 0.605303\n",
      "total step : 235 \n",
      "error : 1.566702, accuarcy : 0.605803\n",
      "total step : 236 \n",
      "error : 1.561009, accuarcy : 0.606303\n",
      "total step : 237 \n",
      "error : 1.555352, accuarcy : 0.607304\n",
      "total step : 238 \n",
      "error : 1.549733, accuarcy : 0.607804\n",
      "total step : 239 \n",
      "error : 1.544150, accuarcy : 0.608804\n",
      "total step : 240 \n",
      "error : 1.538603, accuarcy : 0.609305\n",
      "total step : 241 \n",
      "error : 1.533092, accuarcy : 0.611306\n",
      "total step : 242 \n",
      "error : 1.527617, accuarcy : 0.612306\n",
      "total step : 243 \n",
      "error : 1.522177, accuarcy : 0.612306\n",
      "total step : 244 \n",
      "error : 1.516773, accuarcy : 0.612806\n",
      "total step : 245 \n",
      "error : 1.511404, accuarcy : 0.613807\n",
      "total step : 246 \n",
      "error : 1.506070, accuarcy : 0.614807\n",
      "total step : 247 \n",
      "error : 1.500770, accuarcy : 0.615808\n",
      "total step : 248 \n",
      "error : 1.495506, accuarcy : 0.616308\n",
      "total step : 249 \n",
      "error : 1.490275, accuarcy : 0.617309\n",
      "total step : 250 \n",
      "error : 1.485079, accuarcy : 0.617309\n",
      "total step : 251 \n",
      "error : 1.479916, accuarcy : 0.619310\n",
      "total step : 252 \n",
      "error : 1.474787, accuarcy : 0.619810\n",
      "total step : 253 \n",
      "error : 1.469692, accuarcy : 0.620310\n",
      "total step : 254 \n",
      "error : 1.464630, accuarcy : 0.621311\n",
      "total step : 255 \n",
      "error : 1.459601, accuarcy : 0.622811\n",
      "total step : 256 \n",
      "error : 1.454604, accuarcy : 0.624812\n",
      "total step : 257 \n",
      "error : 1.449641, accuarcy : 0.626313\n",
      "total step : 258 \n",
      "error : 1.444710, accuarcy : 0.627314\n",
      "total step : 259 \n",
      "error : 1.439811, accuarcy : 0.627814\n",
      "total step : 260 \n",
      "error : 1.434944, accuarcy : 0.628814\n",
      "total step : 261 \n",
      "error : 1.430108, accuarcy : 0.629815\n",
      "total step : 262 \n",
      "error : 1.425305, accuarcy : 0.630815\n",
      "total step : 263 \n",
      "error : 1.420533, accuarcy : 0.632316\n",
      "total step : 264 \n",
      "error : 1.415791, accuarcy : 0.632816\n",
      "total step : 265 \n",
      "error : 1.411081, accuarcy : 0.633317\n",
      "total step : 266 \n",
      "error : 1.406402, accuarcy : 0.632816\n",
      "total step : 267 \n",
      "error : 1.401753, accuarcy : 0.633317\n",
      "total step : 268 \n",
      "error : 1.397134, accuarcy : 0.635318\n",
      "total step : 269 \n",
      "error : 1.392546, accuarcy : 0.637819\n",
      "total step : 270 \n",
      "error : 1.387987, accuarcy : 0.638319\n",
      "total step : 271 \n",
      "error : 1.383458, accuarcy : 0.638819\n",
      "total step : 272 \n",
      "error : 1.378959, accuarcy : 0.639820\n",
      "total step : 273 \n",
      "error : 1.374488, accuarcy : 0.640820\n",
      "total step : 274 \n",
      "error : 1.370047, accuarcy : 0.641321\n",
      "total step : 275 \n",
      "error : 1.365635, accuarcy : 0.641821\n",
      "total step : 276 \n",
      "error : 1.361251, accuarcy : 0.642821\n",
      "total step : 277 \n",
      "error : 1.356896, accuarcy : 0.645323\n",
      "total step : 278 \n",
      "error : 1.352569, accuarcy : 0.645323\n",
      "total step : 279 \n",
      "error : 1.348270, accuarcy : 0.645823\n",
      "total step : 280 \n",
      "error : 1.343998, accuarcy : 0.647824\n",
      "total step : 281 \n",
      "error : 1.339755, accuarcy : 0.648324\n",
      "total step : 282 \n",
      "error : 1.335538, accuarcy : 0.650325\n",
      "total step : 283 \n",
      "error : 1.331349, accuarcy : 0.651326\n",
      "total step : 284 \n",
      "error : 1.327187, accuarcy : 0.651826\n",
      "total step : 285 \n",
      "error : 1.323052, accuarcy : 0.652826\n",
      "total step : 286 \n",
      "error : 1.318943, accuarcy : 0.653327\n",
      "total step : 287 \n",
      "error : 1.314861, accuarcy : 0.653827\n",
      "total step : 288 \n",
      "error : 1.310805, accuarcy : 0.654827\n",
      "total step : 289 \n",
      "error : 1.306774, accuarcy : 0.656328\n",
      "total step : 290 \n",
      "error : 1.302770, accuarcy : 0.656328\n",
      "total step : 291 \n",
      "error : 1.298791, accuarcy : 0.657329\n",
      "total step : 292 \n",
      "error : 1.294838, accuarcy : 0.658329\n",
      "total step : 293 \n",
      "error : 1.290909, accuarcy : 0.659330\n",
      "total step : 294 \n",
      "error : 1.287006, accuarcy : 0.660330\n",
      "total step : 295 \n",
      "error : 1.283127, accuarcy : 0.660330\n",
      "total step : 296 \n",
      "error : 1.279273, accuarcy : 0.661331\n",
      "total step : 297 \n",
      "error : 1.275444, accuarcy : 0.662831\n",
      "total step : 298 \n",
      "error : 1.271638, accuarcy : 0.663332\n",
      "total step : 299 \n",
      "error : 1.267857, accuarcy : 0.663332\n",
      "total step : 300 \n",
      "error : 1.264099, accuarcy : 0.663332\n",
      "total step : 301 \n",
      "error : 1.260366, accuarcy : 0.663832\n",
      "total step : 302 \n",
      "error : 1.256655, accuarcy : 0.664332\n",
      "total step : 303 \n",
      "error : 1.252968, accuarcy : 0.665333\n",
      "total step : 304 \n",
      "error : 1.249304, accuarcy : 0.665333\n",
      "total step : 305 \n",
      "error : 1.245662, accuarcy : 0.665833\n",
      "total step : 306 \n",
      "error : 1.242044, accuarcy : 0.667334\n",
      "total step : 307 \n",
      "error : 1.238448, accuarcy : 0.667834\n",
      "total step : 308 \n",
      "error : 1.234874, accuarcy : 0.669335\n",
      "total step : 309 \n",
      "error : 1.231322, accuarcy : 0.670335\n",
      "total step : 310 \n",
      "error : 1.227793, accuarcy : 0.672336\n",
      "total step : 311 \n",
      "error : 1.224285, accuarcy : 0.672836\n",
      "total step : 312 \n",
      "error : 1.220799, accuarcy : 0.672836\n",
      "total step : 313 \n",
      "error : 1.217334, accuarcy : 0.672836\n",
      "total step : 314 \n",
      "error : 1.213891, accuarcy : 0.674337\n",
      "total step : 315 \n",
      "error : 1.210468, accuarcy : 0.674837\n",
      "total step : 316 \n",
      "error : 1.207067, accuarcy : 0.675838\n",
      "total step : 317 \n",
      "error : 1.203686, accuarcy : 0.676838\n",
      "total step : 318 \n",
      "error : 1.200326, accuarcy : 0.677339\n",
      "total step : 319 \n",
      "error : 1.196986, accuarcy : 0.677839\n",
      "total step : 320 \n",
      "error : 1.193667, accuarcy : 0.679340\n",
      "total step : 321 \n",
      "error : 1.190367, accuarcy : 0.679340\n",
      "total step : 322 \n",
      "error : 1.187088, accuarcy : 0.681341\n",
      "total step : 323 \n",
      "error : 1.183828, accuarcy : 0.683342\n",
      "total step : 324 \n",
      "error : 1.180588, accuarcy : 0.683842\n",
      "total step : 325 \n",
      "error : 1.177368, accuarcy : 0.684842\n",
      "total step : 326 \n",
      "error : 1.174166, accuarcy : 0.685343\n",
      "total step : 327 \n",
      "error : 1.170984, accuarcy : 0.686343\n",
      "total step : 328 \n",
      "error : 1.167821, accuarcy : 0.687344\n",
      "total step : 329 \n",
      "error : 1.164676, accuarcy : 0.687844\n",
      "total step : 330 \n",
      "error : 1.161550, accuarcy : 0.687844\n",
      "total step : 331 \n",
      "error : 1.158443, accuarcy : 0.689345\n",
      "total step : 332 \n",
      "error : 1.155354, accuarcy : 0.689845\n",
      "total step : 333 \n",
      "error : 1.152283, accuarcy : 0.690845\n",
      "total step : 334 \n",
      "error : 1.149230, accuarcy : 0.691846\n",
      "total step : 335 \n",
      "error : 1.146196, accuarcy : 0.691846\n",
      "total step : 336 \n",
      "error : 1.143178, accuarcy : 0.692846\n",
      "total step : 337 \n",
      "error : 1.140179, accuarcy : 0.693847\n",
      "total step : 338 \n",
      "error : 1.137197, accuarcy : 0.694347\n",
      "total step : 339 \n",
      "error : 1.134232, accuarcy : 0.694847\n",
      "total step : 340 \n",
      "error : 1.131285, accuarcy : 0.694847\n",
      "total step : 341 \n",
      "error : 1.128354, accuarcy : 0.694847\n",
      "total step : 342 \n",
      "error : 1.125441, accuarcy : 0.695348\n",
      "total step : 343 \n",
      "error : 1.122544, accuarcy : 0.695848\n",
      "total step : 344 \n",
      "error : 1.119664, accuarcy : 0.696848\n",
      "total step : 345 \n",
      "error : 1.116800, accuarcy : 0.696848\n",
      "total step : 346 \n",
      "error : 1.113953, accuarcy : 0.697849\n",
      "total step : 347 \n",
      "error : 1.111122, accuarcy : 0.698849\n",
      "total step : 348 \n",
      "error : 1.108307, accuarcy : 0.699350\n",
      "total step : 349 \n",
      "error : 1.105508, accuarcy : 0.699350\n",
      "total step : 350 \n",
      "error : 1.102725, accuarcy : 0.699350\n",
      "total step : 351 \n",
      "error : 1.099958, accuarcy : 0.699350\n",
      "total step : 352 \n",
      "error : 1.097206, accuarcy : 0.699850\n",
      "total step : 353 \n",
      "error : 1.094469, accuarcy : 0.699850\n",
      "total step : 354 \n",
      "error : 1.091748, accuarcy : 0.699850\n",
      "total step : 355 \n",
      "error : 1.089043, accuarcy : 0.701351\n",
      "total step : 356 \n",
      "error : 1.086352, accuarcy : 0.701351\n",
      "total step : 357 \n",
      "error : 1.083676, accuarcy : 0.701351\n",
      "total step : 358 \n",
      "error : 1.081015, accuarcy : 0.702851\n",
      "total step : 359 \n",
      "error : 1.078369, accuarcy : 0.702851\n",
      "total step : 360 \n",
      "error : 1.075738, accuarcy : 0.704852\n",
      "total step : 361 \n",
      "error : 1.073121, accuarcy : 0.706353\n",
      "total step : 362 \n",
      "error : 1.070518, accuarcy : 0.707354\n",
      "total step : 363 \n",
      "error : 1.067930, accuarcy : 0.707854\n",
      "total step : 364 \n",
      "error : 1.065356, accuarcy : 0.709855\n",
      "total step : 365 \n",
      "error : 1.062796, accuarcy : 0.710355\n",
      "total step : 366 \n",
      "error : 1.060250, accuarcy : 0.710855\n",
      "total step : 367 \n",
      "error : 1.057717, accuarcy : 0.710855\n",
      "total step : 368 \n",
      "error : 1.055199, accuarcy : 0.710855\n",
      "total step : 369 \n",
      "error : 1.052694, accuarcy : 0.710855\n",
      "total step : 370 \n",
      "error : 1.050202, accuarcy : 0.710855\n",
      "total step : 371 \n",
      "error : 1.047724, accuarcy : 0.710855\n",
      "total step : 372 \n",
      "error : 1.045260, accuarcy : 0.711356\n",
      "total step : 373 \n",
      "error : 1.042808, accuarcy : 0.711356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 374 \n",
      "error : 1.040370, accuarcy : 0.711356\n",
      "total step : 375 \n",
      "error : 1.037944, accuarcy : 0.711356\n",
      "total step : 376 \n",
      "error : 1.035532, accuarcy : 0.712356\n",
      "total step : 377 \n",
      "error : 1.033132, accuarcy : 0.712856\n",
      "total step : 378 \n",
      "error : 1.030745, accuarcy : 0.712856\n",
      "total step : 379 \n",
      "error : 1.028371, accuarcy : 0.712856\n",
      "total step : 380 \n",
      "error : 1.026009, accuarcy : 0.713357\n",
      "total step : 381 \n",
      "error : 1.023659, accuarcy : 0.713857\n",
      "total step : 382 \n",
      "error : 1.021322, accuarcy : 0.714857\n",
      "total step : 383 \n",
      "error : 1.018997, accuarcy : 0.714857\n",
      "total step : 384 \n",
      "error : 1.016684, accuarcy : 0.715358\n",
      "total step : 385 \n",
      "error : 1.014384, accuarcy : 0.716358\n",
      "total step : 386 \n",
      "error : 1.012095, accuarcy : 0.717359\n",
      "total step : 387 \n",
      "error : 1.009818, accuarcy : 0.717859\n",
      "total step : 388 \n",
      "error : 1.007553, accuarcy : 0.718359\n",
      "total step : 389 \n",
      "error : 1.005300, accuarcy : 0.718359\n",
      "total step : 390 \n",
      "error : 1.003058, accuarcy : 0.719360\n",
      "total step : 391 \n",
      "error : 1.000827, accuarcy : 0.720360\n",
      "total step : 392 \n",
      "error : 0.998609, accuarcy : 0.720860\n",
      "total step : 393 \n",
      "error : 0.996401, accuarcy : 0.721361\n",
      "total step : 394 \n",
      "error : 0.994205, accuarcy : 0.721861\n",
      "total step : 395 \n",
      "error : 0.992020, accuarcy : 0.722361\n",
      "total step : 396 \n",
      "error : 0.989846, accuarcy : 0.722861\n",
      "total step : 397 \n",
      "error : 0.987683, accuarcy : 0.723362\n",
      "total step : 398 \n",
      "error : 0.985531, accuarcy : 0.724362\n",
      "total step : 399 \n",
      "error : 0.983390, accuarcy : 0.724862\n",
      "total step : 400 \n",
      "error : 0.981259, accuarcy : 0.724862\n",
      "total step : 401 \n",
      "error : 0.979140, accuarcy : 0.725863\n",
      "total step : 402 \n",
      "error : 0.977031, accuarcy : 0.725863\n",
      "total step : 403 \n",
      "error : 0.974932, accuarcy : 0.725863\n",
      "total step : 404 \n",
      "error : 0.972844, accuarcy : 0.726863\n",
      "total step : 405 \n",
      "error : 0.970767, accuarcy : 0.726863\n",
      "total step : 406 \n",
      "error : 0.968700, accuarcy : 0.726863\n",
      "total step : 407 \n",
      "error : 0.966643, accuarcy : 0.727864\n",
      "total step : 408 \n",
      "error : 0.964596, accuarcy : 0.727864\n",
      "total step : 409 \n",
      "error : 0.962559, accuarcy : 0.728864\n",
      "total step : 410 \n",
      "error : 0.960532, accuarcy : 0.728864\n",
      "total step : 411 \n",
      "error : 0.958516, accuarcy : 0.728864\n",
      "total step : 412 \n",
      "error : 0.956509, accuarcy : 0.729365\n",
      "total step : 413 \n",
      "error : 0.954512, accuarcy : 0.729365\n",
      "total step : 414 \n",
      "error : 0.952525, accuarcy : 0.729365\n",
      "total step : 415 \n",
      "error : 0.950547, accuarcy : 0.729865\n",
      "total step : 416 \n",
      "error : 0.948580, accuarcy : 0.730865\n",
      "total step : 417 \n",
      "error : 0.946621, accuarcy : 0.731366\n",
      "total step : 418 \n",
      "error : 0.944673, accuarcy : 0.731866\n",
      "total step : 419 \n",
      "error : 0.942733, accuarcy : 0.731866\n",
      "total step : 420 \n",
      "error : 0.940803, accuarcy : 0.732366\n",
      "total step : 421 \n",
      "error : 0.938883, accuarcy : 0.732366\n",
      "total step : 422 \n",
      "error : 0.936971, accuarcy : 0.732366\n",
      "total step : 423 \n",
      "error : 0.935069, accuarcy : 0.732866\n",
      "total step : 424 \n",
      "error : 0.933176, accuarcy : 0.733867\n",
      "total step : 425 \n",
      "error : 0.931292, accuarcy : 0.735368\n",
      "total step : 426 \n",
      "error : 0.929417, accuarcy : 0.735368\n",
      "total step : 427 \n",
      "error : 0.927550, accuarcy : 0.735368\n",
      "total step : 428 \n",
      "error : 0.925693, accuarcy : 0.735368\n",
      "total step : 429 \n",
      "error : 0.923845, accuarcy : 0.735868\n",
      "total step : 430 \n",
      "error : 0.922005, accuarcy : 0.735868\n",
      "total step : 431 \n",
      "error : 0.920174, accuarcy : 0.736368\n",
      "total step : 432 \n",
      "error : 0.918352, accuarcy : 0.736868\n",
      "total step : 433 \n",
      "error : 0.916538, accuarcy : 0.737369\n",
      "total step : 434 \n",
      "error : 0.914732, accuarcy : 0.737369\n",
      "total step : 435 \n",
      "error : 0.912936, accuarcy : 0.737869\n",
      "total step : 436 \n",
      "error : 0.911147, accuarcy : 0.738369\n",
      "total step : 437 \n",
      "error : 0.909367, accuarcy : 0.738869\n",
      "total step : 438 \n",
      "error : 0.907596, accuarcy : 0.738869\n",
      "total step : 439 \n",
      "error : 0.905832, accuarcy : 0.740370\n",
      "total step : 440 \n",
      "error : 0.904077, accuarcy : 0.740870\n",
      "total step : 441 \n",
      "error : 0.902330, accuarcy : 0.741371\n",
      "total step : 442 \n",
      "error : 0.900590, accuarcy : 0.742371\n",
      "total step : 443 \n",
      "error : 0.898859, accuarcy : 0.742871\n",
      "total step : 444 \n",
      "error : 0.897136, accuarcy : 0.742871\n",
      "total step : 445 \n",
      "error : 0.895421, accuarcy : 0.742871\n",
      "total step : 446 \n",
      "error : 0.893714, accuarcy : 0.743372\n",
      "total step : 447 \n",
      "error : 0.892015, accuarcy : 0.743872\n",
      "total step : 448 \n",
      "error : 0.890323, accuarcy : 0.744372\n",
      "total step : 449 \n",
      "error : 0.888639, accuarcy : 0.744372\n",
      "total step : 450 \n",
      "error : 0.886963, accuarcy : 0.745373\n",
      "total step : 451 \n",
      "error : 0.885294, accuarcy : 0.746373\n",
      "total step : 452 \n",
      "error : 0.883633, accuarcy : 0.746873\n",
      "total step : 453 \n",
      "error : 0.881980, accuarcy : 0.746873\n",
      "total step : 454 \n",
      "error : 0.880334, accuarcy : 0.747374\n",
      "total step : 455 \n",
      "error : 0.878695, accuarcy : 0.747374\n",
      "total step : 456 \n",
      "error : 0.877064, accuarcy : 0.747874\n",
      "total step : 457 \n",
      "error : 0.875440, accuarcy : 0.748374\n",
      "total step : 458 \n",
      "error : 0.873824, accuarcy : 0.748874\n",
      "total step : 459 \n",
      "error : 0.872214, accuarcy : 0.750375\n",
      "total step : 460 \n",
      "error : 0.870612, accuarcy : 0.750375\n",
      "total step : 461 \n",
      "error : 0.869017, accuarcy : 0.750875\n",
      "total step : 462 \n",
      "error : 0.867429, accuarcy : 0.751376\n",
      "total step : 463 \n",
      "error : 0.865849, accuarcy : 0.751876\n",
      "total step : 464 \n",
      "error : 0.864275, accuarcy : 0.752376\n",
      "total step : 465 \n",
      "error : 0.862708, accuarcy : 0.752376\n",
      "total step : 466 \n",
      "error : 0.861148, accuarcy : 0.752376\n",
      "total step : 467 \n",
      "error : 0.859595, accuarcy : 0.753877\n",
      "total step : 468 \n",
      "error : 0.858049, accuarcy : 0.754377\n",
      "total step : 469 \n",
      "error : 0.856510, accuarcy : 0.754377\n",
      "total step : 470 \n",
      "error : 0.854978, accuarcy : 0.754377\n",
      "total step : 471 \n",
      "error : 0.853452, accuarcy : 0.754377\n",
      "total step : 472 \n",
      "error : 0.851933, accuarcy : 0.755378\n",
      "total step : 473 \n",
      "error : 0.850420, accuarcy : 0.755378\n",
      "total step : 474 \n",
      "error : 0.848915, accuarcy : 0.755878\n",
      "total step : 475 \n",
      "error : 0.847415, accuarcy : 0.755878\n",
      "total step : 476 \n",
      "error : 0.845922, accuarcy : 0.755878\n",
      "total step : 477 \n",
      "error : 0.844436, accuarcy : 0.755878\n",
      "total step : 478 \n",
      "error : 0.842956, accuarcy : 0.755878\n",
      "total step : 479 \n",
      "error : 0.841483, accuarcy : 0.756378\n",
      "total step : 480 \n",
      "error : 0.840016, accuarcy : 0.755878\n",
      "total step : 481 \n",
      "error : 0.838555, accuarcy : 0.755878\n",
      "total step : 482 \n",
      "error : 0.837101, accuarcy : 0.756378\n",
      "total step : 483 \n",
      "error : 0.835652, accuarcy : 0.756378\n",
      "total step : 484 \n",
      "error : 0.834210, accuarcy : 0.755878\n",
      "total step : 485 \n",
      "error : 0.832774, accuarcy : 0.755878\n",
      "total step : 486 \n",
      "error : 0.831345, accuarcy : 0.756378\n",
      "total step : 487 \n",
      "error : 0.829921, accuarcy : 0.756378\n",
      "total step : 488 \n",
      "error : 0.828503, accuarcy : 0.757879\n",
      "total step : 489 \n",
      "error : 0.827092, accuarcy : 0.757379\n",
      "total step : 490 \n",
      "error : 0.825686, accuarcy : 0.757379\n",
      "total step : 491 \n",
      "error : 0.824286, accuarcy : 0.757379\n",
      "total step : 492 \n",
      "error : 0.822893, accuarcy : 0.757379\n",
      "total step : 493 \n",
      "error : 0.821505, accuarcy : 0.758379\n",
      "total step : 494 \n",
      "error : 0.820123, accuarcy : 0.758879\n",
      "total step : 495 \n",
      "error : 0.818747, accuarcy : 0.759380\n",
      "total step : 496 \n",
      "error : 0.817376, accuarcy : 0.759380\n",
      "total step : 497 \n",
      "error : 0.816011, accuarcy : 0.759380\n",
      "total step : 498 \n",
      "error : 0.814652, accuarcy : 0.759880\n",
      "total step : 499 \n",
      "error : 0.813299, accuarcy : 0.760380\n",
      "total step : 500 \n",
      "error : 0.811951, accuarcy : 0.759880\n",
      "total step : 501 \n",
      "error : 0.810609, accuarcy : 0.759880\n",
      "total step : 502 \n",
      "error : 0.809272, accuarcy : 0.759880\n",
      "total step : 503 \n",
      "error : 0.807941, accuarcy : 0.760380\n",
      "total step : 504 \n",
      "error : 0.806616, accuarcy : 0.760380\n",
      "total step : 505 \n",
      "error : 0.805296, accuarcy : 0.760380\n",
      "total step : 506 \n",
      "error : 0.803981, accuarcy : 0.760880\n",
      "total step : 507 \n",
      "error : 0.802672, accuarcy : 0.760880\n",
      "total step : 508 \n",
      "error : 0.801368, accuarcy : 0.760880\n",
      "total step : 509 \n",
      "error : 0.800069, accuarcy : 0.761381\n",
      "total step : 510 \n",
      "error : 0.798776, accuarcy : 0.761881\n",
      "total step : 511 \n",
      "error : 0.797488, accuarcy : 0.762881\n",
      "total step : 512 \n",
      "error : 0.796205, accuarcy : 0.763882\n",
      "total step : 513 \n",
      "error : 0.794927, accuarcy : 0.763882\n",
      "total step : 514 \n",
      "error : 0.793655, accuarcy : 0.763882\n",
      "total step : 515 \n",
      "error : 0.792388, accuarcy : 0.763882\n",
      "total step : 516 \n",
      "error : 0.791126, accuarcy : 0.763882\n",
      "total step : 517 \n",
      "error : 0.789869, accuarcy : 0.764882\n",
      "total step : 518 \n",
      "error : 0.788617, accuarcy : 0.764882\n",
      "total step : 519 \n",
      "error : 0.787370, accuarcy : 0.764882\n",
      "total step : 520 \n",
      "error : 0.786128, accuarcy : 0.766383\n",
      "total step : 521 \n",
      "error : 0.784891, accuarcy : 0.766383\n",
      "total step : 522 \n",
      "error : 0.783659, accuarcy : 0.766883\n",
      "total step : 523 \n",
      "error : 0.782431, accuarcy : 0.766883\n",
      "total step : 524 \n",
      "error : 0.781209, accuarcy : 0.767384\n",
      "total step : 525 \n",
      "error : 0.779992, accuarcy : 0.767384\n",
      "total step : 526 \n",
      "error : 0.778779, accuarcy : 0.767384\n",
      "total step : 527 \n",
      "error : 0.777571, accuarcy : 0.767884\n",
      "total step : 528 \n",
      "error : 0.776368, accuarcy : 0.767884\n",
      "total step : 529 \n",
      "error : 0.775170, accuarcy : 0.767884\n",
      "total step : 530 \n",
      "error : 0.773976, accuarcy : 0.768384\n",
      "total step : 531 \n",
      "error : 0.772787, accuarcy : 0.768384\n",
      "total step : 532 \n",
      "error : 0.771603, accuarcy : 0.768384\n",
      "total step : 533 \n",
      "error : 0.770423, accuarcy : 0.768884\n",
      "total step : 534 \n",
      "error : 0.769248, accuarcy : 0.769385\n",
      "total step : 535 \n",
      "error : 0.768078, accuarcy : 0.769385\n",
      "total step : 536 \n",
      "error : 0.766912, accuarcy : 0.769385\n",
      "total step : 537 \n",
      "error : 0.765751, accuarcy : 0.769385\n",
      "total step : 538 \n",
      "error : 0.764594, accuarcy : 0.769885\n",
      "total step : 539 \n",
      "error : 0.763441, accuarcy : 0.770385\n",
      "total step : 540 \n",
      "error : 0.762293, accuarcy : 0.770885\n",
      "total step : 541 \n",
      "error : 0.761150, accuarcy : 0.771386\n",
      "total step : 542 \n",
      "error : 0.760011, accuarcy : 0.771886\n",
      "total step : 543 \n",
      "error : 0.758876, accuarcy : 0.771886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 544 \n",
      "error : 0.757745, accuarcy : 0.772386\n",
      "total step : 545 \n",
      "error : 0.756619, accuarcy : 0.772386\n",
      "total step : 546 \n",
      "error : 0.755497, accuarcy : 0.772386\n",
      "total step : 547 \n",
      "error : 0.754380, accuarcy : 0.772886\n",
      "total step : 548 \n",
      "error : 0.753266, accuarcy : 0.773387\n",
      "total step : 549 \n",
      "error : 0.752157, accuarcy : 0.773387\n",
      "total step : 550 \n",
      "error : 0.751052, accuarcy : 0.773887\n",
      "total step : 551 \n",
      "error : 0.749952, accuarcy : 0.774387\n",
      "total step : 552 \n",
      "error : 0.748855, accuarcy : 0.776388\n",
      "total step : 553 \n",
      "error : 0.747763, accuarcy : 0.776388\n",
      "total step : 554 \n",
      "error : 0.746674, accuarcy : 0.776388\n",
      "total step : 555 \n",
      "error : 0.745590, accuarcy : 0.776388\n",
      "total step : 556 \n",
      "error : 0.744510, accuarcy : 0.776388\n",
      "total step : 557 \n",
      "error : 0.743433, accuarcy : 0.776388\n",
      "total step : 558 \n",
      "error : 0.742361, accuarcy : 0.776888\n",
      "total step : 559 \n",
      "error : 0.741293, accuarcy : 0.776888\n",
      "total step : 560 \n",
      "error : 0.740229, accuarcy : 0.776888\n",
      "total step : 561 \n",
      "error : 0.739169, accuarcy : 0.776888\n",
      "total step : 562 \n",
      "error : 0.738112, accuarcy : 0.776888\n",
      "total step : 563 \n",
      "error : 0.737060, accuarcy : 0.777389\n",
      "total step : 564 \n",
      "error : 0.736011, accuarcy : 0.777389\n",
      "total step : 565 \n",
      "error : 0.734966, accuarcy : 0.777389\n",
      "total step : 566 \n",
      "error : 0.733925, accuarcy : 0.777389\n",
      "total step : 567 \n",
      "error : 0.732888, accuarcy : 0.777389\n",
      "total step : 568 \n",
      "error : 0.731855, accuarcy : 0.777389\n",
      "total step : 569 \n",
      "error : 0.730826, accuarcy : 0.777389\n",
      "total step : 570 \n",
      "error : 0.729800, accuarcy : 0.777389\n",
      "total step : 571 \n",
      "error : 0.728778, accuarcy : 0.777389\n",
      "total step : 572 \n",
      "error : 0.727759, accuarcy : 0.777889\n",
      "total step : 573 \n",
      "error : 0.726745, accuarcy : 0.777889\n",
      "total step : 574 \n",
      "error : 0.725734, accuarcy : 0.777889\n",
      "total step : 575 \n",
      "error : 0.724727, accuarcy : 0.778889\n",
      "total step : 576 \n",
      "error : 0.723723, accuarcy : 0.779890\n",
      "total step : 577 \n",
      "error : 0.722723, accuarcy : 0.779890\n",
      "total step : 578 \n",
      "error : 0.721726, accuarcy : 0.779890\n",
      "total step : 579 \n",
      "error : 0.720733, accuarcy : 0.779890\n",
      "total step : 580 \n",
      "error : 0.719744, accuarcy : 0.780390\n",
      "total step : 581 \n",
      "error : 0.718758, accuarcy : 0.780390\n",
      "total step : 582 \n",
      "error : 0.717776, accuarcy : 0.780390\n",
      "total step : 583 \n",
      "error : 0.716797, accuarcy : 0.780390\n",
      "total step : 584 \n",
      "error : 0.715822, accuarcy : 0.780390\n",
      "total step : 585 \n",
      "error : 0.714850, accuarcy : 0.780390\n",
      "total step : 586 \n",
      "error : 0.713882, accuarcy : 0.780390\n",
      "total step : 587 \n",
      "error : 0.712917, accuarcy : 0.780390\n",
      "total step : 588 \n",
      "error : 0.711955, accuarcy : 0.780390\n",
      "total step : 589 \n",
      "error : 0.710997, accuarcy : 0.780390\n",
      "total step : 590 \n",
      "error : 0.710042, accuarcy : 0.780390\n",
      "total step : 591 \n",
      "error : 0.709090, accuarcy : 0.780390\n",
      "total step : 592 \n",
      "error : 0.708142, accuarcy : 0.780390\n",
      "total step : 593 \n",
      "error : 0.707197, accuarcy : 0.781391\n",
      "total step : 594 \n",
      "error : 0.706256, accuarcy : 0.781891\n",
      "total step : 595 \n",
      "error : 0.705317, accuarcy : 0.781891\n",
      "total step : 596 \n",
      "error : 0.704382, accuarcy : 0.782391\n",
      "total step : 597 \n",
      "error : 0.703451, accuarcy : 0.782391\n",
      "total step : 598 \n",
      "error : 0.702522, accuarcy : 0.782391\n",
      "total step : 599 \n",
      "error : 0.701597, accuarcy : 0.782391\n",
      "total step : 600 \n",
      "error : 0.700674, accuarcy : 0.782891\n",
      "total step : 601 \n",
      "error : 0.699755, accuarcy : 0.782891\n",
      "total step : 602 \n",
      "error : 0.698840, accuarcy : 0.782891\n",
      "total step : 603 \n",
      "error : 0.697927, accuarcy : 0.782891\n",
      "total step : 604 \n",
      "error : 0.697017, accuarcy : 0.782891\n",
      "total step : 605 \n",
      "error : 0.696111, accuarcy : 0.783392\n",
      "total step : 606 \n",
      "error : 0.695208, accuarcy : 0.783392\n",
      "total step : 607 \n",
      "error : 0.694307, accuarcy : 0.783392\n",
      "total step : 608 \n",
      "error : 0.693410, accuarcy : 0.783392\n",
      "total step : 609 \n",
      "error : 0.692516, accuarcy : 0.783892\n",
      "total step : 610 \n",
      "error : 0.691625, accuarcy : 0.783892\n",
      "total step : 611 \n",
      "error : 0.690737, accuarcy : 0.783892\n",
      "total step : 612 \n",
      "error : 0.689852, accuarcy : 0.784392\n",
      "total step : 613 \n",
      "error : 0.688969, accuarcy : 0.783892\n",
      "total step : 614 \n",
      "error : 0.688090, accuarcy : 0.783892\n",
      "total step : 615 \n",
      "error : 0.687214, accuarcy : 0.784392\n",
      "total step : 616 \n",
      "error : 0.686341, accuarcy : 0.785393\n",
      "total step : 617 \n",
      "error : 0.685470, accuarcy : 0.785393\n",
      "total step : 618 \n",
      "error : 0.684603, accuarcy : 0.785393\n",
      "total step : 619 \n",
      "error : 0.683739, accuarcy : 0.785893\n",
      "total step : 620 \n",
      "error : 0.682877, accuarcy : 0.786393\n",
      "total step : 621 \n",
      "error : 0.682018, accuarcy : 0.787394\n",
      "total step : 622 \n",
      "error : 0.681162, accuarcy : 0.787894\n",
      "total step : 623 \n",
      "error : 0.680309, accuarcy : 0.788394\n",
      "total step : 624 \n",
      "error : 0.679459, accuarcy : 0.788894\n",
      "total step : 625 \n",
      "error : 0.678611, accuarcy : 0.788894\n",
      "total step : 626 \n",
      "error : 0.677767, accuarcy : 0.788894\n",
      "total step : 627 \n",
      "error : 0.676925, accuarcy : 0.788894\n",
      "total step : 628 \n",
      "error : 0.676086, accuarcy : 0.789395\n",
      "total step : 629 \n",
      "error : 0.675249, accuarcy : 0.789895\n",
      "total step : 630 \n",
      "error : 0.674416, accuarcy : 0.789895\n",
      "total step : 631 \n",
      "error : 0.673585, accuarcy : 0.789895\n",
      "total step : 632 \n",
      "error : 0.672757, accuarcy : 0.789895\n",
      "total step : 633 \n",
      "error : 0.671931, accuarcy : 0.789895\n",
      "total step : 634 \n",
      "error : 0.671108, accuarcy : 0.789895\n",
      "total step : 635 \n",
      "error : 0.670288, accuarcy : 0.789895\n",
      "total step : 636 \n",
      "error : 0.669471, accuarcy : 0.790395\n",
      "total step : 637 \n",
      "error : 0.668656, accuarcy : 0.790395\n",
      "total step : 638 \n",
      "error : 0.667844, accuarcy : 0.790395\n",
      "total step : 639 \n",
      "error : 0.667034, accuarcy : 0.790395\n",
      "total step : 640 \n",
      "error : 0.666228, accuarcy : 0.790895\n",
      "total step : 641 \n",
      "error : 0.665423, accuarcy : 0.790895\n",
      "total step : 642 \n",
      "error : 0.664621, accuarcy : 0.790895\n",
      "total step : 643 \n",
      "error : 0.663822, accuarcy : 0.791396\n",
      "total step : 644 \n",
      "error : 0.663026, accuarcy : 0.791396\n",
      "total step : 645 \n",
      "error : 0.662232, accuarcy : 0.791396\n",
      "total step : 646 \n",
      "error : 0.661440, accuarcy : 0.791396\n",
      "total step : 647 \n",
      "error : 0.660651, accuarcy : 0.791396\n",
      "total step : 648 \n",
      "error : 0.659865, accuarcy : 0.791396\n",
      "total step : 649 \n",
      "error : 0.659081, accuarcy : 0.791396\n",
      "total step : 650 \n",
      "error : 0.658299, accuarcy : 0.791896\n",
      "total step : 651 \n",
      "error : 0.657520, accuarcy : 0.792396\n",
      "total step : 652 \n",
      "error : 0.656743, accuarcy : 0.792896\n",
      "total step : 653 \n",
      "error : 0.655969, accuarcy : 0.793397\n",
      "total step : 654 \n",
      "error : 0.655198, accuarcy : 0.793397\n",
      "total step : 655 \n",
      "error : 0.654428, accuarcy : 0.793397\n",
      "total step : 656 \n",
      "error : 0.653662, accuarcy : 0.793397\n",
      "total step : 657 \n",
      "error : 0.652897, accuarcy : 0.793397\n",
      "total step : 658 \n",
      "error : 0.652135, accuarcy : 0.793397\n",
      "total step : 659 \n",
      "error : 0.651375, accuarcy : 0.793397\n",
      "total step : 660 \n",
      "error : 0.650618, accuarcy : 0.793897\n",
      "total step : 661 \n",
      "error : 0.649863, accuarcy : 0.793897\n",
      "total step : 662 \n",
      "error : 0.649111, accuarcy : 0.794397\n",
      "total step : 663 \n",
      "error : 0.648360, accuarcy : 0.794897\n",
      "total step : 664 \n",
      "error : 0.647613, accuarcy : 0.796398\n",
      "total step : 665 \n",
      "error : 0.646867, accuarcy : 0.796398\n",
      "total step : 666 \n",
      "error : 0.646124, accuarcy : 0.796398\n",
      "total step : 667 \n",
      "error : 0.645383, accuarcy : 0.796398\n",
      "total step : 668 \n",
      "error : 0.644644, accuarcy : 0.796398\n",
      "total step : 669 \n",
      "error : 0.643908, accuarcy : 0.796398\n",
      "total step : 670 \n",
      "error : 0.643173, accuarcy : 0.796398\n",
      "total step : 671 \n",
      "error : 0.642441, accuarcy : 0.796398\n",
      "total step : 672 \n",
      "error : 0.641712, accuarcy : 0.796398\n",
      "total step : 673 \n",
      "error : 0.640984, accuarcy : 0.796398\n",
      "total step : 674 \n",
      "error : 0.640259, accuarcy : 0.796898\n",
      "total step : 675 \n",
      "error : 0.639536, accuarcy : 0.796898\n",
      "total step : 676 \n",
      "error : 0.638815, accuarcy : 0.797399\n",
      "total step : 677 \n",
      "error : 0.638097, accuarcy : 0.797399\n",
      "total step : 678 \n",
      "error : 0.637380, accuarcy : 0.797399\n",
      "total step : 679 \n",
      "error : 0.636666, accuarcy : 0.797899\n",
      "total step : 680 \n",
      "error : 0.635954, accuarcy : 0.797899\n",
      "total step : 681 \n",
      "error : 0.635244, accuarcy : 0.797899\n",
      "total step : 682 \n",
      "error : 0.634536, accuarcy : 0.797899\n",
      "total step : 683 \n",
      "error : 0.633831, accuarcy : 0.797899\n",
      "total step : 684 \n",
      "error : 0.633127, accuarcy : 0.797899\n",
      "total step : 685 \n",
      "error : 0.632426, accuarcy : 0.797899\n",
      "total step : 686 \n",
      "error : 0.631726, accuarcy : 0.797899\n",
      "total step : 687 \n",
      "error : 0.631029, accuarcy : 0.798899\n",
      "total step : 688 \n",
      "error : 0.630334, accuarcy : 0.798899\n",
      "total step : 689 \n",
      "error : 0.629641, accuarcy : 0.798899\n",
      "total step : 690 \n",
      "error : 0.628950, accuarcy : 0.798899\n",
      "total step : 691 \n",
      "error : 0.628261, accuarcy : 0.798899\n",
      "total step : 692 \n",
      "error : 0.627574, accuarcy : 0.798899\n",
      "total step : 693 \n",
      "error : 0.626889, accuarcy : 0.798899\n",
      "total step : 694 \n",
      "error : 0.626206, accuarcy : 0.798899\n",
      "total step : 695 \n",
      "error : 0.625526, accuarcy : 0.799400\n",
      "total step : 696 \n",
      "error : 0.624847, accuarcy : 0.799400\n",
      "total step : 697 \n",
      "error : 0.624170, accuarcy : 0.799400\n",
      "total step : 698 \n",
      "error : 0.623495, accuarcy : 0.799400\n",
      "total step : 699 \n",
      "error : 0.622822, accuarcy : 0.799900\n",
      "total step : 700 \n",
      "error : 0.622152, accuarcy : 0.799900\n",
      "total step : 701 \n",
      "error : 0.621483, accuarcy : 0.799900\n",
      "total step : 702 \n",
      "error : 0.620816, accuarcy : 0.799900\n",
      "total step : 703 \n",
      "error : 0.620151, accuarcy : 0.799900\n",
      "total step : 704 \n",
      "error : 0.619488, accuarcy : 0.799900\n",
      "total step : 705 \n",
      "error : 0.618827, accuarcy : 0.799900\n",
      "total step : 706 \n",
      "error : 0.618168, accuarcy : 0.800400\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for number 8\n",
      "total step : 1 \n",
      "error : 3.069243, accuarcy : 0.550775\n",
      "total step : 2 \n",
      "error : 3.040674, accuarcy : 0.550775\n",
      "total step : 3 \n",
      "error : 3.012938, accuarcy : 0.548774\n",
      "total step : 4 \n",
      "error : 2.986011, accuarcy : 0.550775\n",
      "total step : 5 \n",
      "error : 2.959872, accuarcy : 0.552276\n",
      "total step : 6 \n",
      "error : 2.934498, accuarcy : 0.550775\n",
      "total step : 7 \n",
      "error : 2.909868, accuarcy : 0.550775\n",
      "total step : 8 \n",
      "error : 2.885960, accuarcy : 0.550275\n",
      "total step : 9 \n",
      "error : 2.862755, accuarcy : 0.552776\n",
      "total step : 10 \n",
      "error : 2.840231, accuarcy : 0.553777\n",
      "total step : 11 \n",
      "error : 2.818370, accuarcy : 0.554277\n",
      "total step : 12 \n",
      "error : 2.797151, accuarcy : 0.555778\n",
      "total step : 13 \n",
      "error : 2.776557, accuarcy : 0.554777\n",
      "total step : 14 \n",
      "error : 2.756568, accuarcy : 0.554277\n",
      "total step : 15 \n",
      "error : 2.737167, accuarcy : 0.556278\n",
      "total step : 16 \n",
      "error : 2.718336, accuarcy : 0.555778\n",
      "total step : 17 \n",
      "error : 2.700059, accuarcy : 0.554277\n",
      "total step : 18 \n",
      "error : 2.682318, accuarcy : 0.554777\n",
      "total step : 19 \n",
      "error : 2.665099, accuarcy : 0.554777\n",
      "total step : 20 \n",
      "error : 2.648385, accuarcy : 0.554277\n",
      "total step : 21 \n",
      "error : 2.632161, accuarcy : 0.554777\n",
      "total step : 22 \n",
      "error : 2.616412, accuarcy : 0.555278\n",
      "total step : 23 \n",
      "error : 2.601124, accuarcy : 0.555278\n",
      "total step : 24 \n",
      "error : 2.586282, accuarcy : 0.556278\n",
      "total step : 25 \n",
      "error : 2.571872, accuarcy : 0.558779\n",
      "total step : 26 \n",
      "error : 2.557879, accuarcy : 0.559780\n",
      "total step : 27 \n",
      "error : 2.544291, accuarcy : 0.557779\n",
      "total step : 28 \n",
      "error : 2.531094, accuarcy : 0.558279\n",
      "total step : 29 \n",
      "error : 2.518275, accuarcy : 0.557779\n",
      "total step : 30 \n",
      "error : 2.505821, accuarcy : 0.557779\n",
      "total step : 31 \n",
      "error : 2.493720, accuarcy : 0.555278\n",
      "total step : 32 \n",
      "error : 2.481959, accuarcy : 0.555778\n",
      "total step : 33 \n",
      "error : 2.470525, accuarcy : 0.556778\n",
      "total step : 34 \n",
      "error : 2.459408, accuarcy : 0.555778\n",
      "total step : 35 \n",
      "error : 2.448596, accuarcy : 0.556778\n",
      "total step : 36 \n",
      "error : 2.438077, accuarcy : 0.558279\n",
      "total step : 37 \n",
      "error : 2.427841, accuarcy : 0.558279\n",
      "total step : 38 \n",
      "error : 2.417878, accuarcy : 0.560280\n",
      "total step : 39 \n",
      "error : 2.408176, accuarcy : 0.562281\n",
      "total step : 40 \n",
      "error : 2.398727, accuarcy : 0.561781\n",
      "total step : 41 \n",
      "error : 2.389520, accuarcy : 0.563282\n",
      "total step : 42 \n",
      "error : 2.380546, accuarcy : 0.563782\n",
      "total step : 43 \n",
      "error : 2.371797, accuarcy : 0.565783\n",
      "total step : 44 \n",
      "error : 2.363263, accuarcy : 0.565783\n",
      "total step : 45 \n",
      "error : 2.354937, accuarcy : 0.565783\n",
      "total step : 46 \n",
      "error : 2.346810, accuarcy : 0.567784\n",
      "total step : 47 \n",
      "error : 2.338874, accuarcy : 0.567784\n",
      "total step : 48 \n",
      "error : 2.331123, accuarcy : 0.568284\n",
      "total step : 49 \n",
      "error : 2.323548, accuarcy : 0.568284\n",
      "total step : 50 \n",
      "error : 2.316144, accuarcy : 0.568784\n",
      "total step : 51 \n",
      "error : 2.308902, accuarcy : 0.569285\n",
      "total step : 52 \n",
      "error : 2.301818, accuarcy : 0.569785\n",
      "total step : 53 \n",
      "error : 2.294884, accuarcy : 0.568284\n",
      "total step : 54 \n",
      "error : 2.288095, accuarcy : 0.570285\n",
      "total step : 55 \n",
      "error : 2.281446, accuarcy : 0.570285\n",
      "total step : 56 \n",
      "error : 2.274930, accuarcy : 0.570285\n",
      "total step : 57 \n",
      "error : 2.268543, accuarcy : 0.571286\n",
      "total step : 58 \n",
      "error : 2.262279, accuarcy : 0.569785\n",
      "total step : 59 \n",
      "error : 2.256134, accuarcy : 0.570285\n",
      "total step : 60 \n",
      "error : 2.250104, accuarcy : 0.570285\n",
      "total step : 61 \n",
      "error : 2.244182, accuarcy : 0.569785\n",
      "total step : 62 \n",
      "error : 2.238367, accuarcy : 0.571286\n",
      "total step : 63 \n",
      "error : 2.232652, accuarcy : 0.571786\n",
      "total step : 64 \n",
      "error : 2.227035, accuarcy : 0.570785\n",
      "total step : 65 \n",
      "error : 2.221512, accuarcy : 0.571286\n",
      "total step : 66 \n",
      "error : 2.216078, accuarcy : 0.571286\n",
      "total step : 67 \n",
      "error : 2.210731, accuarcy : 0.571786\n",
      "total step : 68 \n",
      "error : 2.205467, accuarcy : 0.572786\n",
      "total step : 69 \n",
      "error : 2.200284, accuarcy : 0.572786\n",
      "total step : 70 \n",
      "error : 2.195177, accuarcy : 0.572786\n",
      "total step : 71 \n",
      "error : 2.190144, accuarcy : 0.573287\n",
      "total step : 72 \n",
      "error : 2.185183, accuarcy : 0.574787\n",
      "total step : 73 \n",
      "error : 2.180290, accuarcy : 0.573787\n",
      "total step : 74 \n",
      "error : 2.175463, accuarcy : 0.574287\n",
      "total step : 75 \n",
      "error : 2.170700, accuarcy : 0.575288\n",
      "total step : 76 \n",
      "error : 2.165998, accuarcy : 0.574787\n",
      "total step : 77 \n",
      "error : 2.161355, accuarcy : 0.575288\n",
      "total step : 78 \n",
      "error : 2.156768, accuarcy : 0.574787\n",
      "total step : 79 \n",
      "error : 2.152236, accuarcy : 0.574787\n",
      "total step : 80 \n",
      "error : 2.147757, accuarcy : 0.575288\n",
      "total step : 81 \n",
      "error : 2.143329, accuarcy : 0.575788\n",
      "total step : 82 \n",
      "error : 2.138949, accuarcy : 0.575788\n",
      "total step : 83 \n",
      "error : 2.134617, accuarcy : 0.575788\n",
      "total step : 84 \n",
      "error : 2.130330, accuarcy : 0.576288\n",
      "total step : 85 \n",
      "error : 2.126088, accuarcy : 0.576788\n",
      "total step : 86 \n",
      "error : 2.121887, accuarcy : 0.576288\n",
      "total step : 87 \n",
      "error : 2.117728, accuarcy : 0.575788\n",
      "total step : 88 \n",
      "error : 2.113608, accuarcy : 0.575788\n",
      "total step : 89 \n",
      "error : 2.109526, accuarcy : 0.575788\n",
      "total step : 90 \n",
      "error : 2.105481, accuarcy : 0.576288\n",
      "total step : 91 \n",
      "error : 2.101472, accuarcy : 0.576288\n",
      "total step : 92 \n",
      "error : 2.097498, accuarcy : 0.575788\n",
      "total step : 93 \n",
      "error : 2.093556, accuarcy : 0.576788\n",
      "total step : 94 \n",
      "error : 2.089647, accuarcy : 0.577289\n",
      "total step : 95 \n",
      "error : 2.085769, accuarcy : 0.577789\n",
      "total step : 96 \n",
      "error : 2.081922, accuarcy : 0.577289\n",
      "total step : 97 \n",
      "error : 2.078104, accuarcy : 0.577289\n",
      "total step : 98 \n",
      "error : 2.074314, accuarcy : 0.577789\n",
      "total step : 99 \n",
      "error : 2.070552, accuarcy : 0.578289\n",
      "total step : 100 \n",
      "error : 2.066817, accuarcy : 0.578289\n",
      "total step : 101 \n",
      "error : 2.063107, accuarcy : 0.578789\n",
      "total step : 102 \n",
      "error : 2.059423, accuarcy : 0.578789\n",
      "total step : 103 \n",
      "error : 2.055764, accuarcy : 0.580290\n",
      "total step : 104 \n",
      "error : 2.052128, accuarcy : 0.580790\n",
      "total step : 105 \n",
      "error : 2.048515, accuarcy : 0.580790\n",
      "total step : 106 \n",
      "error : 2.044925, accuarcy : 0.581291\n",
      "total step : 107 \n",
      "error : 2.041356, accuarcy : 0.581291\n",
      "total step : 108 \n",
      "error : 2.037809, accuarcy : 0.580790\n",
      "total step : 109 \n",
      "error : 2.034283, accuarcy : 0.581291\n",
      "total step : 110 \n",
      "error : 2.030777, accuarcy : 0.581291\n",
      "total step : 111 \n",
      "error : 2.027291, accuarcy : 0.580790\n",
      "total step : 112 \n",
      "error : 2.023824, accuarcy : 0.582291\n",
      "total step : 113 \n",
      "error : 2.020376, accuarcy : 0.583792\n",
      "total step : 114 \n",
      "error : 2.016946, accuarcy : 0.584292\n",
      "total step : 115 \n",
      "error : 2.013535, accuarcy : 0.584292\n",
      "total step : 116 \n",
      "error : 2.010140, accuarcy : 0.584292\n",
      "total step : 117 \n",
      "error : 2.006763, accuarcy : 0.584292\n",
      "total step : 118 \n",
      "error : 2.003403, accuarcy : 0.584292\n",
      "total step : 119 \n",
      "error : 2.000059, accuarcy : 0.585293\n",
      "total step : 120 \n",
      "error : 1.996731, accuarcy : 0.585293\n",
      "total step : 121 \n",
      "error : 1.993419, accuarcy : 0.585793\n",
      "total step : 122 \n",
      "error : 1.990123, accuarcy : 0.586293\n",
      "total step : 123 \n",
      "error : 1.986842, accuarcy : 0.586793\n",
      "total step : 124 \n",
      "error : 1.983575, accuarcy : 0.586793\n",
      "total step : 125 \n",
      "error : 1.980323, accuarcy : 0.587294\n",
      "total step : 126 \n",
      "error : 1.977086, accuarcy : 0.587794\n",
      "total step : 127 \n",
      "error : 1.973863, accuarcy : 0.588294\n",
      "total step : 128 \n",
      "error : 1.970653, accuarcy : 0.587794\n",
      "total step : 129 \n",
      "error : 1.967457, accuarcy : 0.588294\n",
      "total step : 130 \n",
      "error : 1.964275, accuarcy : 0.590295\n",
      "total step : 131 \n",
      "error : 1.961106, accuarcy : 0.590295\n",
      "total step : 132 \n",
      "error : 1.957950, accuarcy : 0.590295\n",
      "total step : 133 \n",
      "error : 1.954806, accuarcy : 0.589795\n",
      "total step : 134 \n",
      "error : 1.951676, accuarcy : 0.590795\n",
      "total step : 135 \n",
      "error : 1.948558, accuarcy : 0.591796\n",
      "total step : 136 \n",
      "error : 1.945452, accuarcy : 0.590795\n",
      "total step : 137 \n",
      "error : 1.942358, accuarcy : 0.591296\n",
      "total step : 138 \n",
      "error : 1.939276, accuarcy : 0.591796\n",
      "total step : 139 \n",
      "error : 1.936206, accuarcy : 0.593297\n",
      "total step : 140 \n",
      "error : 1.933148, accuarcy : 0.593797\n",
      "total step : 141 \n",
      "error : 1.930102, accuarcy : 0.594797\n",
      "total step : 142 \n",
      "error : 1.927066, accuarcy : 0.595298\n",
      "total step : 143 \n",
      "error : 1.924042, accuarcy : 0.595298\n",
      "total step : 144 \n",
      "error : 1.921030, accuarcy : 0.595298\n",
      "total step : 145 \n",
      "error : 1.918028, accuarcy : 0.595798\n",
      "total step : 146 \n",
      "error : 1.915037, accuarcy : 0.595798\n",
      "total step : 147 \n",
      "error : 1.912057, accuarcy : 0.595798\n",
      "total step : 148 \n",
      "error : 1.909088, accuarcy : 0.596298\n",
      "total step : 149 \n",
      "error : 1.906129, accuarcy : 0.597299\n",
      "total step : 150 \n",
      "error : 1.903181, accuarcy : 0.597299\n",
      "total step : 151 \n",
      "error : 1.900244, accuarcy : 0.597799\n",
      "total step : 152 \n",
      "error : 1.897316, accuarcy : 0.597299\n",
      "total step : 153 \n",
      "error : 1.894399, accuarcy : 0.596798\n",
      "total step : 154 \n",
      "error : 1.891492, accuarcy : 0.596798\n",
      "total step : 155 \n",
      "error : 1.888596, accuarcy : 0.597799\n",
      "total step : 156 \n",
      "error : 1.885709, accuarcy : 0.598299\n",
      "total step : 157 \n",
      "error : 1.882832, accuarcy : 0.599800\n",
      "total step : 158 \n",
      "error : 1.879965, accuarcy : 0.599800\n",
      "total step : 159 \n",
      "error : 1.877108, accuarcy : 0.600300\n",
      "total step : 160 \n",
      "error : 1.874260, accuarcy : 0.600800\n",
      "total step : 161 \n",
      "error : 1.871422, accuarcy : 0.601301\n",
      "total step : 162 \n",
      "error : 1.868594, accuarcy : 0.601301\n",
      "total step : 163 \n",
      "error : 1.865776, accuarcy : 0.601801\n",
      "total step : 164 \n",
      "error : 1.862966, accuarcy : 0.601801\n",
      "total step : 165 \n",
      "error : 1.860166, accuarcy : 0.601801\n",
      "total step : 166 \n",
      "error : 1.857376, accuarcy : 0.601801\n",
      "total step : 167 \n",
      "error : 1.854595, accuarcy : 0.601301\n",
      "total step : 168 \n",
      "error : 1.851823, accuarcy : 0.601301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 169 \n",
      "error : 1.849060, accuarcy : 0.601301\n",
      "total step : 170 \n",
      "error : 1.846306, accuarcy : 0.601801\n",
      "total step : 171 \n",
      "error : 1.843562, accuarcy : 0.602301\n",
      "total step : 172 \n",
      "error : 1.840826, accuarcy : 0.602801\n",
      "total step : 173 \n",
      "error : 1.838099, accuarcy : 0.603302\n",
      "total step : 174 \n",
      "error : 1.835382, accuarcy : 0.603802\n",
      "total step : 175 \n",
      "error : 1.832673, accuarcy : 0.604802\n",
      "total step : 176 \n",
      "error : 1.829973, accuarcy : 0.605803\n",
      "total step : 177 \n",
      "error : 1.827282, accuarcy : 0.607304\n",
      "total step : 178 \n",
      "error : 1.824599, accuarcy : 0.606803\n",
      "total step : 179 \n",
      "error : 1.821926, accuarcy : 0.606803\n",
      "total step : 180 \n",
      "error : 1.819261, accuarcy : 0.606803\n",
      "total step : 181 \n",
      "error : 1.816604, accuarcy : 0.607304\n",
      "total step : 182 \n",
      "error : 1.813956, accuarcy : 0.607804\n",
      "total step : 183 \n",
      "error : 1.811317, accuarcy : 0.608304\n",
      "total step : 184 \n",
      "error : 1.808686, accuarcy : 0.609305\n",
      "total step : 185 \n",
      "error : 1.806064, accuarcy : 0.609805\n",
      "total step : 186 \n",
      "error : 1.803450, accuarcy : 0.610305\n",
      "total step : 187 \n",
      "error : 1.800845, accuarcy : 0.610305\n",
      "total step : 188 \n",
      "error : 1.798247, accuarcy : 0.610805\n",
      "total step : 189 \n",
      "error : 1.795659, accuarcy : 0.610805\n",
      "total step : 190 \n",
      "error : 1.793078, accuarcy : 0.612306\n",
      "total step : 191 \n",
      "error : 1.790506, accuarcy : 0.612306\n",
      "total step : 192 \n",
      "error : 1.787942, accuarcy : 0.612306\n",
      "total step : 193 \n",
      "error : 1.785386, accuarcy : 0.612806\n",
      "total step : 194 \n",
      "error : 1.782838, accuarcy : 0.613307\n",
      "total step : 195 \n",
      "error : 1.780298, accuarcy : 0.613807\n",
      "total step : 196 \n",
      "error : 1.777767, accuarcy : 0.614807\n",
      "total step : 197 \n",
      "error : 1.775243, accuarcy : 0.614807\n",
      "total step : 198 \n",
      "error : 1.772728, accuarcy : 0.615308\n",
      "total step : 199 \n",
      "error : 1.770220, accuarcy : 0.616308\n",
      "total step : 200 \n",
      "error : 1.767721, accuarcy : 0.616808\n",
      "total step : 201 \n",
      "error : 1.765229, accuarcy : 0.616308\n",
      "total step : 202 \n",
      "error : 1.762746, accuarcy : 0.617309\n",
      "total step : 203 \n",
      "error : 1.760270, accuarcy : 0.617809\n",
      "total step : 204 \n",
      "error : 1.757802, accuarcy : 0.617809\n",
      "total step : 205 \n",
      "error : 1.755342, accuarcy : 0.617809\n",
      "total step : 206 \n",
      "error : 1.752889, accuarcy : 0.618309\n",
      "total step : 207 \n",
      "error : 1.750444, accuarcy : 0.618309\n",
      "total step : 208 \n",
      "error : 1.748007, accuarcy : 0.618309\n",
      "total step : 209 \n",
      "error : 1.745578, accuarcy : 0.618309\n",
      "total step : 210 \n",
      "error : 1.743157, accuarcy : 0.618809\n",
      "total step : 211 \n",
      "error : 1.740743, accuarcy : 0.618309\n",
      "total step : 212 \n",
      "error : 1.738336, accuarcy : 0.618309\n",
      "total step : 213 \n",
      "error : 1.735937, accuarcy : 0.618809\n",
      "total step : 214 \n",
      "error : 1.733546, accuarcy : 0.619310\n",
      "total step : 215 \n",
      "error : 1.731162, accuarcy : 0.619310\n",
      "total step : 216 \n",
      "error : 1.728786, accuarcy : 0.619810\n",
      "total step : 217 \n",
      "error : 1.726417, accuarcy : 0.619810\n",
      "total step : 218 \n",
      "error : 1.724056, accuarcy : 0.620810\n",
      "total step : 219 \n",
      "error : 1.721702, accuarcy : 0.620810\n",
      "total step : 220 \n",
      "error : 1.719355, accuarcy : 0.621311\n",
      "total step : 221 \n",
      "error : 1.717016, accuarcy : 0.621811\n",
      "total step : 222 \n",
      "error : 1.714684, accuarcy : 0.622311\n",
      "total step : 223 \n",
      "error : 1.712359, accuarcy : 0.622311\n",
      "total step : 224 \n",
      "error : 1.710042, accuarcy : 0.622311\n",
      "total step : 225 \n",
      "error : 1.707732, accuarcy : 0.622311\n",
      "total step : 226 \n",
      "error : 1.705429, accuarcy : 0.622311\n",
      "total step : 227 \n",
      "error : 1.703134, accuarcy : 0.622811\n",
      "total step : 228 \n",
      "error : 1.700845, accuarcy : 0.623312\n",
      "total step : 229 \n",
      "error : 1.698564, accuarcy : 0.623312\n",
      "total step : 230 \n",
      "error : 1.696289, accuarcy : 0.623312\n",
      "total step : 231 \n",
      "error : 1.694022, accuarcy : 0.624312\n",
      "total step : 232 \n",
      "error : 1.691762, accuarcy : 0.625313\n",
      "total step : 233 \n",
      "error : 1.689509, accuarcy : 0.625313\n",
      "total step : 234 \n",
      "error : 1.687263, accuarcy : 0.625813\n",
      "total step : 235 \n",
      "error : 1.685024, accuarcy : 0.626313\n",
      "total step : 236 \n",
      "error : 1.682792, accuarcy : 0.626813\n",
      "total step : 237 \n",
      "error : 1.680567, accuarcy : 0.627314\n",
      "total step : 238 \n",
      "error : 1.678349, accuarcy : 0.627814\n",
      "total step : 239 \n",
      "error : 1.676138, accuarcy : 0.628314\n",
      "total step : 240 \n",
      "error : 1.673933, accuarcy : 0.628314\n",
      "total step : 241 \n",
      "error : 1.671736, accuarcy : 0.628314\n",
      "total step : 242 \n",
      "error : 1.669545, accuarcy : 0.628814\n",
      "total step : 243 \n",
      "error : 1.667361, accuarcy : 0.628814\n",
      "total step : 244 \n",
      "error : 1.665184, accuarcy : 0.628814\n",
      "total step : 245 \n",
      "error : 1.663013, accuarcy : 0.629315\n",
      "total step : 246 \n",
      "error : 1.660850, accuarcy : 0.629815\n",
      "total step : 247 \n",
      "error : 1.658693, accuarcy : 0.629815\n",
      "total step : 248 \n",
      "error : 1.656542, accuarcy : 0.630315\n",
      "total step : 249 \n",
      "error : 1.654399, accuarcy : 0.630315\n",
      "total step : 250 \n",
      "error : 1.652262, accuarcy : 0.630315\n",
      "total step : 251 \n",
      "error : 1.650131, accuarcy : 0.631316\n",
      "total step : 252 \n",
      "error : 1.648007, accuarcy : 0.631316\n",
      "total step : 253 \n",
      "error : 1.645890, accuarcy : 0.632816\n",
      "total step : 254 \n",
      "error : 1.643779, accuarcy : 0.633817\n",
      "total step : 255 \n",
      "error : 1.641675, accuarcy : 0.633817\n",
      "total step : 256 \n",
      "error : 1.639577, accuarcy : 0.633817\n",
      "total step : 257 \n",
      "error : 1.637486, accuarcy : 0.633817\n",
      "total step : 258 \n",
      "error : 1.635401, accuarcy : 0.633817\n",
      "total step : 259 \n",
      "error : 1.633323, accuarcy : 0.633817\n",
      "total step : 260 \n",
      "error : 1.631251, accuarcy : 0.634317\n",
      "total step : 261 \n",
      "error : 1.629185, accuarcy : 0.634317\n",
      "total step : 262 \n",
      "error : 1.627126, accuarcy : 0.634317\n",
      "total step : 263 \n",
      "error : 1.625073, accuarcy : 0.634817\n",
      "total step : 264 \n",
      "error : 1.623026, accuarcy : 0.634817\n",
      "total step : 265 \n",
      "error : 1.620986, accuarcy : 0.635318\n",
      "total step : 266 \n",
      "error : 1.618952, accuarcy : 0.636318\n",
      "total step : 267 \n",
      "error : 1.616924, accuarcy : 0.636318\n",
      "total step : 268 \n",
      "error : 1.614902, accuarcy : 0.636318\n",
      "total step : 269 \n",
      "error : 1.612887, accuarcy : 0.636318\n",
      "total step : 270 \n",
      "error : 1.610878, accuarcy : 0.636818\n",
      "total step : 271 \n",
      "error : 1.608875, accuarcy : 0.636818\n",
      "total step : 272 \n",
      "error : 1.606878, accuarcy : 0.636818\n",
      "total step : 273 \n",
      "error : 1.604887, accuarcy : 0.638319\n",
      "total step : 274 \n",
      "error : 1.602902, accuarcy : 0.638819\n",
      "total step : 275 \n",
      "error : 1.600923, accuarcy : 0.638819\n",
      "total step : 276 \n",
      "error : 1.598951, accuarcy : 0.638819\n",
      "total step : 277 \n",
      "error : 1.596984, accuarcy : 0.638819\n",
      "total step : 278 \n",
      "error : 1.595023, accuarcy : 0.638819\n",
      "total step : 279 \n",
      "error : 1.593069, accuarcy : 0.638819\n",
      "total step : 280 \n",
      "error : 1.591120, accuarcy : 0.639320\n",
      "total step : 281 \n",
      "error : 1.589177, accuarcy : 0.639820\n",
      "total step : 282 \n",
      "error : 1.587240, accuarcy : 0.639820\n",
      "total step : 283 \n",
      "error : 1.585309, accuarcy : 0.640820\n",
      "total step : 284 \n",
      "error : 1.583384, accuarcy : 0.641321\n",
      "total step : 285 \n",
      "error : 1.581465, accuarcy : 0.641821\n",
      "total step : 286 \n",
      "error : 1.579552, accuarcy : 0.642821\n",
      "total step : 287 \n",
      "error : 1.577644, accuarcy : 0.642321\n",
      "total step : 288 \n",
      "error : 1.575742, accuarcy : 0.643322\n",
      "total step : 289 \n",
      "error : 1.573846, accuarcy : 0.643822\n",
      "total step : 290 \n",
      "error : 1.571956, accuarcy : 0.644322\n",
      "total step : 291 \n",
      "error : 1.570071, accuarcy : 0.644822\n",
      "total step : 292 \n",
      "error : 1.568192, accuarcy : 0.644822\n",
      "total step : 293 \n",
      "error : 1.566319, accuarcy : 0.644822\n",
      "total step : 294 \n",
      "error : 1.564452, accuarcy : 0.644822\n",
      "total step : 295 \n",
      "error : 1.562590, accuarcy : 0.644822\n",
      "total step : 296 \n",
      "error : 1.560734, accuarcy : 0.644322\n",
      "total step : 297 \n",
      "error : 1.558883, accuarcy : 0.643822\n",
      "total step : 298 \n",
      "error : 1.557038, accuarcy : 0.643822\n",
      "total step : 299 \n",
      "error : 1.555199, accuarcy : 0.643822\n",
      "total step : 300 \n",
      "error : 1.553365, accuarcy : 0.644322\n",
      "total step : 301 \n",
      "error : 1.551536, accuarcy : 0.644322\n",
      "total step : 302 \n",
      "error : 1.549713, accuarcy : 0.644822\n",
      "total step : 303 \n",
      "error : 1.547896, accuarcy : 0.644822\n",
      "total step : 304 \n",
      "error : 1.546084, accuarcy : 0.644822\n",
      "total step : 305 \n",
      "error : 1.544278, accuarcy : 0.644822\n",
      "total step : 306 \n",
      "error : 1.542476, accuarcy : 0.644822\n",
      "total step : 307 \n",
      "error : 1.540681, accuarcy : 0.645323\n",
      "total step : 308 \n",
      "error : 1.538890, accuarcy : 0.644822\n",
      "total step : 309 \n",
      "error : 1.537106, accuarcy : 0.644822\n",
      "total step : 310 \n",
      "error : 1.535326, accuarcy : 0.644822\n",
      "total step : 311 \n",
      "error : 1.533552, accuarcy : 0.644822\n",
      "total step : 312 \n",
      "error : 1.531783, accuarcy : 0.644822\n",
      "total step : 313 \n",
      "error : 1.530019, accuarcy : 0.644822\n",
      "total step : 314 \n",
      "error : 1.528261, accuarcy : 0.645323\n",
      "total step : 315 \n",
      "error : 1.526508, accuarcy : 0.645823\n",
      "total step : 316 \n",
      "error : 1.524760, accuarcy : 0.645823\n",
      "total step : 317 \n",
      "error : 1.523017, accuarcy : 0.645823\n",
      "total step : 318 \n",
      "error : 1.521280, accuarcy : 0.646323\n",
      "total step : 319 \n",
      "error : 1.519547, accuarcy : 0.646323\n",
      "total step : 320 \n",
      "error : 1.517820, accuarcy : 0.645823\n",
      "total step : 321 \n",
      "error : 1.516098, accuarcy : 0.646323\n",
      "total step : 322 \n",
      "error : 1.514381, accuarcy : 0.646323\n",
      "total step : 323 \n",
      "error : 1.512669, accuarcy : 0.646823\n",
      "total step : 324 \n",
      "error : 1.510963, accuarcy : 0.646823\n",
      "total step : 325 \n",
      "error : 1.509261, accuarcy : 0.647324\n",
      "total step : 326 \n",
      "error : 1.507564, accuarcy : 0.647324\n",
      "total step : 327 \n",
      "error : 1.505873, accuarcy : 0.647324\n",
      "total step : 328 \n",
      "error : 1.504186, accuarcy : 0.647824\n",
      "total step : 329 \n",
      "error : 1.502505, accuarcy : 0.647824\n",
      "total step : 330 \n",
      "error : 1.500828, accuarcy : 0.647824\n",
      "total step : 331 \n",
      "error : 1.499156, accuarcy : 0.647824\n",
      "total step : 332 \n",
      "error : 1.497490, accuarcy : 0.648324\n",
      "total step : 333 \n",
      "error : 1.495828, accuarcy : 0.649325\n",
      "total step : 334 \n",
      "error : 1.494171, accuarcy : 0.649325\n",
      "total step : 335 \n",
      "error : 1.492519, accuarcy : 0.649825\n",
      "total step : 336 \n",
      "error : 1.490871, accuarcy : 0.649825\n",
      "total step : 337 \n",
      "error : 1.489229, accuarcy : 0.649825\n",
      "total step : 338 \n",
      "error : 1.487592, accuarcy : 0.650325\n",
      "total step : 339 \n",
      "error : 1.485959, accuarcy : 0.650325\n",
      "total step : 340 \n",
      "error : 1.484331, accuarcy : 0.650825\n",
      "total step : 341 \n",
      "error : 1.482708, accuarcy : 0.650825\n",
      "total step : 342 \n",
      "error : 1.481089, accuarcy : 0.650825\n",
      "total step : 343 \n",
      "error : 1.479475, accuarcy : 0.651326\n",
      "total step : 344 \n",
      "error : 1.477866, accuarcy : 0.651326\n",
      "total step : 345 \n",
      "error : 1.476262, accuarcy : 0.651326\n",
      "total step : 346 \n",
      "error : 1.474663, accuarcy : 0.651326\n",
      "total step : 347 \n",
      "error : 1.473068, accuarcy : 0.651826\n",
      "total step : 348 \n",
      "error : 1.471477, accuarcy : 0.652326\n",
      "total step : 349 \n",
      "error : 1.469892, accuarcy : 0.652826\n",
      "total step : 350 \n",
      "error : 1.468311, accuarcy : 0.652826\n",
      "total step : 351 \n",
      "error : 1.466734, accuarcy : 0.652826\n",
      "total step : 352 \n",
      "error : 1.465162, accuarcy : 0.653327\n",
      "total step : 353 \n",
      "error : 1.463595, accuarcy : 0.653327\n",
      "total step : 354 \n",
      "error : 1.462032, accuarcy : 0.654827\n",
      "total step : 355 \n",
      "error : 1.460474, accuarcy : 0.654827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 356 \n",
      "error : 1.458920, accuarcy : 0.656828\n",
      "total step : 357 \n",
      "error : 1.457371, accuarcy : 0.656828\n",
      "total step : 358 \n",
      "error : 1.455826, accuarcy : 0.656828\n",
      "total step : 359 \n",
      "error : 1.454286, accuarcy : 0.656828\n",
      "total step : 360 \n",
      "error : 1.452750, accuarcy : 0.656828\n",
      "total step : 361 \n",
      "error : 1.451218, accuarcy : 0.656828\n",
      "total step : 362 \n",
      "error : 1.449691, accuarcy : 0.657329\n",
      "total step : 363 \n",
      "error : 1.448169, accuarcy : 0.656828\n",
      "total step : 364 \n",
      "error : 1.446650, accuarcy : 0.656828\n",
      "total step : 365 \n",
      "error : 1.445136, accuarcy : 0.657329\n",
      "total step : 366 \n",
      "error : 1.443627, accuarcy : 0.657829\n",
      "total step : 367 \n",
      "error : 1.442121, accuarcy : 0.657829\n",
      "total step : 368 \n",
      "error : 1.440620, accuarcy : 0.657829\n",
      "total step : 369 \n",
      "error : 1.439124, accuarcy : 0.658329\n",
      "total step : 370 \n",
      "error : 1.437631, accuarcy : 0.658329\n",
      "total step : 371 \n",
      "error : 1.436143, accuarcy : 0.658829\n",
      "total step : 372 \n",
      "error : 1.434659, accuarcy : 0.659330\n",
      "total step : 373 \n",
      "error : 1.433180, accuarcy : 0.659830\n",
      "total step : 374 \n",
      "error : 1.431704, accuarcy : 0.660330\n",
      "total step : 375 \n",
      "error : 1.430233, accuarcy : 0.660330\n",
      "total step : 376 \n",
      "error : 1.428766, accuarcy : 0.660330\n",
      "total step : 377 \n",
      "error : 1.427303, accuarcy : 0.660830\n",
      "total step : 378 \n",
      "error : 1.425844, accuarcy : 0.660830\n",
      "total step : 379 \n",
      "error : 1.424389, accuarcy : 0.661331\n",
      "total step : 380 \n",
      "error : 1.422939, accuarcy : 0.661331\n",
      "total step : 381 \n",
      "error : 1.421492, accuarcy : 0.661331\n",
      "total step : 382 \n",
      "error : 1.420050, accuarcy : 0.661331\n",
      "total step : 383 \n",
      "error : 1.418612, accuarcy : 0.661831\n",
      "total step : 384 \n",
      "error : 1.417177, accuarcy : 0.661831\n",
      "total step : 385 \n",
      "error : 1.415747, accuarcy : 0.661831\n",
      "total step : 386 \n",
      "error : 1.414321, accuarcy : 0.661831\n",
      "total step : 387 \n",
      "error : 1.412899, accuarcy : 0.662331\n",
      "total step : 388 \n",
      "error : 1.411481, accuarcy : 0.662331\n",
      "total step : 389 \n",
      "error : 1.410066, accuarcy : 0.662331\n",
      "total step : 390 \n",
      "error : 1.408656, accuarcy : 0.662831\n",
      "total step : 391 \n",
      "error : 1.407250, accuarcy : 0.662831\n",
      "total step : 392 \n",
      "error : 1.405847, accuarcy : 0.662331\n",
      "total step : 393 \n",
      "error : 1.404449, accuarcy : 0.661831\n",
      "total step : 394 \n",
      "error : 1.403054, accuarcy : 0.661831\n",
      "total step : 395 \n",
      "error : 1.401664, accuarcy : 0.662331\n",
      "total step : 396 \n",
      "error : 1.400277, accuarcy : 0.662331\n",
      "total step : 397 \n",
      "error : 1.398894, accuarcy : 0.662831\n",
      "total step : 398 \n",
      "error : 1.397515, accuarcy : 0.662831\n",
      "total step : 399 \n",
      "error : 1.396139, accuarcy : 0.663332\n",
      "total step : 400 \n",
      "error : 1.394768, accuarcy : 0.663832\n",
      "total step : 401 \n",
      "error : 1.393400, accuarcy : 0.663832\n",
      "total step : 402 \n",
      "error : 1.392036, accuarcy : 0.663832\n",
      "total step : 403 \n",
      "error : 1.390676, accuarcy : 0.664332\n",
      "total step : 404 \n",
      "error : 1.389320, accuarcy : 0.664332\n",
      "total step : 405 \n",
      "error : 1.387967, accuarcy : 0.664332\n",
      "total step : 406 \n",
      "error : 1.386618, accuarcy : 0.664332\n",
      "total step : 407 \n",
      "error : 1.385273, accuarcy : 0.664332\n",
      "total step : 408 \n",
      "error : 1.383932, accuarcy : 0.665333\n",
      "total step : 409 \n",
      "error : 1.382594, accuarcy : 0.665333\n",
      "total step : 410 \n",
      "error : 1.381260, accuarcy : 0.665833\n",
      "total step : 411 \n",
      "error : 1.379929, accuarcy : 0.665833\n",
      "total step : 412 \n",
      "error : 1.378602, accuarcy : 0.665333\n",
      "total step : 413 \n",
      "error : 1.377279, accuarcy : 0.665833\n",
      "total step : 414 \n",
      "error : 1.375959, accuarcy : 0.665833\n",
      "total step : 415 \n",
      "error : 1.374643, accuarcy : 0.665833\n",
      "total step : 416 \n",
      "error : 1.373331, accuarcy : 0.665833\n",
      "total step : 417 \n",
      "error : 1.372022, accuarcy : 0.666333\n",
      "total step : 418 \n",
      "error : 1.370717, accuarcy : 0.666333\n",
      "total step : 419 \n",
      "error : 1.369415, accuarcy : 0.666833\n",
      "total step : 420 \n",
      "error : 1.368117, accuarcy : 0.666833\n",
      "total step : 421 \n",
      "error : 1.366822, accuarcy : 0.666833\n",
      "total step : 422 \n",
      "error : 1.365531, accuarcy : 0.666833\n",
      "total step : 423 \n",
      "error : 1.364243, accuarcy : 0.666833\n",
      "total step : 424 \n",
      "error : 1.362959, accuarcy : 0.667334\n",
      "total step : 425 \n",
      "error : 1.361678, accuarcy : 0.667334\n",
      "total step : 426 \n",
      "error : 1.360401, accuarcy : 0.667334\n",
      "total step : 427 \n",
      "error : 1.359127, accuarcy : 0.667334\n",
      "total step : 428 \n",
      "error : 1.357856, accuarcy : 0.667334\n",
      "total step : 429 \n",
      "error : 1.356589, accuarcy : 0.667834\n",
      "total step : 430 \n",
      "error : 1.355326, accuarcy : 0.668334\n",
      "total step : 431 \n",
      "error : 1.354065, accuarcy : 0.668834\n",
      "total step : 432 \n",
      "error : 1.352808, accuarcy : 0.668834\n",
      "total step : 433 \n",
      "error : 1.351555, accuarcy : 0.668834\n",
      "total step : 434 \n",
      "error : 1.350305, accuarcy : 0.669835\n",
      "total step : 435 \n",
      "error : 1.349058, accuarcy : 0.669835\n",
      "total step : 436 \n",
      "error : 1.347814, accuarcy : 0.669835\n",
      "total step : 437 \n",
      "error : 1.346574, accuarcy : 0.669835\n",
      "total step : 438 \n",
      "error : 1.345337, accuarcy : 0.669835\n",
      "total step : 439 \n",
      "error : 1.344103, accuarcy : 0.669835\n",
      "total step : 440 \n",
      "error : 1.342873, accuarcy : 0.670335\n",
      "total step : 441 \n",
      "error : 1.341646, accuarcy : 0.670335\n",
      "total step : 442 \n",
      "error : 1.340422, accuarcy : 0.670835\n",
      "total step : 443 \n",
      "error : 1.339202, accuarcy : 0.670835\n",
      "total step : 444 \n",
      "error : 1.337984, accuarcy : 0.670835\n",
      "total step : 445 \n",
      "error : 1.336770, accuarcy : 0.670335\n",
      "total step : 446 \n",
      "error : 1.335559, accuarcy : 0.670835\n",
      "total step : 447 \n",
      "error : 1.334351, accuarcy : 0.670835\n",
      "total step : 448 \n",
      "error : 1.333147, accuarcy : 0.671336\n",
      "total step : 449 \n",
      "error : 1.331945, accuarcy : 0.671336\n",
      "total step : 450 \n",
      "error : 1.330747, accuarcy : 0.671336\n",
      "total step : 451 \n",
      "error : 1.329552, accuarcy : 0.671836\n",
      "total step : 452 \n",
      "error : 1.328360, accuarcy : 0.671836\n",
      "total step : 453 \n",
      "error : 1.327171, accuarcy : 0.671836\n",
      "total step : 454 \n",
      "error : 1.325985, accuarcy : 0.672336\n",
      "total step : 455 \n",
      "error : 1.324802, accuarcy : 0.672336\n",
      "total step : 456 \n",
      "error : 1.323623, accuarcy : 0.671836\n",
      "total step : 457 \n",
      "error : 1.322446, accuarcy : 0.671336\n",
      "total step : 458 \n",
      "error : 1.321273, accuarcy : 0.671836\n",
      "total step : 459 \n",
      "error : 1.320102, accuarcy : 0.672336\n",
      "total step : 460 \n",
      "error : 1.318935, accuarcy : 0.672336\n",
      "total step : 461 \n",
      "error : 1.317770, accuarcy : 0.672336\n",
      "total step : 462 \n",
      "error : 1.316609, accuarcy : 0.673337\n",
      "total step : 463 \n",
      "error : 1.315451, accuarcy : 0.674337\n",
      "total step : 464 \n",
      "error : 1.314295, accuarcy : 0.673837\n",
      "total step : 465 \n",
      "error : 1.313143, accuarcy : 0.674337\n",
      "total step : 466 \n",
      "error : 1.311994, accuarcy : 0.674337\n",
      "total step : 467 \n",
      "error : 1.310847, accuarcy : 0.674837\n",
      "total step : 468 \n",
      "error : 1.309704, accuarcy : 0.674837\n",
      "total step : 469 \n",
      "error : 1.308563, accuarcy : 0.674837\n",
      "total step : 470 \n",
      "error : 1.307426, accuarcy : 0.675338\n",
      "total step : 471 \n",
      "error : 1.306291, accuarcy : 0.675338\n",
      "total step : 472 \n",
      "error : 1.305159, accuarcy : 0.675338\n",
      "total step : 473 \n",
      "error : 1.304030, accuarcy : 0.675838\n",
      "total step : 474 \n",
      "error : 1.302904, accuarcy : 0.675838\n",
      "total step : 475 \n",
      "error : 1.301781, accuarcy : 0.675838\n",
      "total step : 476 \n",
      "error : 1.300661, accuarcy : 0.675838\n",
      "total step : 477 \n",
      "error : 1.299543, accuarcy : 0.676338\n",
      "total step : 478 \n",
      "error : 1.298429, accuarcy : 0.676838\n",
      "total step : 479 \n",
      "error : 1.297317, accuarcy : 0.677339\n",
      "total step : 480 \n",
      "error : 1.296208, accuarcy : 0.677339\n",
      "total step : 481 \n",
      "error : 1.295102, accuarcy : 0.677339\n",
      "total step : 482 \n",
      "error : 1.293999, accuarcy : 0.677339\n",
      "total step : 483 \n",
      "error : 1.292898, accuarcy : 0.677339\n",
      "total step : 484 \n",
      "error : 1.291801, accuarcy : 0.677339\n",
      "total step : 485 \n",
      "error : 1.290706, accuarcy : 0.677339\n",
      "total step : 486 \n",
      "error : 1.289614, accuarcy : 0.678339\n",
      "total step : 487 \n",
      "error : 1.288524, accuarcy : 0.678339\n",
      "total step : 488 \n",
      "error : 1.287438, accuarcy : 0.678839\n",
      "total step : 489 \n",
      "error : 1.286354, accuarcy : 0.679340\n",
      "total step : 490 \n",
      "error : 1.285272, accuarcy : 0.679340\n",
      "total step : 491 \n",
      "error : 1.284194, accuarcy : 0.679340\n",
      "total step : 492 \n",
      "error : 1.283118, accuarcy : 0.678839\n",
      "total step : 493 \n",
      "error : 1.282045, accuarcy : 0.678839\n",
      "total step : 494 \n",
      "error : 1.280975, accuarcy : 0.678839\n",
      "total step : 495 \n",
      "error : 1.279907, accuarcy : 0.678839\n",
      "total step : 496 \n",
      "error : 1.278842, accuarcy : 0.678839\n",
      "total step : 497 \n",
      "error : 1.277779, accuarcy : 0.678839\n",
      "total step : 498 \n",
      "error : 1.276720, accuarcy : 0.679340\n",
      "total step : 499 \n",
      "error : 1.275663, accuarcy : 0.680340\n",
      "total step : 500 \n",
      "error : 1.274608, accuarcy : 0.680340\n",
      "total step : 501 \n",
      "error : 1.273556, accuarcy : 0.680340\n",
      "total step : 502 \n",
      "error : 1.272507, accuarcy : 0.680340\n",
      "total step : 503 \n",
      "error : 1.271460, accuarcy : 0.680340\n",
      "total step : 504 \n",
      "error : 1.270416, accuarcy : 0.680840\n",
      "total step : 505 \n",
      "error : 1.269375, accuarcy : 0.680840\n",
      "total step : 506 \n",
      "error : 1.268336, accuarcy : 0.680840\n",
      "total step : 507 \n",
      "error : 1.267300, accuarcy : 0.681341\n",
      "total step : 508 \n",
      "error : 1.266266, accuarcy : 0.681341\n",
      "total step : 509 \n",
      "error : 1.265235, accuarcy : 0.681841\n",
      "total step : 510 \n",
      "error : 1.264206, accuarcy : 0.682341\n",
      "total step : 511 \n",
      "error : 1.263180, accuarcy : 0.682341\n",
      "total step : 512 \n",
      "error : 1.262156, accuarcy : 0.682841\n",
      "total step : 513 \n",
      "error : 1.261135, accuarcy : 0.683342\n",
      "total step : 514 \n",
      "error : 1.260117, accuarcy : 0.683342\n",
      "total step : 515 \n",
      "error : 1.259101, accuarcy : 0.684342\n",
      "total step : 516 \n",
      "error : 1.258087, accuarcy : 0.684842\n",
      "total step : 517 \n",
      "error : 1.257076, accuarcy : 0.684842\n",
      "total step : 518 \n",
      "error : 1.256067, accuarcy : 0.684842\n",
      "total step : 519 \n",
      "error : 1.255061, accuarcy : 0.684842\n",
      "total step : 520 \n",
      "error : 1.254057, accuarcy : 0.685343\n",
      "total step : 521 \n",
      "error : 1.253056, accuarcy : 0.685843\n",
      "total step : 522 \n",
      "error : 1.252057, accuarcy : 0.685843\n",
      "total step : 523 \n",
      "error : 1.251061, accuarcy : 0.685843\n",
      "total step : 524 \n",
      "error : 1.250067, accuarcy : 0.685843\n",
      "total step : 525 \n",
      "error : 1.249075, accuarcy : 0.685843\n",
      "total step : 526 \n",
      "error : 1.248086, accuarcy : 0.686343\n",
      "total step : 527 \n",
      "error : 1.247099, accuarcy : 0.686843\n",
      "total step : 528 \n",
      "error : 1.246115, accuarcy : 0.686843\n",
      "total step : 529 \n",
      "error : 1.245132, accuarcy : 0.686843\n",
      "total step : 530 \n",
      "error : 1.244153, accuarcy : 0.686843\n",
      "total step : 531 \n",
      "error : 1.243176, accuarcy : 0.687844\n",
      "total step : 532 \n",
      "error : 1.242201, accuarcy : 0.687844\n",
      "total step : 533 \n",
      "error : 1.241228, accuarcy : 0.688344\n",
      "total step : 534 \n",
      "error : 1.240258, accuarcy : 0.688344\n",
      "total step : 535 \n",
      "error : 1.239290, accuarcy : 0.688344\n",
      "total step : 536 \n",
      "error : 1.238324, accuarcy : 0.688344\n",
      "total step : 537 \n",
      "error : 1.237361, accuarcy : 0.688344\n",
      "total step : 538 \n",
      "error : 1.236400, accuarcy : 0.688344\n",
      "total step : 539 \n",
      "error : 1.235441, accuarcy : 0.688344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 540 \n",
      "error : 1.234485, accuarcy : 0.688344\n",
      "total step : 541 \n",
      "error : 1.233531, accuarcy : 0.688844\n",
      "total step : 542 \n",
      "error : 1.232579, accuarcy : 0.688844\n",
      "total step : 543 \n",
      "error : 1.231629, accuarcy : 0.689345\n",
      "total step : 544 \n",
      "error : 1.230682, accuarcy : 0.689345\n",
      "total step : 545 \n",
      "error : 1.229737, accuarcy : 0.689345\n",
      "total step : 546 \n",
      "error : 1.228794, accuarcy : 0.689345\n",
      "total step : 547 \n",
      "error : 1.227854, accuarcy : 0.689345\n",
      "total step : 548 \n",
      "error : 1.226915, accuarcy : 0.688844\n",
      "total step : 549 \n",
      "error : 1.225979, accuarcy : 0.689345\n",
      "total step : 550 \n",
      "error : 1.225045, accuarcy : 0.689345\n",
      "total step : 551 \n",
      "error : 1.224114, accuarcy : 0.689345\n",
      "total step : 552 \n",
      "error : 1.223184, accuarcy : 0.689345\n",
      "total step : 553 \n",
      "error : 1.222257, accuarcy : 0.689345\n",
      "total step : 554 \n",
      "error : 1.221332, accuarcy : 0.689345\n",
      "total step : 555 \n",
      "error : 1.220409, accuarcy : 0.689345\n",
      "total step : 556 \n",
      "error : 1.219488, accuarcy : 0.689345\n",
      "total step : 557 \n",
      "error : 1.218570, accuarcy : 0.689345\n",
      "total step : 558 \n",
      "error : 1.217653, accuarcy : 0.689845\n",
      "total step : 559 \n",
      "error : 1.216739, accuarcy : 0.689845\n",
      "total step : 560 \n",
      "error : 1.215827, accuarcy : 0.690345\n",
      "total step : 561 \n",
      "error : 1.214917, accuarcy : 0.690345\n",
      "total step : 562 \n",
      "error : 1.214009, accuarcy : 0.690345\n",
      "total step : 563 \n",
      "error : 1.213103, accuarcy : 0.690345\n",
      "total step : 564 \n",
      "error : 1.212200, accuarcy : 0.690845\n",
      "total step : 565 \n",
      "error : 1.211298, accuarcy : 0.690845\n",
      "total step : 566 \n",
      "error : 1.210399, accuarcy : 0.690845\n",
      "total step : 567 \n",
      "error : 1.209502, accuarcy : 0.690845\n",
      "total step : 568 \n",
      "error : 1.208606, accuarcy : 0.691346\n",
      "total step : 569 \n",
      "error : 1.207713, accuarcy : 0.691346\n",
      "total step : 570 \n",
      "error : 1.206822, accuarcy : 0.691346\n",
      "total step : 571 \n",
      "error : 1.205933, accuarcy : 0.691346\n",
      "total step : 572 \n",
      "error : 1.205046, accuarcy : 0.691846\n",
      "total step : 573 \n",
      "error : 1.204162, accuarcy : 0.691846\n",
      "total step : 574 \n",
      "error : 1.203279, accuarcy : 0.691846\n",
      "total step : 575 \n",
      "error : 1.202398, accuarcy : 0.691346\n",
      "total step : 576 \n",
      "error : 1.201519, accuarcy : 0.691346\n",
      "total step : 577 \n",
      "error : 1.200643, accuarcy : 0.691346\n",
      "total step : 578 \n",
      "error : 1.199768, accuarcy : 0.691346\n",
      "total step : 579 \n",
      "error : 1.198895, accuarcy : 0.691346\n",
      "total step : 580 \n",
      "error : 1.198025, accuarcy : 0.690845\n",
      "total step : 581 \n",
      "error : 1.197156, accuarcy : 0.690845\n",
      "total step : 582 \n",
      "error : 1.196289, accuarcy : 0.690845\n",
      "total step : 583 \n",
      "error : 1.195425, accuarcy : 0.690845\n",
      "total step : 584 \n",
      "error : 1.194562, accuarcy : 0.690845\n",
      "total step : 585 \n",
      "error : 1.193701, accuarcy : 0.690845\n",
      "total step : 586 \n",
      "error : 1.192843, accuarcy : 0.690845\n",
      "total step : 587 \n",
      "error : 1.191986, accuarcy : 0.690845\n",
      "total step : 588 \n",
      "error : 1.191131, accuarcy : 0.690845\n",
      "total step : 589 \n",
      "error : 1.190278, accuarcy : 0.690845\n",
      "total step : 590 \n",
      "error : 1.189427, accuarcy : 0.690845\n",
      "total step : 591 \n",
      "error : 1.188578, accuarcy : 0.690845\n",
      "total step : 592 \n",
      "error : 1.187731, accuarcy : 0.690845\n",
      "total step : 593 \n",
      "error : 1.186886, accuarcy : 0.690845\n",
      "total step : 594 \n",
      "error : 1.186043, accuarcy : 0.690845\n",
      "total step : 595 \n",
      "error : 1.185202, accuarcy : 0.690845\n",
      "total step : 596 \n",
      "error : 1.184362, accuarcy : 0.690845\n",
      "total step : 597 \n",
      "error : 1.183525, accuarcy : 0.690845\n",
      "total step : 598 \n",
      "error : 1.182689, accuarcy : 0.690845\n",
      "total step : 599 \n",
      "error : 1.181856, accuarcy : 0.690845\n",
      "total step : 600 \n",
      "error : 1.181024, accuarcy : 0.690845\n",
      "total step : 601 \n",
      "error : 1.180194, accuarcy : 0.690845\n",
      "total step : 602 \n",
      "error : 1.179366, accuarcy : 0.690845\n",
      "total step : 603 \n",
      "error : 1.178540, accuarcy : 0.690845\n",
      "total step : 604 \n",
      "error : 1.177715, accuarcy : 0.691346\n",
      "total step : 605 \n",
      "error : 1.176893, accuarcy : 0.691846\n",
      "total step : 606 \n",
      "error : 1.176072, accuarcy : 0.691846\n",
      "total step : 607 \n",
      "error : 1.175253, accuarcy : 0.692346\n",
      "total step : 608 \n",
      "error : 1.174436, accuarcy : 0.692846\n",
      "total step : 609 \n",
      "error : 1.173621, accuarcy : 0.692846\n",
      "total step : 610 \n",
      "error : 1.172808, accuarcy : 0.692846\n",
      "total step : 611 \n",
      "error : 1.171997, accuarcy : 0.692846\n",
      "total step : 612 \n",
      "error : 1.171187, accuarcy : 0.693347\n",
      "total step : 613 \n",
      "error : 1.170379, accuarcy : 0.693347\n",
      "total step : 614 \n",
      "error : 1.169573, accuarcy : 0.693847\n",
      "total step : 615 \n",
      "error : 1.168769, accuarcy : 0.693847\n",
      "total step : 616 \n",
      "error : 1.167966, accuarcy : 0.693847\n",
      "total step : 617 \n",
      "error : 1.167166, accuarcy : 0.694347\n",
      "total step : 618 \n",
      "error : 1.166367, accuarcy : 0.694347\n",
      "total step : 619 \n",
      "error : 1.165570, accuarcy : 0.694347\n",
      "total step : 620 \n",
      "error : 1.164774, accuarcy : 0.693847\n",
      "total step : 621 \n",
      "error : 1.163981, accuarcy : 0.693847\n",
      "total step : 622 \n",
      "error : 1.163189, accuarcy : 0.694347\n",
      "total step : 623 \n",
      "error : 1.162399, accuarcy : 0.694347\n",
      "total step : 624 \n",
      "error : 1.161610, accuarcy : 0.694847\n",
      "total step : 625 \n",
      "error : 1.160824, accuarcy : 0.694847\n",
      "total step : 626 \n",
      "error : 1.160039, accuarcy : 0.694847\n",
      "total step : 627 \n",
      "error : 1.159256, accuarcy : 0.694847\n",
      "total step : 628 \n",
      "error : 1.158474, accuarcy : 0.694847\n",
      "total step : 629 \n",
      "error : 1.157695, accuarcy : 0.694847\n",
      "total step : 630 \n",
      "error : 1.156917, accuarcy : 0.694847\n",
      "total step : 631 \n",
      "error : 1.156140, accuarcy : 0.694847\n",
      "total step : 632 \n",
      "error : 1.155366, accuarcy : 0.694847\n",
      "total step : 633 \n",
      "error : 1.154593, accuarcy : 0.694847\n",
      "total step : 634 \n",
      "error : 1.153822, accuarcy : 0.694847\n",
      "total step : 635 \n",
      "error : 1.153052, accuarcy : 0.694847\n",
      "total step : 636 \n",
      "error : 1.152284, accuarcy : 0.694847\n",
      "total step : 637 \n",
      "error : 1.151518, accuarcy : 0.695348\n",
      "total step : 638 \n",
      "error : 1.150754, accuarcy : 0.695348\n",
      "total step : 639 \n",
      "error : 1.149991, accuarcy : 0.695348\n",
      "total step : 640 \n",
      "error : 1.149230, accuarcy : 0.695348\n",
      "total step : 641 \n",
      "error : 1.148470, accuarcy : 0.695848\n",
      "total step : 642 \n",
      "error : 1.147713, accuarcy : 0.695848\n",
      "total step : 643 \n",
      "error : 1.146956, accuarcy : 0.695848\n",
      "total step : 644 \n",
      "error : 1.146202, accuarcy : 0.696848\n",
      "total step : 645 \n",
      "error : 1.145449, accuarcy : 0.696848\n",
      "total step : 646 \n",
      "error : 1.144698, accuarcy : 0.697849\n",
      "total step : 647 \n",
      "error : 1.143948, accuarcy : 0.698349\n",
      "total step : 648 \n",
      "error : 1.143200, accuarcy : 0.698849\n",
      "total step : 649 \n",
      "error : 1.142453, accuarcy : 0.699350\n",
      "total step : 650 \n",
      "error : 1.141709, accuarcy : 0.699850\n",
      "total step : 651 \n",
      "error : 1.140965, accuarcy : 0.699850\n",
      "total step : 652 \n",
      "error : 1.140224, accuarcy : 0.699850\n",
      "total step : 653 \n",
      "error : 1.139484, accuarcy : 0.700350\n",
      "total step : 654 \n",
      "error : 1.138745, accuarcy : 0.700350\n",
      "total step : 655 \n",
      "error : 1.138008, accuarcy : 0.700350\n",
      "total step : 656 \n",
      "error : 1.137273, accuarcy : 0.700350\n",
      "total step : 657 \n",
      "error : 1.136539, accuarcy : 0.700350\n",
      "total step : 658 \n",
      "error : 1.135807, accuarcy : 0.700350\n",
      "total step : 659 \n",
      "error : 1.135077, accuarcy : 0.700350\n",
      "total step : 660 \n",
      "error : 1.134348, accuarcy : 0.700850\n",
      "total step : 661 \n",
      "error : 1.133620, accuarcy : 0.700850\n",
      "total step : 662 \n",
      "error : 1.132894, accuarcy : 0.700850\n",
      "total step : 663 \n",
      "error : 1.132170, accuarcy : 0.701351\n",
      "total step : 664 \n",
      "error : 1.131447, accuarcy : 0.701851\n",
      "total step : 665 \n",
      "error : 1.130726, accuarcy : 0.701851\n",
      "total step : 666 \n",
      "error : 1.130006, accuarcy : 0.701851\n",
      "total step : 667 \n",
      "error : 1.129288, accuarcy : 0.702351\n",
      "total step : 668 \n",
      "error : 1.128571, accuarcy : 0.702351\n",
      "total step : 669 \n",
      "error : 1.127856, accuarcy : 0.702851\n",
      "total step : 670 \n",
      "error : 1.127142, accuarcy : 0.703852\n",
      "total step : 671 \n",
      "error : 1.126430, accuarcy : 0.703852\n",
      "total step : 672 \n",
      "error : 1.125719, accuarcy : 0.703852\n",
      "total step : 673 \n",
      "error : 1.125010, accuarcy : 0.703852\n",
      "total step : 674 \n",
      "error : 1.124302, accuarcy : 0.703352\n",
      "total step : 675 \n",
      "error : 1.123596, accuarcy : 0.703352\n",
      "total step : 676 \n",
      "error : 1.122892, accuarcy : 0.703352\n",
      "total step : 677 \n",
      "error : 1.122188, accuarcy : 0.703852\n",
      "total step : 678 \n",
      "error : 1.121487, accuarcy : 0.703852\n",
      "total step : 679 \n",
      "error : 1.120786, accuarcy : 0.703852\n",
      "total step : 680 \n",
      "error : 1.120087, accuarcy : 0.703852\n",
      "total step : 681 \n",
      "error : 1.119390, accuarcy : 0.703852\n",
      "total step : 682 \n",
      "error : 1.118694, accuarcy : 0.703852\n",
      "total step : 683 \n",
      "error : 1.118000, accuarcy : 0.703852\n",
      "total step : 684 \n",
      "error : 1.117307, accuarcy : 0.704352\n",
      "total step : 685 \n",
      "error : 1.116615, accuarcy : 0.704352\n",
      "total step : 686 \n",
      "error : 1.115925, accuarcy : 0.704352\n",
      "total step : 687 \n",
      "error : 1.115236, accuarcy : 0.704352\n",
      "total step : 688 \n",
      "error : 1.114549, accuarcy : 0.704352\n",
      "total step : 689 \n",
      "error : 1.113863, accuarcy : 0.704852\n",
      "total step : 690 \n",
      "error : 1.113179, accuarcy : 0.704852\n",
      "total step : 691 \n",
      "error : 1.112496, accuarcy : 0.704852\n",
      "total step : 692 \n",
      "error : 1.111814, accuarcy : 0.705353\n",
      "total step : 693 \n",
      "error : 1.111134, accuarcy : 0.705353\n",
      "total step : 694 \n",
      "error : 1.110455, accuarcy : 0.705353\n",
      "total step : 695 \n",
      "error : 1.109778, accuarcy : 0.706353\n",
      "total step : 696 \n",
      "error : 1.109102, accuarcy : 0.706353\n",
      "total step : 697 \n",
      "error : 1.108428, accuarcy : 0.706353\n",
      "total step : 698 \n",
      "error : 1.107754, accuarcy : 0.706353\n",
      "total step : 699 \n",
      "error : 1.107083, accuarcy : 0.706353\n",
      "total step : 700 \n",
      "error : 1.106412, accuarcy : 0.706353\n",
      "total step : 701 \n",
      "error : 1.105743, accuarcy : 0.706353\n",
      "total step : 702 \n",
      "error : 1.105075, accuarcy : 0.706853\n",
      "total step : 703 \n",
      "error : 1.104409, accuarcy : 0.706853\n",
      "total step : 704 \n",
      "error : 1.103744, accuarcy : 0.706853\n",
      "total step : 705 \n",
      "error : 1.103081, accuarcy : 0.707354\n",
      "total step : 706 \n",
      "error : 1.102418, accuarcy : 0.707354\n",
      "total step : 707 \n",
      "error : 1.101758, accuarcy : 0.707354\n",
      "total step : 708 \n",
      "error : 1.101098, accuarcy : 0.707354\n",
      "total step : 709 \n",
      "error : 1.100440, accuarcy : 0.707354\n",
      "total step : 710 \n",
      "error : 1.099783, accuarcy : 0.707354\n",
      "total step : 711 \n",
      "error : 1.099127, accuarcy : 0.707354\n",
      "total step : 712 \n",
      "error : 1.098473, accuarcy : 0.707854\n",
      "total step : 713 \n",
      "error : 1.097820, accuarcy : 0.707854\n",
      "total step : 714 \n",
      "error : 1.097169, accuarcy : 0.708354\n",
      "total step : 715 \n",
      "error : 1.096519, accuarcy : 0.708354\n",
      "total step : 716 \n",
      "error : 1.095870, accuarcy : 0.708354\n",
      "total step : 717 \n",
      "error : 1.095222, accuarcy : 0.708854\n",
      "total step : 718 \n",
      "error : 1.094576, accuarcy : 0.708854\n",
      "total step : 719 \n",
      "error : 1.093931, accuarcy : 0.708854\n",
      "total step : 720 \n",
      "error : 1.093287, accuarcy : 0.708854\n",
      "total step : 721 \n",
      "error : 1.092645, accuarcy : 0.709355\n",
      "total step : 722 \n",
      "error : 1.092004, accuarcy : 0.709355\n",
      "total step : 723 \n",
      "error : 1.091364, accuarcy : 0.709355\n",
      "total step : 724 \n",
      "error : 1.090725, accuarcy : 0.709355\n",
      "total step : 725 \n",
      "error : 1.090088, accuarcy : 0.709355\n",
      "total step : 726 \n",
      "error : 1.089452, accuarcy : 0.709355\n",
      "total step : 727 \n",
      "error : 1.088817, accuarcy : 0.709355\n",
      "total step : 728 \n",
      "error : 1.088184, accuarcy : 0.709355\n",
      "total step : 729 \n",
      "error : 1.087552, accuarcy : 0.709355\n",
      "total step : 730 \n",
      "error : 1.086921, accuarcy : 0.709855\n",
      "total step : 731 \n",
      "error : 1.086291, accuarcy : 0.709855\n",
      "total step : 732 \n",
      "error : 1.085663, accuarcy : 0.709855\n",
      "total step : 733 \n",
      "error : 1.085035, accuarcy : 0.709855\n",
      "total step : 734 \n",
      "error : 1.084409, accuarcy : 0.709855\n",
      "total step : 735 \n",
      "error : 1.083785, accuarcy : 0.709855\n",
      "total step : 736 \n",
      "error : 1.083161, accuarcy : 0.709855\n",
      "total step : 737 \n",
      "error : 1.082539, accuarcy : 0.709855\n",
      "total step : 738 \n",
      "error : 1.081918, accuarcy : 0.709855\n",
      "total step : 739 \n",
      "error : 1.081298, accuarcy : 0.710355\n",
      "total step : 740 \n",
      "error : 1.080680, accuarcy : 0.710855\n",
      "total step : 741 \n",
      "error : 1.080063, accuarcy : 0.710855\n",
      "total step : 742 \n",
      "error : 1.079446, accuarcy : 0.711856\n",
      "total step : 743 \n",
      "error : 1.078832, accuarcy : 0.711856\n",
      "total step : 744 \n",
      "error : 1.078218, accuarcy : 0.712856\n",
      "total step : 745 \n",
      "error : 1.077605, accuarcy : 0.712856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 746 \n",
      "error : 1.076994, accuarcy : 0.713357\n",
      "total step : 747 \n",
      "error : 1.076384, accuarcy : 0.713357\n",
      "total step : 748 \n",
      "error : 1.075775, accuarcy : 0.713857\n",
      "total step : 749 \n",
      "error : 1.075167, accuarcy : 0.713857\n",
      "total step : 750 \n",
      "error : 1.074561, accuarcy : 0.713857\n",
      "total step : 751 \n",
      "error : 1.073956, accuarcy : 0.714357\n",
      "total step : 752 \n",
      "error : 1.073351, accuarcy : 0.714857\n",
      "total step : 753 \n",
      "error : 1.072748, accuarcy : 0.714857\n",
      "total step : 754 \n",
      "error : 1.072147, accuarcy : 0.714857\n",
      "total step : 755 \n",
      "error : 1.071546, accuarcy : 0.715358\n",
      "total step : 756 \n",
      "error : 1.070947, accuarcy : 0.715358\n",
      "total step : 757 \n",
      "error : 1.070348, accuarcy : 0.715858\n",
      "total step : 758 \n",
      "error : 1.069751, accuarcy : 0.715858\n",
      "total step : 759 \n",
      "error : 1.069155, accuarcy : 0.716358\n",
      "total step : 760 \n",
      "error : 1.068560, accuarcy : 0.716358\n",
      "total step : 761 \n",
      "error : 1.067967, accuarcy : 0.716358\n",
      "total step : 762 \n",
      "error : 1.067374, accuarcy : 0.716358\n",
      "total step : 763 \n",
      "error : 1.066783, accuarcy : 0.716358\n",
      "total step : 764 \n",
      "error : 1.066192, accuarcy : 0.716858\n",
      "total step : 765 \n",
      "error : 1.065603, accuarcy : 0.716858\n",
      "total step : 766 \n",
      "error : 1.065015, accuarcy : 0.717359\n",
      "total step : 767 \n",
      "error : 1.064429, accuarcy : 0.717859\n",
      "total step : 768 \n",
      "error : 1.063843, accuarcy : 0.717859\n",
      "total step : 769 \n",
      "error : 1.063258, accuarcy : 0.717859\n",
      "total step : 770 \n",
      "error : 1.062675, accuarcy : 0.717859\n",
      "total step : 771 \n",
      "error : 1.062092, accuarcy : 0.717859\n",
      "total step : 772 \n",
      "error : 1.061511, accuarcy : 0.717859\n",
      "total step : 773 \n",
      "error : 1.060931, accuarcy : 0.718859\n",
      "total step : 774 \n",
      "error : 1.060352, accuarcy : 0.718859\n",
      "total step : 775 \n",
      "error : 1.059774, accuarcy : 0.718859\n",
      "total step : 776 \n",
      "error : 1.059197, accuarcy : 0.718859\n",
      "total step : 777 \n",
      "error : 1.058621, accuarcy : 0.718859\n",
      "total step : 778 \n",
      "error : 1.058047, accuarcy : 0.719360\n",
      "total step : 779 \n",
      "error : 1.057473, accuarcy : 0.719860\n",
      "total step : 780 \n",
      "error : 1.056901, accuarcy : 0.719860\n",
      "total step : 781 \n",
      "error : 1.056329, accuarcy : 0.720360\n",
      "total step : 782 \n",
      "error : 1.055759, accuarcy : 0.720360\n",
      "total step : 783 \n",
      "error : 1.055190, accuarcy : 0.720360\n",
      "total step : 784 \n",
      "error : 1.054621, accuarcy : 0.720860\n",
      "total step : 785 \n",
      "error : 1.054054, accuarcy : 0.720860\n",
      "total step : 786 \n",
      "error : 1.053488, accuarcy : 0.720860\n",
      "total step : 787 \n",
      "error : 1.052923, accuarcy : 0.721361\n",
      "total step : 788 \n",
      "error : 1.052359, accuarcy : 0.721361\n",
      "total step : 789 \n",
      "error : 1.051797, accuarcy : 0.721361\n",
      "total step : 790 \n",
      "error : 1.051235, accuarcy : 0.722361\n",
      "total step : 791 \n",
      "error : 1.050674, accuarcy : 0.722361\n",
      "total step : 792 \n",
      "error : 1.050114, accuarcy : 0.722361\n",
      "total step : 793 \n",
      "error : 1.049556, accuarcy : 0.722361\n",
      "total step : 794 \n",
      "error : 1.048998, accuarcy : 0.721861\n",
      "total step : 795 \n",
      "error : 1.048442, accuarcy : 0.721861\n",
      "total step : 796 \n",
      "error : 1.047886, accuarcy : 0.721861\n",
      "total step : 797 \n",
      "error : 1.047332, accuarcy : 0.721861\n",
      "total step : 798 \n",
      "error : 1.046778, accuarcy : 0.721861\n",
      "total step : 799 \n",
      "error : 1.046226, accuarcy : 0.722361\n",
      "total step : 800 \n",
      "error : 1.045674, accuarcy : 0.722361\n",
      "total step : 801 \n",
      "error : 1.045124, accuarcy : 0.722361\n",
      "total step : 802 \n",
      "error : 1.044575, accuarcy : 0.722861\n",
      "total step : 803 \n",
      "error : 1.044026, accuarcy : 0.722861\n",
      "total step : 804 \n",
      "error : 1.043479, accuarcy : 0.722861\n",
      "total step : 805 \n",
      "error : 1.042933, accuarcy : 0.723362\n",
      "total step : 806 \n",
      "error : 1.042387, accuarcy : 0.723362\n",
      "total step : 807 \n",
      "error : 1.041843, accuarcy : 0.723362\n",
      "total step : 808 \n",
      "error : 1.041300, accuarcy : 0.723362\n",
      "total step : 809 \n",
      "error : 1.040757, accuarcy : 0.723362\n",
      "total step : 810 \n",
      "error : 1.040216, accuarcy : 0.723362\n",
      "total step : 811 \n",
      "error : 1.039676, accuarcy : 0.723362\n",
      "total step : 812 \n",
      "error : 1.039136, accuarcy : 0.723362\n",
      "total step : 813 \n",
      "error : 1.038598, accuarcy : 0.723362\n",
      "total step : 814 \n",
      "error : 1.038061, accuarcy : 0.723362\n",
      "total step : 815 \n",
      "error : 1.037524, accuarcy : 0.723362\n",
      "total step : 816 \n",
      "error : 1.036989, accuarcy : 0.723362\n",
      "total step : 817 \n",
      "error : 1.036454, accuarcy : 0.723362\n",
      "total step : 818 \n",
      "error : 1.035921, accuarcy : 0.723862\n",
      "total step : 819 \n",
      "error : 1.035388, accuarcy : 0.723862\n",
      "total step : 820 \n",
      "error : 1.034857, accuarcy : 0.723862\n",
      "total step : 821 \n",
      "error : 1.034326, accuarcy : 0.723862\n",
      "total step : 822 \n",
      "error : 1.033797, accuarcy : 0.723862\n",
      "total step : 823 \n",
      "error : 1.033268, accuarcy : 0.723862\n",
      "total step : 824 \n",
      "error : 1.032741, accuarcy : 0.723862\n",
      "total step : 825 \n",
      "error : 1.032214, accuarcy : 0.723862\n",
      "total step : 826 \n",
      "error : 1.031688, accuarcy : 0.723862\n",
      "total step : 827 \n",
      "error : 1.031163, accuarcy : 0.723862\n",
      "total step : 828 \n",
      "error : 1.030639, accuarcy : 0.723862\n",
      "total step : 829 \n",
      "error : 1.030116, accuarcy : 0.723862\n",
      "total step : 830 \n",
      "error : 1.029594, accuarcy : 0.723862\n",
      "total step : 831 \n",
      "error : 1.029073, accuarcy : 0.724362\n",
      "total step : 832 \n",
      "error : 1.028553, accuarcy : 0.724362\n",
      "total step : 833 \n",
      "error : 1.028034, accuarcy : 0.724862\n",
      "total step : 834 \n",
      "error : 1.027516, accuarcy : 0.724862\n",
      "total step : 835 \n",
      "error : 1.026998, accuarcy : 0.725363\n",
      "total step : 836 \n",
      "error : 1.026482, accuarcy : 0.725363\n",
      "total step : 837 \n",
      "error : 1.025967, accuarcy : 0.725863\n",
      "total step : 838 \n",
      "error : 1.025452, accuarcy : 0.725863\n",
      "total step : 839 \n",
      "error : 1.024938, accuarcy : 0.726363\n",
      "total step : 840 \n",
      "error : 1.024426, accuarcy : 0.726363\n",
      "total step : 841 \n",
      "error : 1.023914, accuarcy : 0.726363\n",
      "total step : 842 \n",
      "error : 1.023403, accuarcy : 0.726363\n",
      "total step : 843 \n",
      "error : 1.022893, accuarcy : 0.726363\n",
      "total step : 844 \n",
      "error : 1.022384, accuarcy : 0.726363\n",
      "total step : 845 \n",
      "error : 1.021876, accuarcy : 0.726363\n",
      "total step : 846 \n",
      "error : 1.021368, accuarcy : 0.726863\n",
      "total step : 847 \n",
      "error : 1.020862, accuarcy : 0.727364\n",
      "total step : 848 \n",
      "error : 1.020356, accuarcy : 0.727364\n",
      "total step : 849 \n",
      "error : 1.019852, accuarcy : 0.727364\n",
      "total step : 850 \n",
      "error : 1.019348, accuarcy : 0.727364\n",
      "total step : 851 \n",
      "error : 1.018845, accuarcy : 0.727364\n",
      "total step : 852 \n",
      "error : 1.018343, accuarcy : 0.727364\n",
      "total step : 853 \n",
      "error : 1.017842, accuarcy : 0.727864\n",
      "total step : 854 \n",
      "error : 1.017342, accuarcy : 0.727864\n",
      "total step : 855 \n",
      "error : 1.016843, accuarcy : 0.727864\n",
      "total step : 856 \n",
      "error : 1.016344, accuarcy : 0.727864\n",
      "total step : 857 \n",
      "error : 1.015847, accuarcy : 0.728364\n",
      "total step : 858 \n",
      "error : 1.015350, accuarcy : 0.728364\n",
      "total step : 859 \n",
      "error : 1.014854, accuarcy : 0.728864\n",
      "total step : 860 \n",
      "error : 1.014359, accuarcy : 0.729865\n",
      "total step : 861 \n",
      "error : 1.013865, accuarcy : 0.729865\n",
      "total step : 862 \n",
      "error : 1.013372, accuarcy : 0.729865\n",
      "total step : 863 \n",
      "error : 1.012879, accuarcy : 0.729865\n",
      "total step : 864 \n",
      "error : 1.012388, accuarcy : 0.729865\n",
      "total step : 865 \n",
      "error : 1.011897, accuarcy : 0.729865\n",
      "total step : 866 \n",
      "error : 1.011407, accuarcy : 0.729865\n",
      "total step : 867 \n",
      "error : 1.010918, accuarcy : 0.729865\n",
      "total step : 868 \n",
      "error : 1.010430, accuarcy : 0.729865\n",
      "total step : 869 \n",
      "error : 1.009943, accuarcy : 0.729865\n",
      "total step : 870 \n",
      "error : 1.009456, accuarcy : 0.729865\n",
      "total step : 871 \n",
      "error : 1.008970, accuarcy : 0.729865\n",
      "total step : 872 \n",
      "error : 1.008486, accuarcy : 0.729865\n",
      "total step : 873 \n",
      "error : 1.008002, accuarcy : 0.729865\n",
      "total step : 874 \n",
      "error : 1.007518, accuarcy : 0.729865\n",
      "total step : 875 \n",
      "error : 1.007036, accuarcy : 0.729865\n",
      "total step : 876 \n",
      "error : 1.006555, accuarcy : 0.730365\n",
      "total step : 877 \n",
      "error : 1.006074, accuarcy : 0.730365\n",
      "total step : 878 \n",
      "error : 1.005594, accuarcy : 0.729865\n",
      "total step : 879 \n",
      "error : 1.005115, accuarcy : 0.729365\n",
      "total step : 880 \n",
      "error : 1.004637, accuarcy : 0.729365\n",
      "total step : 881 \n",
      "error : 1.004159, accuarcy : 0.729365\n",
      "total step : 882 \n",
      "error : 1.003683, accuarcy : 0.729365\n",
      "total step : 883 \n",
      "error : 1.003207, accuarcy : 0.729865\n",
      "total step : 884 \n",
      "error : 1.002732, accuarcy : 0.729865\n",
      "total step : 885 \n",
      "error : 1.002258, accuarcy : 0.729865\n",
      "total step : 886 \n",
      "error : 1.001784, accuarcy : 0.729865\n",
      "total step : 887 \n",
      "error : 1.001312, accuarcy : 0.729865\n",
      "total step : 888 \n",
      "error : 1.000840, accuarcy : 0.729865\n",
      "total step : 889 \n",
      "error : 1.000369, accuarcy : 0.729865\n",
      "total step : 890 \n",
      "error : 0.999899, accuarcy : 0.729865\n",
      "total step : 891 \n",
      "error : 0.999430, accuarcy : 0.729865\n",
      "total step : 892 \n",
      "error : 0.998961, accuarcy : 0.729865\n",
      "total step : 893 \n",
      "error : 0.998493, accuarcy : 0.729865\n",
      "total step : 894 \n",
      "error : 0.998026, accuarcy : 0.730365\n",
      "total step : 895 \n",
      "error : 0.997560, accuarcy : 0.730865\n",
      "total step : 896 \n",
      "error : 0.997094, accuarcy : 0.730865\n",
      "total step : 897 \n",
      "error : 0.996630, accuarcy : 0.730865\n",
      "total step : 898 \n",
      "error : 0.996166, accuarcy : 0.730865\n",
      "total step : 899 \n",
      "error : 0.995703, accuarcy : 0.731366\n",
      "total step : 900 \n",
      "error : 0.995240, accuarcy : 0.731366\n",
      "total step : 901 \n",
      "error : 0.994779, accuarcy : 0.731866\n",
      "total step : 902 \n",
      "error : 0.994318, accuarcy : 0.732366\n",
      "total step : 903 \n",
      "error : 0.993858, accuarcy : 0.732366\n",
      "total step : 904 \n",
      "error : 0.993398, accuarcy : 0.732366\n",
      "total step : 905 \n",
      "error : 0.992940, accuarcy : 0.732366\n",
      "total step : 906 \n",
      "error : 0.992482, accuarcy : 0.732366\n",
      "total step : 907 \n",
      "error : 0.992025, accuarcy : 0.732366\n",
      "total step : 908 \n",
      "error : 0.991569, accuarcy : 0.732866\n",
      "total step : 909 \n",
      "error : 0.991113, accuarcy : 0.732866\n",
      "total step : 910 \n",
      "error : 0.990659, accuarcy : 0.732866\n",
      "total step : 911 \n",
      "error : 0.990205, accuarcy : 0.732866\n",
      "total step : 912 \n",
      "error : 0.989751, accuarcy : 0.732866\n",
      "total step : 913 \n",
      "error : 0.989299, accuarcy : 0.732866\n",
      "total step : 914 \n",
      "error : 0.988847, accuarcy : 0.733367\n",
      "total step : 915 \n",
      "error : 0.988396, accuarcy : 0.733367\n",
      "total step : 916 \n",
      "error : 0.987946, accuarcy : 0.733867\n",
      "total step : 917 \n",
      "error : 0.987496, accuarcy : 0.734367\n",
      "total step : 918 \n",
      "error : 0.987047, accuarcy : 0.734867\n",
      "total step : 919 \n",
      "error : 0.986599, accuarcy : 0.734867\n",
      "total step : 920 \n",
      "error : 0.986152, accuarcy : 0.734867\n",
      "total step : 921 \n",
      "error : 0.985705, accuarcy : 0.735368\n",
      "total step : 922 \n",
      "error : 0.985259, accuarcy : 0.735368\n",
      "total step : 923 \n",
      "error : 0.984814, accuarcy : 0.735368\n",
      "total step : 924 \n",
      "error : 0.984369, accuarcy : 0.735368\n",
      "total step : 925 \n",
      "error : 0.983926, accuarcy : 0.735868\n",
      "total step : 926 \n",
      "error : 0.983483, accuarcy : 0.736368\n",
      "total step : 927 \n",
      "error : 0.983040, accuarcy : 0.736368\n",
      "total step : 928 \n",
      "error : 0.982599, accuarcy : 0.736868\n",
      "total step : 929 \n",
      "error : 0.982158, accuarcy : 0.736368\n",
      "total step : 930 \n",
      "error : 0.981718, accuarcy : 0.736368\n",
      "total step : 931 \n",
      "error : 0.981278, accuarcy : 0.736368\n",
      "total step : 932 \n",
      "error : 0.980839, accuarcy : 0.736368\n",
      "total step : 933 \n",
      "error : 0.980401, accuarcy : 0.736368\n",
      "total step : 934 \n",
      "error : 0.979964, accuarcy : 0.736368\n",
      "total step : 935 \n",
      "error : 0.979527, accuarcy : 0.736368\n",
      "total step : 936 \n",
      "error : 0.979091, accuarcy : 0.736368\n",
      "total step : 937 \n",
      "error : 0.978656, accuarcy : 0.736368\n",
      "total step : 938 \n",
      "error : 0.978222, accuarcy : 0.736368\n",
      "total step : 939 \n",
      "error : 0.977788, accuarcy : 0.736868\n",
      "total step : 940 \n",
      "error : 0.977355, accuarcy : 0.736868\n",
      "total step : 941 \n",
      "error : 0.976922, accuarcy : 0.736868\n",
      "total step : 942 \n",
      "error : 0.976490, accuarcy : 0.737369\n",
      "total step : 943 \n",
      "error : 0.976059, accuarcy : 0.737369\n",
      "total step : 944 \n",
      "error : 0.975629, accuarcy : 0.737869\n",
      "total step : 945 \n",
      "error : 0.975199, accuarcy : 0.738369\n",
      "total step : 946 \n",
      "error : 0.974770, accuarcy : 0.738369\n",
      "total step : 947 \n",
      "error : 0.974341, accuarcy : 0.738369\n",
      "total step : 948 \n",
      "error : 0.973914, accuarcy : 0.738369\n",
      "total step : 949 \n",
      "error : 0.973487, accuarcy : 0.738369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 950 \n",
      "error : 0.973060, accuarcy : 0.738369\n",
      "total step : 951 \n",
      "error : 0.972635, accuarcy : 0.738369\n",
      "total step : 952 \n",
      "error : 0.972210, accuarcy : 0.738869\n",
      "total step : 953 \n",
      "error : 0.971785, accuarcy : 0.738869\n",
      "total step : 954 \n",
      "error : 0.971362, accuarcy : 0.738869\n",
      "total step : 955 \n",
      "error : 0.970939, accuarcy : 0.738869\n",
      "total step : 956 \n",
      "error : 0.970516, accuarcy : 0.738869\n",
      "total step : 957 \n",
      "error : 0.970094, accuarcy : 0.738869\n",
      "total step : 958 \n",
      "error : 0.969673, accuarcy : 0.738869\n",
      "total step : 959 \n",
      "error : 0.969253, accuarcy : 0.738869\n",
      "total step : 960 \n",
      "error : 0.968833, accuarcy : 0.739370\n",
      "total step : 961 \n",
      "error : 0.968414, accuarcy : 0.739870\n",
      "total step : 962 \n",
      "error : 0.967996, accuarcy : 0.739870\n",
      "total step : 963 \n",
      "error : 0.967578, accuarcy : 0.739870\n",
      "total step : 964 \n",
      "error : 0.967161, accuarcy : 0.739870\n",
      "total step : 965 \n",
      "error : 0.966744, accuarcy : 0.739870\n",
      "total step : 966 \n",
      "error : 0.966328, accuarcy : 0.739870\n",
      "total step : 967 \n",
      "error : 0.965913, accuarcy : 0.739870\n",
      "total step : 968 \n",
      "error : 0.965499, accuarcy : 0.739870\n",
      "total step : 969 \n",
      "error : 0.965085, accuarcy : 0.739870\n",
      "total step : 970 \n",
      "error : 0.964671, accuarcy : 0.739870\n",
      "total step : 971 \n",
      "error : 0.964259, accuarcy : 0.739870\n",
      "total step : 972 \n",
      "error : 0.963847, accuarcy : 0.739870\n",
      "total step : 973 \n",
      "error : 0.963435, accuarcy : 0.739870\n",
      "total step : 974 \n",
      "error : 0.963025, accuarcy : 0.739870\n",
      "total step : 975 \n",
      "error : 0.962615, accuarcy : 0.739870\n",
      "total step : 976 \n",
      "error : 0.962205, accuarcy : 0.739870\n",
      "total step : 977 \n",
      "error : 0.961796, accuarcy : 0.739870\n",
      "total step : 978 \n",
      "error : 0.961388, accuarcy : 0.740370\n",
      "total step : 979 \n",
      "error : 0.960980, accuarcy : 0.740370\n",
      "total step : 980 \n",
      "error : 0.960573, accuarcy : 0.740370\n",
      "total step : 981 \n",
      "error : 0.960167, accuarcy : 0.740870\n",
      "total step : 982 \n",
      "error : 0.959761, accuarcy : 0.740870\n",
      "total step : 983 \n",
      "error : 0.959356, accuarcy : 0.740870\n",
      "total step : 984 \n",
      "error : 0.958951, accuarcy : 0.740870\n",
      "total step : 985 \n",
      "error : 0.958548, accuarcy : 0.740870\n",
      "total step : 986 \n",
      "error : 0.958144, accuarcy : 0.740870\n",
      "total step : 987 \n",
      "error : 0.957741, accuarcy : 0.741371\n",
      "total step : 988 \n",
      "error : 0.957339, accuarcy : 0.741371\n",
      "total step : 989 \n",
      "error : 0.956938, accuarcy : 0.741871\n",
      "total step : 990 \n",
      "error : 0.956537, accuarcy : 0.741871\n",
      "total step : 991 \n",
      "error : 0.956137, accuarcy : 0.743372\n",
      "total step : 992 \n",
      "error : 0.955737, accuarcy : 0.743372\n",
      "total step : 993 \n",
      "error : 0.955338, accuarcy : 0.743372\n",
      "total step : 994 \n",
      "error : 0.954939, accuarcy : 0.743372\n",
      "total step : 995 \n",
      "error : 0.954542, accuarcy : 0.743872\n",
      "total step : 996 \n",
      "error : 0.954144, accuarcy : 0.743872\n",
      "total step : 997 \n",
      "error : 0.953748, accuarcy : 0.743372\n",
      "total step : 998 \n",
      "error : 0.953351, accuarcy : 0.743372\n",
      "total step : 999 \n",
      "error : 0.952956, accuarcy : 0.743872\n",
      "total step : 1000 \n",
      "error : 0.952561, accuarcy : 0.743872\n",
      "total step : 1001 \n",
      "error : 0.952167, accuarcy : 0.743872\n",
      "total step : 1002 \n",
      "error : 0.951773, accuarcy : 0.743872\n",
      "total step : 1003 \n",
      "error : 0.951380, accuarcy : 0.744372\n",
      "total step : 1004 \n",
      "error : 0.950987, accuarcy : 0.744372\n",
      "total step : 1005 \n",
      "error : 0.950595, accuarcy : 0.744372\n",
      "total step : 1006 \n",
      "error : 0.950204, accuarcy : 0.744372\n",
      "total step : 1007 \n",
      "error : 0.949813, accuarcy : 0.744372\n",
      "total step : 1008 \n",
      "error : 0.949422, accuarcy : 0.744872\n",
      "total step : 1009 \n",
      "error : 0.949033, accuarcy : 0.744872\n",
      "total step : 1010 \n",
      "error : 0.948644, accuarcy : 0.744872\n",
      "total step : 1011 \n",
      "error : 0.948255, accuarcy : 0.744872\n",
      "total step : 1012 \n",
      "error : 0.947867, accuarcy : 0.745373\n",
      "total step : 1013 \n",
      "error : 0.947480, accuarcy : 0.745873\n",
      "total step : 1014 \n",
      "error : 0.947093, accuarcy : 0.746373\n",
      "total step : 1015 \n",
      "error : 0.946706, accuarcy : 0.746373\n",
      "total step : 1016 \n",
      "error : 0.946321, accuarcy : 0.746373\n",
      "total step : 1017 \n",
      "error : 0.945936, accuarcy : 0.746373\n",
      "total step : 1018 \n",
      "error : 0.945551, accuarcy : 0.746373\n",
      "total step : 1019 \n",
      "error : 0.945167, accuarcy : 0.746373\n",
      "total step : 1020 \n",
      "error : 0.944783, accuarcy : 0.746373\n",
      "total step : 1021 \n",
      "error : 0.944401, accuarcy : 0.746373\n",
      "total step : 1022 \n",
      "error : 0.944018, accuarcy : 0.746873\n",
      "total step : 1023 \n",
      "error : 0.943636, accuarcy : 0.747374\n",
      "total step : 1024 \n",
      "error : 0.943255, accuarcy : 0.747874\n",
      "total step : 1025 \n",
      "error : 0.942874, accuarcy : 0.748874\n",
      "total step : 1026 \n",
      "error : 0.942494, accuarcy : 0.748874\n",
      "total step : 1027 \n",
      "error : 0.942115, accuarcy : 0.748874\n",
      "total step : 1028 \n",
      "error : 0.941735, accuarcy : 0.748874\n",
      "total step : 1029 \n",
      "error : 0.941357, accuarcy : 0.748874\n",
      "total step : 1030 \n",
      "error : 0.940979, accuarcy : 0.748874\n",
      "total step : 1031 \n",
      "error : 0.940602, accuarcy : 0.748874\n",
      "total step : 1032 \n",
      "error : 0.940225, accuarcy : 0.748874\n",
      "total step : 1033 \n",
      "error : 0.939848, accuarcy : 0.748874\n",
      "total step : 1034 \n",
      "error : 0.939472, accuarcy : 0.748874\n",
      "total step : 1035 \n",
      "error : 0.939097, accuarcy : 0.748874\n",
      "total step : 1036 \n",
      "error : 0.938722, accuarcy : 0.749375\n",
      "total step : 1037 \n",
      "error : 0.938348, accuarcy : 0.749375\n",
      "total step : 1038 \n",
      "error : 0.937975, accuarcy : 0.749375\n",
      "total step : 1039 \n",
      "error : 0.937601, accuarcy : 0.749375\n",
      "total step : 1040 \n",
      "error : 0.937229, accuarcy : 0.749375\n",
      "total step : 1041 \n",
      "error : 0.936857, accuarcy : 0.749375\n",
      "total step : 1042 \n",
      "error : 0.936485, accuarcy : 0.749375\n",
      "total step : 1043 \n",
      "error : 0.936114, accuarcy : 0.749375\n",
      "total step : 1044 \n",
      "error : 0.935744, accuarcy : 0.749875\n",
      "total step : 1045 \n",
      "error : 0.935374, accuarcy : 0.749875\n",
      "total step : 1046 \n",
      "error : 0.935004, accuarcy : 0.749375\n",
      "total step : 1047 \n",
      "error : 0.934635, accuarcy : 0.749375\n",
      "total step : 1048 \n",
      "error : 0.934267, accuarcy : 0.749375\n",
      "total step : 1049 \n",
      "error : 0.933899, accuarcy : 0.749375\n",
      "total step : 1050 \n",
      "error : 0.933531, accuarcy : 0.748874\n",
      "total step : 1051 \n",
      "error : 0.933165, accuarcy : 0.748874\n",
      "total step : 1052 \n",
      "error : 0.932798, accuarcy : 0.748874\n",
      "total step : 1053 \n",
      "error : 0.932432, accuarcy : 0.748874\n",
      "total step : 1054 \n",
      "error : 0.932067, accuarcy : 0.748874\n",
      "total step : 1055 \n",
      "error : 0.931702, accuarcy : 0.748874\n",
      "total step : 1056 \n",
      "error : 0.931338, accuarcy : 0.748874\n",
      "total step : 1057 \n",
      "error : 0.930974, accuarcy : 0.749375\n",
      "total step : 1058 \n",
      "error : 0.930611, accuarcy : 0.749375\n",
      "total step : 1059 \n",
      "error : 0.930248, accuarcy : 0.749875\n",
      "total step : 1060 \n",
      "error : 0.929886, accuarcy : 0.749875\n",
      "total step : 1061 \n",
      "error : 0.929524, accuarcy : 0.749875\n",
      "total step : 1062 \n",
      "error : 0.929163, accuarcy : 0.749875\n",
      "total step : 1063 \n",
      "error : 0.928802, accuarcy : 0.749875\n",
      "total step : 1064 \n",
      "error : 0.928442, accuarcy : 0.749875\n",
      "total step : 1065 \n",
      "error : 0.928082, accuarcy : 0.749875\n",
      "total step : 1066 \n",
      "error : 0.927722, accuarcy : 0.749875\n",
      "total step : 1067 \n",
      "error : 0.927364, accuarcy : 0.749875\n",
      "total step : 1068 \n",
      "error : 0.927005, accuarcy : 0.749875\n",
      "total step : 1069 \n",
      "error : 0.926648, accuarcy : 0.749875\n",
      "total step : 1070 \n",
      "error : 0.926290, accuarcy : 0.749875\n",
      "total step : 1071 \n",
      "error : 0.925933, accuarcy : 0.749875\n",
      "total step : 1072 \n",
      "error : 0.925577, accuarcy : 0.749875\n",
      "total step : 1073 \n",
      "error : 0.925221, accuarcy : 0.749875\n",
      "total step : 1074 \n",
      "error : 0.924866, accuarcy : 0.749875\n",
      "total step : 1075 \n",
      "error : 0.924511, accuarcy : 0.749875\n",
      "total step : 1076 \n",
      "error : 0.924157, accuarcy : 0.749875\n",
      "total step : 1077 \n",
      "error : 0.923803, accuarcy : 0.749375\n",
      "total step : 1078 \n",
      "error : 0.923449, accuarcy : 0.749375\n",
      "total step : 1079 \n",
      "error : 0.923096, accuarcy : 0.749375\n",
      "total step : 1080 \n",
      "error : 0.922744, accuarcy : 0.749875\n",
      "total step : 1081 \n",
      "error : 0.922392, accuarcy : 0.749875\n",
      "total step : 1082 \n",
      "error : 0.922040, accuarcy : 0.749875\n",
      "total step : 1083 \n",
      "error : 0.921689, accuarcy : 0.750375\n",
      "total step : 1084 \n",
      "error : 0.921339, accuarcy : 0.750375\n",
      "total step : 1085 \n",
      "error : 0.920988, accuarcy : 0.750375\n",
      "total step : 1086 \n",
      "error : 0.920639, accuarcy : 0.750375\n",
      "total step : 1087 \n",
      "error : 0.920290, accuarcy : 0.750375\n",
      "total step : 1088 \n",
      "error : 0.919941, accuarcy : 0.750375\n",
      "total step : 1089 \n",
      "error : 0.919593, accuarcy : 0.750375\n",
      "total step : 1090 \n",
      "error : 0.919245, accuarcy : 0.750375\n",
      "total step : 1091 \n",
      "error : 0.918898, accuarcy : 0.750375\n",
      "total step : 1092 \n",
      "error : 0.918551, accuarcy : 0.750375\n",
      "total step : 1093 \n",
      "error : 0.918205, accuarcy : 0.750875\n",
      "total step : 1094 \n",
      "error : 0.917859, accuarcy : 0.750875\n",
      "total step : 1095 \n",
      "error : 0.917513, accuarcy : 0.750875\n",
      "total step : 1096 \n",
      "error : 0.917168, accuarcy : 0.751376\n",
      "total step : 1097 \n",
      "error : 0.916824, accuarcy : 0.751876\n",
      "total step : 1098 \n",
      "error : 0.916480, accuarcy : 0.751876\n",
      "total step : 1099 \n",
      "error : 0.916136, accuarcy : 0.751876\n",
      "total step : 1100 \n",
      "error : 0.915793, accuarcy : 0.751876\n",
      "total step : 1101 \n",
      "error : 0.915450, accuarcy : 0.751876\n",
      "total step : 1102 \n",
      "error : 0.915108, accuarcy : 0.751876\n",
      "total step : 1103 \n",
      "error : 0.914766, accuarcy : 0.751876\n",
      "total step : 1104 \n",
      "error : 0.914425, accuarcy : 0.751876\n",
      "total step : 1105 \n",
      "error : 0.914084, accuarcy : 0.751876\n",
      "total step : 1106 \n",
      "error : 0.913744, accuarcy : 0.751876\n",
      "total step : 1107 \n",
      "error : 0.913404, accuarcy : 0.751876\n",
      "total step : 1108 \n",
      "error : 0.913064, accuarcy : 0.751876\n",
      "total step : 1109 \n",
      "error : 0.912725, accuarcy : 0.751876\n",
      "total step : 1110 \n",
      "error : 0.912386, accuarcy : 0.751876\n",
      "total step : 1111 \n",
      "error : 0.912048, accuarcy : 0.751876\n",
      "total step : 1112 \n",
      "error : 0.911711, accuarcy : 0.751876\n",
      "total step : 1113 \n",
      "error : 0.911373, accuarcy : 0.751876\n",
      "total step : 1114 \n",
      "error : 0.911036, accuarcy : 0.751876\n",
      "total step : 1115 \n",
      "error : 0.910700, accuarcy : 0.751876\n",
      "total step : 1116 \n",
      "error : 0.910364, accuarcy : 0.751876\n",
      "total step : 1117 \n",
      "error : 0.910028, accuarcy : 0.751876\n",
      "total step : 1118 \n",
      "error : 0.909693, accuarcy : 0.752376\n",
      "total step : 1119 \n",
      "error : 0.909359, accuarcy : 0.752376\n",
      "total step : 1120 \n",
      "error : 0.909024, accuarcy : 0.752376\n",
      "total step : 1121 \n",
      "error : 0.908691, accuarcy : 0.752376\n",
      "total step : 1122 \n",
      "error : 0.908357, accuarcy : 0.752876\n",
      "total step : 1123 \n",
      "error : 0.908024, accuarcy : 0.753377\n",
      "total step : 1124 \n",
      "error : 0.907692, accuarcy : 0.753377\n",
      "total step : 1125 \n",
      "error : 0.907360, accuarcy : 0.753377\n",
      "total step : 1126 \n",
      "error : 0.907028, accuarcy : 0.753377\n",
      "total step : 1127 \n",
      "error : 0.906697, accuarcy : 0.753877\n",
      "total step : 1128 \n",
      "error : 0.906366, accuarcy : 0.753877\n",
      "total step : 1129 \n",
      "error : 0.906036, accuarcy : 0.753877\n",
      "total step : 1130 \n",
      "error : 0.905706, accuarcy : 0.753877\n",
      "total step : 1131 \n",
      "error : 0.905376, accuarcy : 0.753877\n",
      "total step : 1132 \n",
      "error : 0.905047, accuarcy : 0.754877\n",
      "total step : 1133 \n",
      "error : 0.904718, accuarcy : 0.754877\n",
      "total step : 1134 \n",
      "error : 0.904390, accuarcy : 0.754877\n",
      "total step : 1135 \n",
      "error : 0.904062, accuarcy : 0.754877\n",
      "total step : 1136 \n",
      "error : 0.903735, accuarcy : 0.754877\n",
      "total step : 1137 \n",
      "error : 0.903408, accuarcy : 0.755378\n",
      "total step : 1138 \n",
      "error : 0.903081, accuarcy : 0.755378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 1139 \n",
      "error : 0.902755, accuarcy : 0.755378\n",
      "total step : 1140 \n",
      "error : 0.902429, accuarcy : 0.755378\n",
      "total step : 1141 \n",
      "error : 0.902104, accuarcy : 0.755378\n",
      "total step : 1142 \n",
      "error : 0.901779, accuarcy : 0.755378\n",
      "total step : 1143 \n",
      "error : 0.901454, accuarcy : 0.755878\n",
      "total step : 1144 \n",
      "error : 0.901130, accuarcy : 0.755878\n",
      "total step : 1145 \n",
      "error : 0.900806, accuarcy : 0.755878\n",
      "total step : 1146 \n",
      "error : 0.900483, accuarcy : 0.755878\n",
      "total step : 1147 \n",
      "error : 0.900160, accuarcy : 0.755878\n",
      "total step : 1148 \n",
      "error : 0.899838, accuarcy : 0.755878\n",
      "total step : 1149 \n",
      "error : 0.899516, accuarcy : 0.755878\n",
      "total step : 1150 \n",
      "error : 0.899194, accuarcy : 0.755878\n",
      "total step : 1151 \n",
      "error : 0.898873, accuarcy : 0.755878\n",
      "total step : 1152 \n",
      "error : 0.898552, accuarcy : 0.755878\n",
      "total step : 1153 \n",
      "error : 0.898231, accuarcy : 0.755878\n",
      "total step : 1154 \n",
      "error : 0.897911, accuarcy : 0.755878\n",
      "total step : 1155 \n",
      "error : 0.897592, accuarcy : 0.755878\n",
      "total step : 1156 \n",
      "error : 0.897272, accuarcy : 0.755878\n",
      "total step : 1157 \n",
      "error : 0.896954, accuarcy : 0.755878\n",
      "total step : 1158 \n",
      "error : 0.896635, accuarcy : 0.755878\n",
      "total step : 1159 \n",
      "error : 0.896317, accuarcy : 0.755878\n",
      "total step : 1160 \n",
      "error : 0.895999, accuarcy : 0.755878\n",
      "total step : 1161 \n",
      "error : 0.895682, accuarcy : 0.755878\n",
      "total step : 1162 \n",
      "error : 0.895365, accuarcy : 0.756878\n",
      "total step : 1163 \n",
      "error : 0.895049, accuarcy : 0.756878\n",
      "total step : 1164 \n",
      "error : 0.894733, accuarcy : 0.756878\n",
      "total step : 1165 \n",
      "error : 0.894417, accuarcy : 0.756878\n",
      "total step : 1166 \n",
      "error : 0.894101, accuarcy : 0.756878\n",
      "total step : 1167 \n",
      "error : 0.893787, accuarcy : 0.756878\n",
      "total step : 1168 \n",
      "error : 0.893472, accuarcy : 0.756878\n",
      "total step : 1169 \n",
      "error : 0.893158, accuarcy : 0.756878\n",
      "total step : 1170 \n",
      "error : 0.892844, accuarcy : 0.756878\n",
      "total step : 1171 \n",
      "error : 0.892531, accuarcy : 0.756878\n",
      "total step : 1172 \n",
      "error : 0.892218, accuarcy : 0.756878\n",
      "total step : 1173 \n",
      "error : 0.891905, accuarcy : 0.756878\n",
      "total step : 1174 \n",
      "error : 0.891593, accuarcy : 0.757379\n",
      "total step : 1175 \n",
      "error : 0.891281, accuarcy : 0.757379\n",
      "total step : 1176 \n",
      "error : 0.890969, accuarcy : 0.757879\n",
      "total step : 1177 \n",
      "error : 0.890658, accuarcy : 0.757879\n",
      "total step : 1178 \n",
      "error : 0.890348, accuarcy : 0.757879\n",
      "total step : 1179 \n",
      "error : 0.890037, accuarcy : 0.758879\n",
      "total step : 1180 \n",
      "error : 0.889727, accuarcy : 0.758879\n",
      "total step : 1181 \n",
      "error : 0.889418, accuarcy : 0.758879\n",
      "total step : 1182 \n",
      "error : 0.889109, accuarcy : 0.759380\n",
      "total step : 1183 \n",
      "error : 0.888800, accuarcy : 0.759380\n",
      "total step : 1184 \n",
      "error : 0.888491, accuarcy : 0.759380\n",
      "total step : 1185 \n",
      "error : 0.888183, accuarcy : 0.759380\n",
      "total step : 1186 \n",
      "error : 0.887875, accuarcy : 0.759380\n",
      "total step : 1187 \n",
      "error : 0.887568, accuarcy : 0.759380\n",
      "total step : 1188 \n",
      "error : 0.887261, accuarcy : 0.759380\n",
      "total step : 1189 \n",
      "error : 0.886955, accuarcy : 0.759380\n",
      "total step : 1190 \n",
      "error : 0.886648, accuarcy : 0.759380\n",
      "total step : 1191 \n",
      "error : 0.886342, accuarcy : 0.759380\n",
      "total step : 1192 \n",
      "error : 0.886037, accuarcy : 0.759380\n",
      "total step : 1193 \n",
      "error : 0.885732, accuarcy : 0.759380\n",
      "total step : 1194 \n",
      "error : 0.885427, accuarcy : 0.759380\n",
      "total step : 1195 \n",
      "error : 0.885123, accuarcy : 0.759380\n",
      "total step : 1196 \n",
      "error : 0.884819, accuarcy : 0.759380\n",
      "total step : 1197 \n",
      "error : 0.884515, accuarcy : 0.759380\n",
      "total step : 1198 \n",
      "error : 0.884212, accuarcy : 0.759380\n",
      "total step : 1199 \n",
      "error : 0.883909, accuarcy : 0.759380\n",
      "total step : 1200 \n",
      "error : 0.883606, accuarcy : 0.759880\n",
      "total step : 1201 \n",
      "error : 0.883304, accuarcy : 0.759880\n",
      "total step : 1202 \n",
      "error : 0.883002, accuarcy : 0.759880\n",
      "total step : 1203 \n",
      "error : 0.882701, accuarcy : 0.759880\n",
      "total step : 1204 \n",
      "error : 0.882399, accuarcy : 0.760380\n",
      "total step : 1205 \n",
      "error : 0.882099, accuarcy : 0.760880\n",
      "total step : 1206 \n",
      "error : 0.881798, accuarcy : 0.760880\n",
      "total step : 1207 \n",
      "error : 0.881498, accuarcy : 0.760880\n",
      "total step : 1208 \n",
      "error : 0.881198, accuarcy : 0.760880\n",
      "total step : 1209 \n",
      "error : 0.880899, accuarcy : 0.760880\n",
      "total step : 1210 \n",
      "error : 0.880600, accuarcy : 0.760880\n",
      "total step : 1211 \n",
      "error : 0.880301, accuarcy : 0.760880\n",
      "total step : 1212 \n",
      "error : 0.880003, accuarcy : 0.760880\n",
      "total step : 1213 \n",
      "error : 0.879705, accuarcy : 0.760880\n",
      "total step : 1214 \n",
      "error : 0.879407, accuarcy : 0.760880\n",
      "total step : 1215 \n",
      "error : 0.879110, accuarcy : 0.761881\n",
      "total step : 1216 \n",
      "error : 0.878813, accuarcy : 0.761881\n",
      "total step : 1217 \n",
      "error : 0.878517, accuarcy : 0.761881\n",
      "total step : 1218 \n",
      "error : 0.878220, accuarcy : 0.762381\n",
      "total step : 1219 \n",
      "error : 0.877925, accuarcy : 0.762381\n",
      "total step : 1220 \n",
      "error : 0.877629, accuarcy : 0.762381\n",
      "total step : 1221 \n",
      "error : 0.877334, accuarcy : 0.762381\n",
      "total step : 1222 \n",
      "error : 0.877039, accuarcy : 0.762381\n",
      "total step : 1223 \n",
      "error : 0.876745, accuarcy : 0.762381\n",
      "total step : 1224 \n",
      "error : 0.876450, accuarcy : 0.762381\n",
      "total step : 1225 \n",
      "error : 0.876157, accuarcy : 0.762381\n",
      "total step : 1226 \n",
      "error : 0.875863, accuarcy : 0.762381\n",
      "total step : 1227 \n",
      "error : 0.875570, accuarcy : 0.762381\n",
      "total step : 1228 \n",
      "error : 0.875277, accuarcy : 0.762381\n",
      "total step : 1229 \n",
      "error : 0.874985, accuarcy : 0.762381\n",
      "total step : 1230 \n",
      "error : 0.874693, accuarcy : 0.762881\n",
      "total step : 1231 \n",
      "error : 0.874401, accuarcy : 0.762881\n",
      "total step : 1232 \n",
      "error : 0.874109, accuarcy : 0.762881\n",
      "total step : 1233 \n",
      "error : 0.873818, accuarcy : 0.762881\n",
      "total step : 1234 \n",
      "error : 0.873528, accuarcy : 0.762881\n",
      "total step : 1235 \n",
      "error : 0.873237, accuarcy : 0.762881\n",
      "total step : 1236 \n",
      "error : 0.872947, accuarcy : 0.762881\n",
      "total step : 1237 \n",
      "error : 0.872657, accuarcy : 0.762881\n",
      "total step : 1238 \n",
      "error : 0.872368, accuarcy : 0.762881\n",
      "total step : 1239 \n",
      "error : 0.872079, accuarcy : 0.762881\n",
      "total step : 1240 \n",
      "error : 0.871790, accuarcy : 0.762881\n",
      "total step : 1241 \n",
      "error : 0.871502, accuarcy : 0.762881\n",
      "total step : 1242 \n",
      "error : 0.871214, accuarcy : 0.762881\n",
      "total step : 1243 \n",
      "error : 0.870926, accuarcy : 0.762881\n",
      "total step : 1244 \n",
      "error : 0.870638, accuarcy : 0.762881\n",
      "total step : 1245 \n",
      "error : 0.870351, accuarcy : 0.763382\n",
      "total step : 1246 \n",
      "error : 0.870064, accuarcy : 0.763382\n",
      "total step : 1247 \n",
      "error : 0.869778, accuarcy : 0.763382\n",
      "total step : 1248 \n",
      "error : 0.869492, accuarcy : 0.763382\n",
      "total step : 1249 \n",
      "error : 0.869206, accuarcy : 0.763382\n",
      "total step : 1250 \n",
      "error : 0.868920, accuarcy : 0.763382\n",
      "total step : 1251 \n",
      "error : 0.868635, accuarcy : 0.763382\n",
      "total step : 1252 \n",
      "error : 0.868350, accuarcy : 0.763382\n",
      "total step : 1253 \n",
      "error : 0.868066, accuarcy : 0.763882\n",
      "total step : 1254 \n",
      "error : 0.867782, accuarcy : 0.763882\n",
      "total step : 1255 \n",
      "error : 0.867498, accuarcy : 0.763882\n",
      "total step : 1256 \n",
      "error : 0.867214, accuarcy : 0.763882\n",
      "total step : 1257 \n",
      "error : 0.866931, accuarcy : 0.763882\n",
      "total step : 1258 \n",
      "error : 0.866648, accuarcy : 0.763882\n",
      "total step : 1259 \n",
      "error : 0.866365, accuarcy : 0.763882\n",
      "total step : 1260 \n",
      "error : 0.866083, accuarcy : 0.763882\n",
      "total step : 1261 \n",
      "error : 0.865801, accuarcy : 0.763882\n",
      "total step : 1262 \n",
      "error : 0.865520, accuarcy : 0.763882\n",
      "total step : 1263 \n",
      "error : 0.865238, accuarcy : 0.763882\n",
      "total step : 1264 \n",
      "error : 0.864957, accuarcy : 0.763882\n",
      "total step : 1265 \n",
      "error : 0.864676, accuarcy : 0.763882\n",
      "total step : 1266 \n",
      "error : 0.864396, accuarcy : 0.763882\n",
      "total step : 1267 \n",
      "error : 0.864116, accuarcy : 0.763882\n",
      "total step : 1268 \n",
      "error : 0.863836, accuarcy : 0.764382\n",
      "total step : 1269 \n",
      "error : 0.863557, accuarcy : 0.764382\n",
      "total step : 1270 \n",
      "error : 0.863278, accuarcy : 0.764382\n",
      "total step : 1271 \n",
      "error : 0.862999, accuarcy : 0.764382\n",
      "total step : 1272 \n",
      "error : 0.862720, accuarcy : 0.764382\n",
      "total step : 1273 \n",
      "error : 0.862442, accuarcy : 0.764382\n",
      "total step : 1274 \n",
      "error : 0.862164, accuarcy : 0.764382\n",
      "total step : 1275 \n",
      "error : 0.861886, accuarcy : 0.764882\n",
      "total step : 1276 \n",
      "error : 0.861609, accuarcy : 0.764882\n",
      "total step : 1277 \n",
      "error : 0.861332, accuarcy : 0.764882\n",
      "total step : 1278 \n",
      "error : 0.861055, accuarcy : 0.764882\n",
      "total step : 1279 \n",
      "error : 0.860779, accuarcy : 0.764882\n",
      "total step : 1280 \n",
      "error : 0.860503, accuarcy : 0.764882\n",
      "total step : 1281 \n",
      "error : 0.860227, accuarcy : 0.764882\n",
      "total step : 1282 \n",
      "error : 0.859952, accuarcy : 0.764882\n",
      "total step : 1283 \n",
      "error : 0.859677, accuarcy : 0.764882\n",
      "total step : 1284 \n",
      "error : 0.859402, accuarcy : 0.764882\n",
      "total step : 1285 \n",
      "error : 0.859127, accuarcy : 0.764882\n",
      "total step : 1286 \n",
      "error : 0.858853, accuarcy : 0.765383\n",
      "total step : 1287 \n",
      "error : 0.858579, accuarcy : 0.765383\n",
      "total step : 1288 \n",
      "error : 0.858305, accuarcy : 0.765383\n",
      "total step : 1289 \n",
      "error : 0.858032, accuarcy : 0.765383\n",
      "total step : 1290 \n",
      "error : 0.857759, accuarcy : 0.765383\n",
      "total step : 1291 \n",
      "error : 0.857486, accuarcy : 0.765383\n",
      "total step : 1292 \n",
      "error : 0.857214, accuarcy : 0.765383\n",
      "total step : 1293 \n",
      "error : 0.856941, accuarcy : 0.765383\n",
      "total step : 1294 \n",
      "error : 0.856670, accuarcy : 0.765383\n",
      "total step : 1295 \n",
      "error : 0.856398, accuarcy : 0.765383\n",
      "total step : 1296 \n",
      "error : 0.856127, accuarcy : 0.765383\n",
      "total step : 1297 \n",
      "error : 0.855856, accuarcy : 0.765383\n",
      "total step : 1298 \n",
      "error : 0.855585, accuarcy : 0.765383\n",
      "total step : 1299 \n",
      "error : 0.855315, accuarcy : 0.765383\n",
      "total step : 1300 \n",
      "error : 0.855044, accuarcy : 0.765383\n",
      "total step : 1301 \n",
      "error : 0.854775, accuarcy : 0.765383\n",
      "total step : 1302 \n",
      "error : 0.854505, accuarcy : 0.765383\n",
      "total step : 1303 \n",
      "error : 0.854236, accuarcy : 0.765383\n",
      "total step : 1304 \n",
      "error : 0.853967, accuarcy : 0.765383\n",
      "total step : 1305 \n",
      "error : 0.853698, accuarcy : 0.765383\n",
      "total step : 1306 \n",
      "error : 0.853430, accuarcy : 0.765383\n",
      "total step : 1307 \n",
      "error : 0.853162, accuarcy : 0.765383\n",
      "total step : 1308 \n",
      "error : 0.852894, accuarcy : 0.765383\n",
      "total step : 1309 \n",
      "error : 0.852626, accuarcy : 0.765383\n",
      "total step : 1310 \n",
      "error : 0.852359, accuarcy : 0.765383\n",
      "total step : 1311 \n",
      "error : 0.852092, accuarcy : 0.765383\n",
      "total step : 1312 \n",
      "error : 0.851826, accuarcy : 0.765383\n",
      "total step : 1313 \n",
      "error : 0.851559, accuarcy : 0.765383\n",
      "total step : 1314 \n",
      "error : 0.851293, accuarcy : 0.765383\n",
      "total step : 1315 \n",
      "error : 0.851027, accuarcy : 0.765383\n",
      "total step : 1316 \n",
      "error : 0.850762, accuarcy : 0.765383\n",
      "total step : 1317 \n",
      "error : 0.850497, accuarcy : 0.765383\n",
      "total step : 1318 \n",
      "error : 0.850232, accuarcy : 0.765383\n",
      "total step : 1319 \n",
      "error : 0.849967, accuarcy : 0.765383\n",
      "total step : 1320 \n",
      "error : 0.849703, accuarcy : 0.765383\n",
      "total step : 1321 \n",
      "error : 0.849439, accuarcy : 0.765383\n",
      "total step : 1322 \n",
      "error : 0.849175, accuarcy : 0.765383\n",
      "total step : 1323 \n",
      "error : 0.848911, accuarcy : 0.765383\n",
      "total step : 1324 \n",
      "error : 0.848648, accuarcy : 0.765383\n",
      "total step : 1325 \n",
      "error : 0.848385, accuarcy : 0.765383\n",
      "total step : 1326 \n",
      "error : 0.848122, accuarcy : 0.765883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 1327 \n",
      "error : 0.847860, accuarcy : 0.765883\n",
      "total step : 1328 \n",
      "error : 0.847598, accuarcy : 0.765383\n",
      "total step : 1329 \n",
      "error : 0.847336, accuarcy : 0.765883\n",
      "total step : 1330 \n",
      "error : 0.847074, accuarcy : 0.765883\n",
      "total step : 1331 \n",
      "error : 0.846813, accuarcy : 0.765883\n",
      "total step : 1332 \n",
      "error : 0.846552, accuarcy : 0.765883\n",
      "total step : 1333 \n",
      "error : 0.846291, accuarcy : 0.765883\n",
      "total step : 1334 \n",
      "error : 0.846030, accuarcy : 0.765883\n",
      "total step : 1335 \n",
      "error : 0.845770, accuarcy : 0.765883\n",
      "total step : 1336 \n",
      "error : 0.845510, accuarcy : 0.765883\n",
      "total step : 1337 \n",
      "error : 0.845251, accuarcy : 0.765883\n",
      "total step : 1338 \n",
      "error : 0.844991, accuarcy : 0.765883\n",
      "total step : 1339 \n",
      "error : 0.844732, accuarcy : 0.766383\n",
      "total step : 1340 \n",
      "error : 0.844473, accuarcy : 0.766883\n",
      "total step : 1341 \n",
      "error : 0.844215, accuarcy : 0.766883\n",
      "total step : 1342 \n",
      "error : 0.843956, accuarcy : 0.766883\n",
      "total step : 1343 \n",
      "error : 0.843698, accuarcy : 0.766883\n",
      "total step : 1344 \n",
      "error : 0.843440, accuarcy : 0.766883\n",
      "total step : 1345 \n",
      "error : 0.843183, accuarcy : 0.766883\n",
      "total step : 1346 \n",
      "error : 0.842926, accuarcy : 0.766883\n",
      "total step : 1347 \n",
      "error : 0.842669, accuarcy : 0.766883\n",
      "total step : 1348 \n",
      "error : 0.842412, accuarcy : 0.766883\n",
      "total step : 1349 \n",
      "error : 0.842155, accuarcy : 0.766883\n",
      "total step : 1350 \n",
      "error : 0.841899, accuarcy : 0.766883\n",
      "total step : 1351 \n",
      "error : 0.841643, accuarcy : 0.766883\n",
      "total step : 1352 \n",
      "error : 0.841388, accuarcy : 0.766883\n",
      "total step : 1353 \n",
      "error : 0.841132, accuarcy : 0.766883\n",
      "total step : 1354 \n",
      "error : 0.840877, accuarcy : 0.766883\n",
      "total step : 1355 \n",
      "error : 0.840622, accuarcy : 0.766883\n",
      "total step : 1356 \n",
      "error : 0.840367, accuarcy : 0.766883\n",
      "total step : 1357 \n",
      "error : 0.840113, accuarcy : 0.766883\n",
      "total step : 1358 \n",
      "error : 0.839859, accuarcy : 0.766883\n",
      "total step : 1359 \n",
      "error : 0.839605, accuarcy : 0.766883\n",
      "total step : 1360 \n",
      "error : 0.839352, accuarcy : 0.767384\n",
      "total step : 1361 \n",
      "error : 0.839098, accuarcy : 0.767384\n",
      "total step : 1362 \n",
      "error : 0.838845, accuarcy : 0.767384\n",
      "total step : 1363 \n",
      "error : 0.838592, accuarcy : 0.767384\n",
      "total step : 1364 \n",
      "error : 0.838340, accuarcy : 0.767384\n",
      "total step : 1365 \n",
      "error : 0.838088, accuarcy : 0.767384\n",
      "total step : 1366 \n",
      "error : 0.837836, accuarcy : 0.767384\n",
      "total step : 1367 \n",
      "error : 0.837584, accuarcy : 0.767384\n",
      "total step : 1368 \n",
      "error : 0.837332, accuarcy : 0.767384\n",
      "total step : 1369 \n",
      "error : 0.837081, accuarcy : 0.767384\n",
      "total step : 1370 \n",
      "error : 0.836830, accuarcy : 0.767384\n",
      "total step : 1371 \n",
      "error : 0.836579, accuarcy : 0.767384\n",
      "total step : 1372 \n",
      "error : 0.836329, accuarcy : 0.767384\n",
      "total step : 1373 \n",
      "error : 0.836079, accuarcy : 0.767384\n",
      "total step : 1374 \n",
      "error : 0.835829, accuarcy : 0.767384\n",
      "total step : 1375 \n",
      "error : 0.835579, accuarcy : 0.767384\n",
      "total step : 1376 \n",
      "error : 0.835329, accuarcy : 0.767384\n",
      "total step : 1377 \n",
      "error : 0.835080, accuarcy : 0.767384\n",
      "total step : 1378 \n",
      "error : 0.834831, accuarcy : 0.767884\n",
      "total step : 1379 \n",
      "error : 0.834582, accuarcy : 0.767884\n",
      "total step : 1380 \n",
      "error : 0.834334, accuarcy : 0.767884\n",
      "total step : 1381 \n",
      "error : 0.834086, accuarcy : 0.767884\n",
      "total step : 1382 \n",
      "error : 0.833838, accuarcy : 0.767884\n",
      "total step : 1383 \n",
      "error : 0.833590, accuarcy : 0.767884\n",
      "total step : 1384 \n",
      "error : 0.833343, accuarcy : 0.767884\n",
      "total step : 1385 \n",
      "error : 0.833095, accuarcy : 0.767884\n",
      "total step : 1386 \n",
      "error : 0.832848, accuarcy : 0.768384\n",
      "total step : 1387 \n",
      "error : 0.832602, accuarcy : 0.768384\n",
      "total step : 1388 \n",
      "error : 0.832355, accuarcy : 0.768884\n",
      "total step : 1389 \n",
      "error : 0.832109, accuarcy : 0.768884\n",
      "total step : 1390 \n",
      "error : 0.831863, accuarcy : 0.768884\n",
      "total step : 1391 \n",
      "error : 0.831617, accuarcy : 0.768884\n",
      "total step : 1392 \n",
      "error : 0.831372, accuarcy : 0.768884\n",
      "total step : 1393 \n",
      "error : 0.831126, accuarcy : 0.768884\n",
      "total step : 1394 \n",
      "error : 0.830881, accuarcy : 0.768884\n",
      "total step : 1395 \n",
      "error : 0.830637, accuarcy : 0.768884\n",
      "total step : 1396 \n",
      "error : 0.830392, accuarcy : 0.768884\n",
      "total step : 1397 \n",
      "error : 0.830148, accuarcy : 0.769385\n",
      "total step : 1398 \n",
      "error : 0.829904, accuarcy : 0.769385\n",
      "total step : 1399 \n",
      "error : 0.829660, accuarcy : 0.769385\n",
      "total step : 1400 \n",
      "error : 0.829417, accuarcy : 0.769385\n",
      "total step : 1401 \n",
      "error : 0.829173, accuarcy : 0.769385\n",
      "total step : 1402 \n",
      "error : 0.828930, accuarcy : 0.769385\n",
      "total step : 1403 \n",
      "error : 0.828687, accuarcy : 0.769385\n",
      "total step : 1404 \n",
      "error : 0.828445, accuarcy : 0.769385\n",
      "total step : 1405 \n",
      "error : 0.828202, accuarcy : 0.769385\n",
      "total step : 1406 \n",
      "error : 0.827960, accuarcy : 0.769385\n",
      "total step : 1407 \n",
      "error : 0.827718, accuarcy : 0.769885\n",
      "total step : 1408 \n",
      "error : 0.827477, accuarcy : 0.769885\n",
      "total step : 1409 \n",
      "error : 0.827235, accuarcy : 0.769885\n",
      "total step : 1410 \n",
      "error : 0.826994, accuarcy : 0.769885\n",
      "total step : 1411 \n",
      "error : 0.826753, accuarcy : 0.769885\n",
      "total step : 1412 \n",
      "error : 0.826513, accuarcy : 0.769885\n",
      "total step : 1413 \n",
      "error : 0.826272, accuarcy : 0.770385\n",
      "total step : 1414 \n",
      "error : 0.826032, accuarcy : 0.770385\n",
      "total step : 1415 \n",
      "error : 0.825792, accuarcy : 0.770385\n",
      "total step : 1416 \n",
      "error : 0.825552, accuarcy : 0.770885\n",
      "total step : 1417 \n",
      "error : 0.825313, accuarcy : 0.770885\n",
      "total step : 1418 \n",
      "error : 0.825073, accuarcy : 0.770885\n",
      "total step : 1419 \n",
      "error : 0.824834, accuarcy : 0.770885\n",
      "total step : 1420 \n",
      "error : 0.824596, accuarcy : 0.771386\n",
      "total step : 1421 \n",
      "error : 0.824357, accuarcy : 0.771386\n",
      "total step : 1422 \n",
      "error : 0.824119, accuarcy : 0.771386\n",
      "total step : 1423 \n",
      "error : 0.823881, accuarcy : 0.771386\n",
      "total step : 1424 \n",
      "error : 0.823643, accuarcy : 0.771386\n",
      "total step : 1425 \n",
      "error : 0.823405, accuarcy : 0.771386\n",
      "total step : 1426 \n",
      "error : 0.823168, accuarcy : 0.771386\n",
      "total step : 1427 \n",
      "error : 0.822930, accuarcy : 0.771386\n",
      "total step : 1428 \n",
      "error : 0.822693, accuarcy : 0.771386\n",
      "total step : 1429 \n",
      "error : 0.822457, accuarcy : 0.771386\n",
      "total step : 1430 \n",
      "error : 0.822220, accuarcy : 0.771386\n",
      "total step : 1431 \n",
      "error : 0.821984, accuarcy : 0.771386\n",
      "total step : 1432 \n",
      "error : 0.821748, accuarcy : 0.771386\n",
      "total step : 1433 \n",
      "error : 0.821512, accuarcy : 0.771386\n",
      "total step : 1434 \n",
      "error : 0.821277, accuarcy : 0.771386\n",
      "total step : 1435 \n",
      "error : 0.821041, accuarcy : 0.771386\n",
      "total step : 1436 \n",
      "error : 0.820806, accuarcy : 0.771886\n",
      "total step : 1437 \n",
      "error : 0.820571, accuarcy : 0.771886\n",
      "total step : 1438 \n",
      "error : 0.820336, accuarcy : 0.771886\n",
      "total step : 1439 \n",
      "error : 0.820102, accuarcy : 0.771886\n",
      "total step : 1440 \n",
      "error : 0.819868, accuarcy : 0.771886\n",
      "total step : 1441 \n",
      "error : 0.819634, accuarcy : 0.771886\n",
      "total step : 1442 \n",
      "error : 0.819400, accuarcy : 0.771886\n",
      "total step : 1443 \n",
      "error : 0.819166, accuarcy : 0.772386\n",
      "total step : 1444 \n",
      "error : 0.818933, accuarcy : 0.772386\n",
      "total step : 1445 \n",
      "error : 0.818700, accuarcy : 0.772386\n",
      "total step : 1446 \n",
      "error : 0.818467, accuarcy : 0.772386\n",
      "total step : 1447 \n",
      "error : 0.818234, accuarcy : 0.772386\n",
      "total step : 1448 \n",
      "error : 0.818002, accuarcy : 0.772386\n",
      "total step : 1449 \n",
      "error : 0.817770, accuarcy : 0.772386\n",
      "total step : 1450 \n",
      "error : 0.817538, accuarcy : 0.772386\n",
      "total step : 1451 \n",
      "error : 0.817306, accuarcy : 0.772386\n",
      "total step : 1452 \n",
      "error : 0.817074, accuarcy : 0.772886\n",
      "total step : 1453 \n",
      "error : 0.816843, accuarcy : 0.772886\n",
      "total step : 1454 \n",
      "error : 0.816612, accuarcy : 0.772886\n",
      "total step : 1455 \n",
      "error : 0.816381, accuarcy : 0.772886\n",
      "total step : 1456 \n",
      "error : 0.816150, accuarcy : 0.773387\n",
      "total step : 1457 \n",
      "error : 0.815920, accuarcy : 0.773387\n",
      "total step : 1458 \n",
      "error : 0.815690, accuarcy : 0.773387\n",
      "total step : 1459 \n",
      "error : 0.815460, accuarcy : 0.773387\n",
      "total step : 1460 \n",
      "error : 0.815230, accuarcy : 0.773387\n",
      "total step : 1461 \n",
      "error : 0.815000, accuarcy : 0.773387\n",
      "total step : 1462 \n",
      "error : 0.814771, accuarcy : 0.773387\n",
      "total step : 1463 \n",
      "error : 0.814542, accuarcy : 0.773387\n",
      "total step : 1464 \n",
      "error : 0.814313, accuarcy : 0.773387\n",
      "total step : 1465 \n",
      "error : 0.814084, accuarcy : 0.773387\n",
      "total step : 1466 \n",
      "error : 0.813856, accuarcy : 0.773387\n",
      "total step : 1467 \n",
      "error : 0.813627, accuarcy : 0.773387\n",
      "total step : 1468 \n",
      "error : 0.813399, accuarcy : 0.774387\n",
      "total step : 1469 \n",
      "error : 0.813171, accuarcy : 0.774387\n",
      "total step : 1470 \n",
      "error : 0.812944, accuarcy : 0.774387\n",
      "total step : 1471 \n",
      "error : 0.812716, accuarcy : 0.774387\n",
      "total step : 1472 \n",
      "error : 0.812489, accuarcy : 0.774387\n",
      "total step : 1473 \n",
      "error : 0.812262, accuarcy : 0.774387\n",
      "total step : 1474 \n",
      "error : 0.812035, accuarcy : 0.774387\n",
      "total step : 1475 \n",
      "error : 0.811809, accuarcy : 0.774387\n",
      "total step : 1476 \n",
      "error : 0.811582, accuarcy : 0.774387\n",
      "total step : 1477 \n",
      "error : 0.811356, accuarcy : 0.774387\n",
      "total step : 1478 \n",
      "error : 0.811130, accuarcy : 0.774387\n",
      "total step : 1479 \n",
      "error : 0.810904, accuarcy : 0.774387\n",
      "total step : 1480 \n",
      "error : 0.810679, accuarcy : 0.774387\n",
      "total step : 1481 \n",
      "error : 0.810454, accuarcy : 0.774387\n",
      "total step : 1482 \n",
      "error : 0.810228, accuarcy : 0.774387\n",
      "total step : 1483 \n",
      "error : 0.810004, accuarcy : 0.774387\n",
      "total step : 1484 \n",
      "error : 0.809779, accuarcy : 0.774387\n",
      "total step : 1485 \n",
      "error : 0.809554, accuarcy : 0.774387\n",
      "total step : 1486 \n",
      "error : 0.809330, accuarcy : 0.774387\n",
      "total step : 1487 \n",
      "error : 0.809106, accuarcy : 0.774387\n",
      "total step : 1488 \n",
      "error : 0.808882, accuarcy : 0.774387\n",
      "total step : 1489 \n",
      "error : 0.808659, accuarcy : 0.774387\n",
      "total step : 1490 \n",
      "error : 0.808435, accuarcy : 0.774387\n",
      "total step : 1491 \n",
      "error : 0.808212, accuarcy : 0.774387\n",
      "total step : 1492 \n",
      "error : 0.807989, accuarcy : 0.774387\n",
      "total step : 1493 \n",
      "error : 0.807766, accuarcy : 0.774887\n",
      "total step : 1494 \n",
      "error : 0.807543, accuarcy : 0.774887\n",
      "total step : 1495 \n",
      "error : 0.807321, accuarcy : 0.774887\n",
      "total step : 1496 \n",
      "error : 0.807099, accuarcy : 0.774887\n",
      "total step : 1497 \n",
      "error : 0.806877, accuarcy : 0.774887\n",
      "total step : 1498 \n",
      "error : 0.806655, accuarcy : 0.774887\n",
      "total step : 1499 \n",
      "error : 0.806433, accuarcy : 0.774887\n",
      "total step : 1500 \n",
      "error : 0.806212, accuarcy : 0.774887\n",
      "total step : 1501 \n",
      "error : 0.805991, accuarcy : 0.774887\n",
      "total step : 1502 \n",
      "error : 0.805770, accuarcy : 0.774887\n",
      "total step : 1503 \n",
      "error : 0.805549, accuarcy : 0.774887\n",
      "total step : 1504 \n",
      "error : 0.805328, accuarcy : 0.774887\n",
      "total step : 1505 \n",
      "error : 0.805108, accuarcy : 0.775388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 1506 \n",
      "error : 0.804888, accuarcy : 0.775388\n",
      "total step : 1507 \n",
      "error : 0.804668, accuarcy : 0.775388\n",
      "total step : 1508 \n",
      "error : 0.804448, accuarcy : 0.775388\n",
      "total step : 1509 \n",
      "error : 0.804228, accuarcy : 0.775388\n",
      "total step : 1510 \n",
      "error : 0.804009, accuarcy : 0.775388\n",
      "total step : 1511 \n",
      "error : 0.803790, accuarcy : 0.775388\n",
      "total step : 1512 \n",
      "error : 0.803571, accuarcy : 0.775388\n",
      "total step : 1513 \n",
      "error : 0.803352, accuarcy : 0.775888\n",
      "total step : 1514 \n",
      "error : 0.803133, accuarcy : 0.775888\n",
      "total step : 1515 \n",
      "error : 0.802915, accuarcy : 0.775888\n",
      "total step : 1516 \n",
      "error : 0.802697, accuarcy : 0.775888\n",
      "total step : 1517 \n",
      "error : 0.802479, accuarcy : 0.775888\n",
      "total step : 1518 \n",
      "error : 0.802261, accuarcy : 0.775888\n",
      "total step : 1519 \n",
      "error : 0.802043, accuarcy : 0.775888\n",
      "total step : 1520 \n",
      "error : 0.801826, accuarcy : 0.775888\n",
      "total step : 1521 \n",
      "error : 0.801609, accuarcy : 0.775888\n",
      "total step : 1522 \n",
      "error : 0.801392, accuarcy : 0.775888\n",
      "total step : 1523 \n",
      "error : 0.801175, accuarcy : 0.775888\n",
      "total step : 1524 \n",
      "error : 0.800958, accuarcy : 0.775888\n",
      "total step : 1525 \n",
      "error : 0.800742, accuarcy : 0.775888\n",
      "total step : 1526 \n",
      "error : 0.800526, accuarcy : 0.775888\n",
      "total step : 1527 \n",
      "error : 0.800310, accuarcy : 0.775888\n",
      "total step : 1528 \n",
      "error : 0.800094, accuarcy : 0.775888\n",
      "total step : 1529 \n",
      "error : 0.799878, accuarcy : 0.775888\n",
      "total step : 1530 \n",
      "error : 0.799663, accuarcy : 0.775888\n",
      "total step : 1531 \n",
      "error : 0.799447, accuarcy : 0.775888\n",
      "total step : 1532 \n",
      "error : 0.799232, accuarcy : 0.775888\n",
      "total step : 1533 \n",
      "error : 0.799017, accuarcy : 0.775888\n",
      "total step : 1534 \n",
      "error : 0.798803, accuarcy : 0.775888\n",
      "total step : 1535 \n",
      "error : 0.798588, accuarcy : 0.775888\n",
      "total step : 1536 \n",
      "error : 0.798374, accuarcy : 0.775888\n",
      "total step : 1537 \n",
      "error : 0.798160, accuarcy : 0.775888\n",
      "total step : 1538 \n",
      "error : 0.797946, accuarcy : 0.775888\n",
      "total step : 1539 \n",
      "error : 0.797732, accuarcy : 0.776388\n",
      "total step : 1540 \n",
      "error : 0.797519, accuarcy : 0.776388\n",
      "total step : 1541 \n",
      "error : 0.797305, accuarcy : 0.776388\n",
      "total step : 1542 \n",
      "error : 0.797092, accuarcy : 0.776388\n",
      "total step : 1543 \n",
      "error : 0.796879, accuarcy : 0.776388\n",
      "total step : 1544 \n",
      "error : 0.796666, accuarcy : 0.776388\n",
      "total step : 1545 \n",
      "error : 0.796454, accuarcy : 0.776388\n",
      "total step : 1546 \n",
      "error : 0.796241, accuarcy : 0.776388\n",
      "total step : 1547 \n",
      "error : 0.796029, accuarcy : 0.776388\n",
      "total step : 1548 \n",
      "error : 0.795817, accuarcy : 0.776388\n",
      "total step : 1549 \n",
      "error : 0.795605, accuarcy : 0.776388\n",
      "total step : 1550 \n",
      "error : 0.795394, accuarcy : 0.776388\n",
      "total step : 1551 \n",
      "error : 0.795182, accuarcy : 0.776388\n",
      "total step : 1552 \n",
      "error : 0.794971, accuarcy : 0.776388\n",
      "total step : 1553 \n",
      "error : 0.794760, accuarcy : 0.776388\n",
      "total step : 1554 \n",
      "error : 0.794549, accuarcy : 0.776388\n",
      "total step : 1555 \n",
      "error : 0.794338, accuarcy : 0.776388\n",
      "total step : 1556 \n",
      "error : 0.794128, accuarcy : 0.776388\n",
      "total step : 1557 \n",
      "error : 0.793917, accuarcy : 0.776388\n",
      "total step : 1558 \n",
      "error : 0.793707, accuarcy : 0.776388\n",
      "total step : 1559 \n",
      "error : 0.793497, accuarcy : 0.775888\n",
      "total step : 1560 \n",
      "error : 0.793287, accuarcy : 0.775888\n",
      "total step : 1561 \n",
      "error : 0.793078, accuarcy : 0.775888\n",
      "total step : 1562 \n",
      "error : 0.792868, accuarcy : 0.776388\n",
      "total step : 1563 \n",
      "error : 0.792659, accuarcy : 0.776388\n",
      "total step : 1564 \n",
      "error : 0.792450, accuarcy : 0.776388\n",
      "total step : 1565 \n",
      "error : 0.792241, accuarcy : 0.776388\n",
      "total step : 1566 \n",
      "error : 0.792033, accuarcy : 0.776388\n",
      "total step : 1567 \n",
      "error : 0.791824, accuarcy : 0.776388\n",
      "total step : 1568 \n",
      "error : 0.791616, accuarcy : 0.776388\n",
      "total step : 1569 \n",
      "error : 0.791408, accuarcy : 0.776388\n",
      "total step : 1570 \n",
      "error : 0.791200, accuarcy : 0.776388\n",
      "total step : 1571 \n",
      "error : 0.790992, accuarcy : 0.776388\n",
      "total step : 1572 \n",
      "error : 0.790784, accuarcy : 0.776388\n",
      "total step : 1573 \n",
      "error : 0.790577, accuarcy : 0.776388\n",
      "total step : 1574 \n",
      "error : 0.790370, accuarcy : 0.776388\n",
      "total step : 1575 \n",
      "error : 0.790163, accuarcy : 0.776388\n",
      "total step : 1576 \n",
      "error : 0.789956, accuarcy : 0.776388\n",
      "total step : 1577 \n",
      "error : 0.789749, accuarcy : 0.776388\n",
      "total step : 1578 \n",
      "error : 0.789542, accuarcy : 0.776388\n",
      "total step : 1579 \n",
      "error : 0.789336, accuarcy : 0.776388\n",
      "total step : 1580 \n",
      "error : 0.789130, accuarcy : 0.776388\n",
      "total step : 1581 \n",
      "error : 0.788924, accuarcy : 0.776388\n",
      "total step : 1582 \n",
      "error : 0.788718, accuarcy : 0.776388\n",
      "total step : 1583 \n",
      "error : 0.788512, accuarcy : 0.776888\n",
      "total step : 1584 \n",
      "error : 0.788307, accuarcy : 0.776888\n",
      "total step : 1585 \n",
      "error : 0.788102, accuarcy : 0.776888\n",
      "total step : 1586 \n",
      "error : 0.787897, accuarcy : 0.776888\n",
      "total step : 1587 \n",
      "error : 0.787692, accuarcy : 0.776888\n",
      "total step : 1588 \n",
      "error : 0.787487, accuarcy : 0.776888\n",
      "total step : 1589 \n",
      "error : 0.787282, accuarcy : 0.777389\n",
      "total step : 1590 \n",
      "error : 0.787078, accuarcy : 0.777389\n",
      "total step : 1591 \n",
      "error : 0.786874, accuarcy : 0.777389\n",
      "total step : 1592 \n",
      "error : 0.786670, accuarcy : 0.777389\n",
      "total step : 1593 \n",
      "error : 0.786466, accuarcy : 0.777389\n",
      "total step : 1594 \n",
      "error : 0.786262, accuarcy : 0.777889\n",
      "total step : 1595 \n",
      "error : 0.786059, accuarcy : 0.777889\n",
      "total step : 1596 \n",
      "error : 0.785855, accuarcy : 0.777889\n",
      "total step : 1597 \n",
      "error : 0.785652, accuarcy : 0.777889\n",
      "total step : 1598 \n",
      "error : 0.785449, accuarcy : 0.778389\n",
      "total step : 1599 \n",
      "error : 0.785246, accuarcy : 0.778389\n",
      "total step : 1600 \n",
      "error : 0.785044, accuarcy : 0.778389\n",
      "total step : 1601 \n",
      "error : 0.784841, accuarcy : 0.778389\n",
      "total step : 1602 \n",
      "error : 0.784639, accuarcy : 0.778389\n",
      "total step : 1603 \n",
      "error : 0.784437, accuarcy : 0.778389\n",
      "total step : 1604 \n",
      "error : 0.784235, accuarcy : 0.778389\n",
      "total step : 1605 \n",
      "error : 0.784033, accuarcy : 0.778389\n",
      "total step : 1606 \n",
      "error : 0.783832, accuarcy : 0.778389\n",
      "total step : 1607 \n",
      "error : 0.783630, accuarcy : 0.778389\n",
      "total step : 1608 \n",
      "error : 0.783429, accuarcy : 0.778389\n",
      "total step : 1609 \n",
      "error : 0.783228, accuarcy : 0.778389\n",
      "total step : 1610 \n",
      "error : 0.783027, accuarcy : 0.778389\n",
      "total step : 1611 \n",
      "error : 0.782826, accuarcy : 0.778389\n",
      "total step : 1612 \n",
      "error : 0.782625, accuarcy : 0.778389\n",
      "total step : 1613 \n",
      "error : 0.782425, accuarcy : 0.778389\n",
      "total step : 1614 \n",
      "error : 0.782225, accuarcy : 0.778389\n",
      "total step : 1615 \n",
      "error : 0.782025, accuarcy : 0.778389\n",
      "total step : 1616 \n",
      "error : 0.781825, accuarcy : 0.778389\n",
      "total step : 1617 \n",
      "error : 0.781625, accuarcy : 0.778389\n",
      "total step : 1618 \n",
      "error : 0.781426, accuarcy : 0.778389\n",
      "total step : 1619 \n",
      "error : 0.781226, accuarcy : 0.778389\n",
      "total step : 1620 \n",
      "error : 0.781027, accuarcy : 0.778389\n",
      "total step : 1621 \n",
      "error : 0.780828, accuarcy : 0.778389\n",
      "total step : 1622 \n",
      "error : 0.780629, accuarcy : 0.778389\n",
      "total step : 1623 \n",
      "error : 0.780430, accuarcy : 0.778389\n",
      "total step : 1624 \n",
      "error : 0.780232, accuarcy : 0.778389\n",
      "total step : 1625 \n",
      "error : 0.780033, accuarcy : 0.778389\n",
      "total step : 1626 \n",
      "error : 0.779835, accuarcy : 0.778389\n",
      "total step : 1627 \n",
      "error : 0.779637, accuarcy : 0.778389\n",
      "total step : 1628 \n",
      "error : 0.779439, accuarcy : 0.778889\n",
      "total step : 1629 \n",
      "error : 0.779241, accuarcy : 0.778889\n",
      "total step : 1630 \n",
      "error : 0.779044, accuarcy : 0.778889\n",
      "total step : 1631 \n",
      "error : 0.778846, accuarcy : 0.778889\n",
      "total step : 1632 \n",
      "error : 0.778649, accuarcy : 0.778889\n",
      "total step : 1633 \n",
      "error : 0.778452, accuarcy : 0.778889\n",
      "total step : 1634 \n",
      "error : 0.778255, accuarcy : 0.778889\n",
      "total step : 1635 \n",
      "error : 0.778058, accuarcy : 0.779390\n",
      "total step : 1636 \n",
      "error : 0.777862, accuarcy : 0.779390\n",
      "total step : 1637 \n",
      "error : 0.777665, accuarcy : 0.779390\n",
      "total step : 1638 \n",
      "error : 0.777469, accuarcy : 0.779390\n",
      "total step : 1639 \n",
      "error : 0.777273, accuarcy : 0.779390\n",
      "total step : 1640 \n",
      "error : 0.777077, accuarcy : 0.779390\n",
      "total step : 1641 \n",
      "error : 0.776881, accuarcy : 0.779390\n",
      "total step : 1642 \n",
      "error : 0.776686, accuarcy : 0.779390\n",
      "total step : 1643 \n",
      "error : 0.776490, accuarcy : 0.779390\n",
      "total step : 1644 \n",
      "error : 0.776295, accuarcy : 0.779390\n",
      "total step : 1645 \n",
      "error : 0.776100, accuarcy : 0.779390\n",
      "total step : 1646 \n",
      "error : 0.775905, accuarcy : 0.779390\n",
      "total step : 1647 \n",
      "error : 0.775710, accuarcy : 0.779390\n",
      "total step : 1648 \n",
      "error : 0.775516, accuarcy : 0.779390\n",
      "total step : 1649 \n",
      "error : 0.775321, accuarcy : 0.779890\n",
      "total step : 1650 \n",
      "error : 0.775127, accuarcy : 0.779890\n",
      "total step : 1651 \n",
      "error : 0.774933, accuarcy : 0.779890\n",
      "total step : 1652 \n",
      "error : 0.774739, accuarcy : 0.779890\n",
      "total step : 1653 \n",
      "error : 0.774545, accuarcy : 0.779890\n",
      "total step : 1654 \n",
      "error : 0.774351, accuarcy : 0.779890\n",
      "total step : 1655 \n",
      "error : 0.774158, accuarcy : 0.779890\n",
      "total step : 1656 \n",
      "error : 0.773964, accuarcy : 0.779890\n",
      "total step : 1657 \n",
      "error : 0.773771, accuarcy : 0.779890\n",
      "total step : 1658 \n",
      "error : 0.773578, accuarcy : 0.779890\n",
      "total step : 1659 \n",
      "error : 0.773385, accuarcy : 0.779890\n",
      "total step : 1660 \n",
      "error : 0.773193, accuarcy : 0.779890\n",
      "total step : 1661 \n",
      "error : 0.773000, accuarcy : 0.779890\n",
      "total step : 1662 \n",
      "error : 0.772808, accuarcy : 0.779890\n",
      "total step : 1663 \n",
      "error : 0.772615, accuarcy : 0.779890\n",
      "total step : 1664 \n",
      "error : 0.772423, accuarcy : 0.779890\n",
      "total step : 1665 \n",
      "error : 0.772231, accuarcy : 0.779890\n",
      "total step : 1666 \n",
      "error : 0.772040, accuarcy : 0.779890\n",
      "total step : 1667 \n",
      "error : 0.771848, accuarcy : 0.779890\n",
      "total step : 1668 \n",
      "error : 0.771657, accuarcy : 0.779890\n",
      "total step : 1669 \n",
      "error : 0.771465, accuarcy : 0.779890\n",
      "total step : 1670 \n",
      "error : 0.771274, accuarcy : 0.779890\n",
      "total step : 1671 \n",
      "error : 0.771083, accuarcy : 0.779890\n",
      "total step : 1672 \n",
      "error : 0.770892, accuarcy : 0.779890\n",
      "total step : 1673 \n",
      "error : 0.770702, accuarcy : 0.779890\n",
      "total step : 1674 \n",
      "error : 0.770511, accuarcy : 0.779890\n",
      "total step : 1675 \n",
      "error : 0.770321, accuarcy : 0.779890\n",
      "total step : 1676 \n",
      "error : 0.770131, accuarcy : 0.779890\n",
      "total step : 1677 \n",
      "error : 0.769941, accuarcy : 0.779890\n",
      "total step : 1678 \n",
      "error : 0.769751, accuarcy : 0.779890\n",
      "total step : 1679 \n",
      "error : 0.769561, accuarcy : 0.779890\n",
      "total step : 1680 \n",
      "error : 0.769371, accuarcy : 0.779890\n",
      "total step : 1681 \n",
      "error : 0.769182, accuarcy : 0.780390\n",
      "total step : 1682 \n",
      "error : 0.768993, accuarcy : 0.780390\n",
      "total step : 1683 \n",
      "error : 0.768804, accuarcy : 0.780390\n",
      "total step : 1684 \n",
      "error : 0.768615, accuarcy : 0.780390\n",
      "total step : 1685 \n",
      "error : 0.768426, accuarcy : 0.780390\n",
      "total step : 1686 \n",
      "error : 0.768237, accuarcy : 0.780890\n",
      "total step : 1687 \n",
      "error : 0.768049, accuarcy : 0.780890\n",
      "total step : 1688 \n",
      "error : 0.767860, accuarcy : 0.781391\n",
      "total step : 1689 \n",
      "error : 0.767672, accuarcy : 0.781391\n",
      "total step : 1690 \n",
      "error : 0.767484, accuarcy : 0.781391\n",
      "total step : 1691 \n",
      "error : 0.767296, accuarcy : 0.781391\n",
      "total step : 1692 \n",
      "error : 0.767108, accuarcy : 0.781391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 1693 \n",
      "error : 0.766921, accuarcy : 0.781391\n",
      "total step : 1694 \n",
      "error : 0.766733, accuarcy : 0.781391\n",
      "total step : 1695 \n",
      "error : 0.766546, accuarcy : 0.781391\n",
      "total step : 1696 \n",
      "error : 0.766359, accuarcy : 0.781391\n",
      "total step : 1697 \n",
      "error : 0.766172, accuarcy : 0.780890\n",
      "total step : 1698 \n",
      "error : 0.765985, accuarcy : 0.780890\n",
      "total step : 1699 \n",
      "error : 0.765798, accuarcy : 0.780890\n",
      "total step : 1700 \n",
      "error : 0.765612, accuarcy : 0.780890\n",
      "total step : 1701 \n",
      "error : 0.765425, accuarcy : 0.780890\n",
      "total step : 1702 \n",
      "error : 0.765239, accuarcy : 0.780890\n",
      "total step : 1703 \n",
      "error : 0.765053, accuarcy : 0.780890\n",
      "total step : 1704 \n",
      "error : 0.764867, accuarcy : 0.780890\n",
      "total step : 1705 \n",
      "error : 0.764681, accuarcy : 0.780890\n",
      "total step : 1706 \n",
      "error : 0.764495, accuarcy : 0.780890\n",
      "total step : 1707 \n",
      "error : 0.764310, accuarcy : 0.780890\n",
      "total step : 1708 \n",
      "error : 0.764125, accuarcy : 0.780890\n",
      "total step : 1709 \n",
      "error : 0.763939, accuarcy : 0.781391\n",
      "total step : 1710 \n",
      "error : 0.763754, accuarcy : 0.781391\n",
      "total step : 1711 \n",
      "error : 0.763569, accuarcy : 0.781391\n",
      "total step : 1712 \n",
      "error : 0.763385, accuarcy : 0.781391\n",
      "total step : 1713 \n",
      "error : 0.763200, accuarcy : 0.781391\n",
      "total step : 1714 \n",
      "error : 0.763015, accuarcy : 0.781391\n",
      "total step : 1715 \n",
      "error : 0.762831, accuarcy : 0.781391\n",
      "total step : 1716 \n",
      "error : 0.762647, accuarcy : 0.781391\n",
      "total step : 1717 \n",
      "error : 0.762463, accuarcy : 0.781391\n",
      "total step : 1718 \n",
      "error : 0.762279, accuarcy : 0.781891\n",
      "total step : 1719 \n",
      "error : 0.762095, accuarcy : 0.781891\n",
      "total step : 1720 \n",
      "error : 0.761912, accuarcy : 0.781891\n",
      "total step : 1721 \n",
      "error : 0.761728, accuarcy : 0.781891\n",
      "total step : 1722 \n",
      "error : 0.761545, accuarcy : 0.781891\n",
      "total step : 1723 \n",
      "error : 0.761362, accuarcy : 0.781891\n",
      "total step : 1724 \n",
      "error : 0.761179, accuarcy : 0.781891\n",
      "total step : 1725 \n",
      "error : 0.760996, accuarcy : 0.781891\n",
      "total step : 1726 \n",
      "error : 0.760813, accuarcy : 0.781891\n",
      "total step : 1727 \n",
      "error : 0.760630, accuarcy : 0.781891\n",
      "total step : 1728 \n",
      "error : 0.760448, accuarcy : 0.781891\n",
      "total step : 1729 \n",
      "error : 0.760266, accuarcy : 0.781891\n",
      "total step : 1730 \n",
      "error : 0.760083, accuarcy : 0.781891\n",
      "total step : 1731 \n",
      "error : 0.759901, accuarcy : 0.781891\n",
      "total step : 1732 \n",
      "error : 0.759720, accuarcy : 0.781891\n",
      "total step : 1733 \n",
      "error : 0.759538, accuarcy : 0.781891\n",
      "total step : 1734 \n",
      "error : 0.759356, accuarcy : 0.781891\n",
      "total step : 1735 \n",
      "error : 0.759175, accuarcy : 0.781891\n",
      "total step : 1736 \n",
      "error : 0.758993, accuarcy : 0.781891\n",
      "total step : 1737 \n",
      "error : 0.758812, accuarcy : 0.781891\n",
      "total step : 1738 \n",
      "error : 0.758631, accuarcy : 0.781891\n",
      "total step : 1739 \n",
      "error : 0.758450, accuarcy : 0.781891\n",
      "total step : 1740 \n",
      "error : 0.758270, accuarcy : 0.781891\n",
      "total step : 1741 \n",
      "error : 0.758089, accuarcy : 0.781891\n",
      "total step : 1742 \n",
      "error : 0.757909, accuarcy : 0.781891\n",
      "total step : 1743 \n",
      "error : 0.757728, accuarcy : 0.781891\n",
      "total step : 1744 \n",
      "error : 0.757548, accuarcy : 0.781891\n",
      "total step : 1745 \n",
      "error : 0.757368, accuarcy : 0.781891\n",
      "total step : 1746 \n",
      "error : 0.757188, accuarcy : 0.781891\n",
      "total step : 1747 \n",
      "error : 0.757008, accuarcy : 0.781891\n",
      "total step : 1748 \n",
      "error : 0.756829, accuarcy : 0.781891\n",
      "total step : 1749 \n",
      "error : 0.756649, accuarcy : 0.781891\n",
      "total step : 1750 \n",
      "error : 0.756470, accuarcy : 0.781891\n",
      "total step : 1751 \n",
      "error : 0.756291, accuarcy : 0.781891\n",
      "total step : 1752 \n",
      "error : 0.756112, accuarcy : 0.781891\n",
      "total step : 1753 \n",
      "error : 0.755933, accuarcy : 0.781891\n",
      "total step : 1754 \n",
      "error : 0.755754, accuarcy : 0.781891\n",
      "total step : 1755 \n",
      "error : 0.755575, accuarcy : 0.782391\n",
      "total step : 1756 \n",
      "error : 0.755397, accuarcy : 0.782391\n",
      "total step : 1757 \n",
      "error : 0.755218, accuarcy : 0.782391\n",
      "total step : 1758 \n",
      "error : 0.755040, accuarcy : 0.782391\n",
      "total step : 1759 \n",
      "error : 0.754862, accuarcy : 0.782391\n",
      "total step : 1760 \n",
      "error : 0.754684, accuarcy : 0.782391\n",
      "total step : 1761 \n",
      "error : 0.754506, accuarcy : 0.782391\n",
      "total step : 1762 \n",
      "error : 0.754329, accuarcy : 0.782391\n",
      "total step : 1763 \n",
      "error : 0.754151, accuarcy : 0.782391\n",
      "total step : 1764 \n",
      "error : 0.753974, accuarcy : 0.782391\n",
      "total step : 1765 \n",
      "error : 0.753796, accuarcy : 0.782391\n",
      "total step : 1766 \n",
      "error : 0.753619, accuarcy : 0.782391\n",
      "total step : 1767 \n",
      "error : 0.753442, accuarcy : 0.782391\n",
      "total step : 1768 \n",
      "error : 0.753265, accuarcy : 0.782391\n",
      "total step : 1769 \n",
      "error : 0.753089, accuarcy : 0.782391\n",
      "total step : 1770 \n",
      "error : 0.752912, accuarcy : 0.782391\n",
      "total step : 1771 \n",
      "error : 0.752736, accuarcy : 0.782891\n",
      "total step : 1772 \n",
      "error : 0.752559, accuarcy : 0.782391\n",
      "total step : 1773 \n",
      "error : 0.752383, accuarcy : 0.782891\n",
      "total step : 1774 \n",
      "error : 0.752207, accuarcy : 0.783392\n",
      "total step : 1775 \n",
      "error : 0.752031, accuarcy : 0.783392\n",
      "total step : 1776 \n",
      "error : 0.751855, accuarcy : 0.783392\n",
      "total step : 1777 \n",
      "error : 0.751680, accuarcy : 0.783392\n",
      "total step : 1778 \n",
      "error : 0.751504, accuarcy : 0.783392\n",
      "total step : 1779 \n",
      "error : 0.751329, accuarcy : 0.783892\n",
      "total step : 1780 \n",
      "error : 0.751154, accuarcy : 0.783892\n",
      "total step : 1781 \n",
      "error : 0.750978, accuarcy : 0.783892\n",
      "total step : 1782 \n",
      "error : 0.750804, accuarcy : 0.783892\n",
      "total step : 1783 \n",
      "error : 0.750629, accuarcy : 0.783892\n",
      "total step : 1784 \n",
      "error : 0.750454, accuarcy : 0.783892\n",
      "total step : 1785 \n",
      "error : 0.750279, accuarcy : 0.783892\n",
      "total step : 1786 \n",
      "error : 0.750105, accuarcy : 0.783892\n",
      "total step : 1787 \n",
      "error : 0.749931, accuarcy : 0.783892\n",
      "total step : 1788 \n",
      "error : 0.749756, accuarcy : 0.783892\n",
      "total step : 1789 \n",
      "error : 0.749582, accuarcy : 0.783892\n",
      "total step : 1790 \n",
      "error : 0.749408, accuarcy : 0.783892\n",
      "total step : 1791 \n",
      "error : 0.749235, accuarcy : 0.783892\n",
      "total step : 1792 \n",
      "error : 0.749061, accuarcy : 0.783892\n",
      "total step : 1793 \n",
      "error : 0.748887, accuarcy : 0.783892\n",
      "total step : 1794 \n",
      "error : 0.748714, accuarcy : 0.783892\n",
      "total step : 1795 \n",
      "error : 0.748541, accuarcy : 0.783892\n",
      "total step : 1796 \n",
      "error : 0.748368, accuarcy : 0.783892\n",
      "total step : 1797 \n",
      "error : 0.748195, accuarcy : 0.784392\n",
      "total step : 1798 \n",
      "error : 0.748022, accuarcy : 0.784392\n",
      "total step : 1799 \n",
      "error : 0.747849, accuarcy : 0.784892\n",
      "total step : 1800 \n",
      "error : 0.747676, accuarcy : 0.784892\n",
      "total step : 1801 \n",
      "error : 0.747504, accuarcy : 0.784892\n",
      "total step : 1802 \n",
      "error : 0.747332, accuarcy : 0.784892\n",
      "total step : 1803 \n",
      "error : 0.747159, accuarcy : 0.784892\n",
      "total step : 1804 \n",
      "error : 0.746987, accuarcy : 0.784892\n",
      "total step : 1805 \n",
      "error : 0.746815, accuarcy : 0.785393\n",
      "total step : 1806 \n",
      "error : 0.746644, accuarcy : 0.785393\n",
      "total step : 1807 \n",
      "error : 0.746472, accuarcy : 0.785393\n",
      "total step : 1808 \n",
      "error : 0.746300, accuarcy : 0.786893\n",
      "total step : 1809 \n",
      "error : 0.746129, accuarcy : 0.786893\n",
      "total step : 1810 \n",
      "error : 0.745958, accuarcy : 0.786893\n",
      "total step : 1811 \n",
      "error : 0.745786, accuarcy : 0.786893\n",
      "total step : 1812 \n",
      "error : 0.745615, accuarcy : 0.786893\n",
      "total step : 1813 \n",
      "error : 0.745444, accuarcy : 0.787394\n",
      "total step : 1814 \n",
      "error : 0.745274, accuarcy : 0.787394\n",
      "total step : 1815 \n",
      "error : 0.745103, accuarcy : 0.787394\n",
      "total step : 1816 \n",
      "error : 0.744932, accuarcy : 0.787394\n",
      "total step : 1817 \n",
      "error : 0.744762, accuarcy : 0.787394\n",
      "total step : 1818 \n",
      "error : 0.744592, accuarcy : 0.787394\n",
      "total step : 1819 \n",
      "error : 0.744421, accuarcy : 0.787394\n",
      "total step : 1820 \n",
      "error : 0.744251, accuarcy : 0.787394\n",
      "total step : 1821 \n",
      "error : 0.744081, accuarcy : 0.787394\n",
      "total step : 1822 \n",
      "error : 0.743912, accuarcy : 0.787394\n",
      "total step : 1823 \n",
      "error : 0.743742, accuarcy : 0.787394\n",
      "total step : 1824 \n",
      "error : 0.743572, accuarcy : 0.787394\n",
      "total step : 1825 \n",
      "error : 0.743403, accuarcy : 0.787394\n",
      "total step : 1826 \n",
      "error : 0.743234, accuarcy : 0.787394\n",
      "total step : 1827 \n",
      "error : 0.743065, accuarcy : 0.787394\n",
      "total step : 1828 \n",
      "error : 0.742896, accuarcy : 0.787394\n",
      "total step : 1829 \n",
      "error : 0.742727, accuarcy : 0.787394\n",
      "total step : 1830 \n",
      "error : 0.742558, accuarcy : 0.787394\n",
      "total step : 1831 \n",
      "error : 0.742389, accuarcy : 0.787394\n",
      "total step : 1832 \n",
      "error : 0.742221, accuarcy : 0.787894\n",
      "total step : 1833 \n",
      "error : 0.742052, accuarcy : 0.787894\n",
      "total step : 1834 \n",
      "error : 0.741884, accuarcy : 0.787894\n",
      "total step : 1835 \n",
      "error : 0.741716, accuarcy : 0.787894\n",
      "total step : 1836 \n",
      "error : 0.741548, accuarcy : 0.788394\n",
      "total step : 1837 \n",
      "error : 0.741380, accuarcy : 0.788394\n",
      "total step : 1838 \n",
      "error : 0.741212, accuarcy : 0.788394\n",
      "total step : 1839 \n",
      "error : 0.741045, accuarcy : 0.788394\n",
      "total step : 1840 \n",
      "error : 0.740877, accuarcy : 0.788394\n",
      "total step : 1841 \n",
      "error : 0.740710, accuarcy : 0.788394\n",
      "total step : 1842 \n",
      "error : 0.740542, accuarcy : 0.788394\n",
      "total step : 1843 \n",
      "error : 0.740375, accuarcy : 0.788394\n",
      "total step : 1844 \n",
      "error : 0.740208, accuarcy : 0.788394\n",
      "total step : 1845 \n",
      "error : 0.740041, accuarcy : 0.788394\n",
      "total step : 1846 \n",
      "error : 0.739874, accuarcy : 0.788394\n",
      "total step : 1847 \n",
      "error : 0.739708, accuarcy : 0.788894\n",
      "total step : 1848 \n",
      "error : 0.739541, accuarcy : 0.788894\n",
      "total step : 1849 \n",
      "error : 0.739375, accuarcy : 0.788894\n",
      "total step : 1850 \n",
      "error : 0.739208, accuarcy : 0.788394\n",
      "total step : 1851 \n",
      "error : 0.739042, accuarcy : 0.788394\n",
      "total step : 1852 \n",
      "error : 0.738876, accuarcy : 0.788894\n",
      "total step : 1853 \n",
      "error : 0.738710, accuarcy : 0.788894\n",
      "total step : 1854 \n",
      "error : 0.738544, accuarcy : 0.788894\n",
      "total step : 1855 \n",
      "error : 0.738379, accuarcy : 0.788894\n",
      "total step : 1856 \n",
      "error : 0.738213, accuarcy : 0.788894\n",
      "total step : 1857 \n",
      "error : 0.738048, accuarcy : 0.788894\n",
      "total step : 1858 \n",
      "error : 0.737882, accuarcy : 0.788894\n",
      "total step : 1859 \n",
      "error : 0.737717, accuarcy : 0.788894\n",
      "total step : 1860 \n",
      "error : 0.737552, accuarcy : 0.788894\n",
      "total step : 1861 \n",
      "error : 0.737387, accuarcy : 0.788894\n",
      "total step : 1862 \n",
      "error : 0.737222, accuarcy : 0.788894\n",
      "total step : 1863 \n",
      "error : 0.737057, accuarcy : 0.788894\n",
      "total step : 1864 \n",
      "error : 0.736893, accuarcy : 0.789395\n",
      "total step : 1865 \n",
      "error : 0.736728, accuarcy : 0.789395\n",
      "total step : 1866 \n",
      "error : 0.736564, accuarcy : 0.789395\n",
      "total step : 1867 \n",
      "error : 0.736400, accuarcy : 0.789395\n",
      "total step : 1868 \n",
      "error : 0.736235, accuarcy : 0.789395\n",
      "total step : 1869 \n",
      "error : 0.736071, accuarcy : 0.789395\n",
      "total step : 1870 \n",
      "error : 0.735907, accuarcy : 0.789395\n",
      "total step : 1871 \n",
      "error : 0.735744, accuarcy : 0.789395\n",
      "total step : 1872 \n",
      "error : 0.735580, accuarcy : 0.789395\n",
      "total step : 1873 \n",
      "error : 0.735416, accuarcy : 0.789395\n",
      "total step : 1874 \n",
      "error : 0.735253, accuarcy : 0.789395\n",
      "total step : 1875 \n",
      "error : 0.735090, accuarcy : 0.789395\n",
      "total step : 1876 \n",
      "error : 0.734927, accuarcy : 0.789395\n",
      "total step : 1877 \n",
      "error : 0.734763, accuarcy : 0.789395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 1878 \n",
      "error : 0.734600, accuarcy : 0.789395\n",
      "total step : 1879 \n",
      "error : 0.734438, accuarcy : 0.789895\n",
      "total step : 1880 \n",
      "error : 0.734275, accuarcy : 0.789895\n",
      "total step : 1881 \n",
      "error : 0.734112, accuarcy : 0.789895\n",
      "total step : 1882 \n",
      "error : 0.733950, accuarcy : 0.789895\n",
      "total step : 1883 \n",
      "error : 0.733787, accuarcy : 0.789895\n",
      "total step : 1884 \n",
      "error : 0.733625, accuarcy : 0.789895\n",
      "total step : 1885 \n",
      "error : 0.733463, accuarcy : 0.789895\n",
      "total step : 1886 \n",
      "error : 0.733301, accuarcy : 0.789895\n",
      "total step : 1887 \n",
      "error : 0.733139, accuarcy : 0.789895\n",
      "total step : 1888 \n",
      "error : 0.732977, accuarcy : 0.789895\n",
      "total step : 1889 \n",
      "error : 0.732816, accuarcy : 0.789895\n",
      "total step : 1890 \n",
      "error : 0.732654, accuarcy : 0.789895\n",
      "total step : 1891 \n",
      "error : 0.732493, accuarcy : 0.789895\n",
      "total step : 1892 \n",
      "error : 0.732331, accuarcy : 0.789895\n",
      "total step : 1893 \n",
      "error : 0.732170, accuarcy : 0.789895\n",
      "total step : 1894 \n",
      "error : 0.732009, accuarcy : 0.790395\n",
      "total step : 1895 \n",
      "error : 0.731848, accuarcy : 0.790395\n",
      "total step : 1896 \n",
      "error : 0.731687, accuarcy : 0.790395\n",
      "total step : 1897 \n",
      "error : 0.731526, accuarcy : 0.790395\n",
      "total step : 1898 \n",
      "error : 0.731366, accuarcy : 0.790395\n",
      "total step : 1899 \n",
      "error : 0.731205, accuarcy : 0.790395\n",
      "total step : 1900 \n",
      "error : 0.731045, accuarcy : 0.790395\n",
      "total step : 1901 \n",
      "error : 0.730884, accuarcy : 0.790395\n",
      "total step : 1902 \n",
      "error : 0.730724, accuarcy : 0.790395\n",
      "total step : 1903 \n",
      "error : 0.730564, accuarcy : 0.790395\n",
      "total step : 1904 \n",
      "error : 0.730404, accuarcy : 0.790395\n",
      "total step : 1905 \n",
      "error : 0.730244, accuarcy : 0.790395\n",
      "total step : 1906 \n",
      "error : 0.730085, accuarcy : 0.790395\n",
      "total step : 1907 \n",
      "error : 0.729925, accuarcy : 0.790395\n",
      "total step : 1908 \n",
      "error : 0.729766, accuarcy : 0.790395\n",
      "total step : 1909 \n",
      "error : 0.729606, accuarcy : 0.790395\n",
      "total step : 1910 \n",
      "error : 0.729447, accuarcy : 0.790395\n",
      "total step : 1911 \n",
      "error : 0.729288, accuarcy : 0.790395\n",
      "total step : 1912 \n",
      "error : 0.729129, accuarcy : 0.790395\n",
      "total step : 1913 \n",
      "error : 0.728970, accuarcy : 0.790395\n",
      "total step : 1914 \n",
      "error : 0.728811, accuarcy : 0.790395\n",
      "total step : 1915 \n",
      "error : 0.728652, accuarcy : 0.790395\n",
      "total step : 1916 \n",
      "error : 0.728494, accuarcy : 0.790395\n",
      "total step : 1917 \n",
      "error : 0.728335, accuarcy : 0.790395\n",
      "total step : 1918 \n",
      "error : 0.728177, accuarcy : 0.790395\n",
      "total step : 1919 \n",
      "error : 0.728019, accuarcy : 0.790395\n",
      "total step : 1920 \n",
      "error : 0.727860, accuarcy : 0.790395\n",
      "total step : 1921 \n",
      "error : 0.727702, accuarcy : 0.790395\n",
      "total step : 1922 \n",
      "error : 0.727544, accuarcy : 0.790395\n",
      "total step : 1923 \n",
      "error : 0.727387, accuarcy : 0.790395\n",
      "total step : 1924 \n",
      "error : 0.727229, accuarcy : 0.790395\n",
      "total step : 1925 \n",
      "error : 0.727071, accuarcy : 0.790395\n",
      "total step : 1926 \n",
      "error : 0.726914, accuarcy : 0.790395\n",
      "total step : 1927 \n",
      "error : 0.726756, accuarcy : 0.790395\n",
      "total step : 1928 \n",
      "error : 0.726599, accuarcy : 0.790395\n",
      "total step : 1929 \n",
      "error : 0.726442, accuarcy : 0.790395\n",
      "total step : 1930 \n",
      "error : 0.726285, accuarcy : 0.790395\n",
      "total step : 1931 \n",
      "error : 0.726128, accuarcy : 0.790895\n",
      "total step : 1932 \n",
      "error : 0.725971, accuarcy : 0.790895\n",
      "total step : 1933 \n",
      "error : 0.725814, accuarcy : 0.790895\n",
      "total step : 1934 \n",
      "error : 0.725658, accuarcy : 0.790895\n",
      "total step : 1935 \n",
      "error : 0.725501, accuarcy : 0.790895\n",
      "total step : 1936 \n",
      "error : 0.725345, accuarcy : 0.790895\n",
      "total step : 1937 \n",
      "error : 0.725189, accuarcy : 0.790395\n",
      "total step : 1938 \n",
      "error : 0.725033, accuarcy : 0.790395\n",
      "total step : 1939 \n",
      "error : 0.724876, accuarcy : 0.790395\n",
      "total step : 1940 \n",
      "error : 0.724721, accuarcy : 0.790395\n",
      "total step : 1941 \n",
      "error : 0.724565, accuarcy : 0.790395\n",
      "total step : 1942 \n",
      "error : 0.724409, accuarcy : 0.790395\n",
      "total step : 1943 \n",
      "error : 0.724253, accuarcy : 0.790395\n",
      "total step : 1944 \n",
      "error : 0.724098, accuarcy : 0.790395\n",
      "total step : 1945 \n",
      "error : 0.723942, accuarcy : 0.790395\n",
      "total step : 1946 \n",
      "error : 0.723787, accuarcy : 0.790395\n",
      "total step : 1947 \n",
      "error : 0.723632, accuarcy : 0.790395\n",
      "total step : 1948 \n",
      "error : 0.723477, accuarcy : 0.790395\n",
      "total step : 1949 \n",
      "error : 0.723322, accuarcy : 0.791396\n",
      "total step : 1950 \n",
      "error : 0.723167, accuarcy : 0.791396\n",
      "total step : 1951 \n",
      "error : 0.723012, accuarcy : 0.791396\n",
      "total step : 1952 \n",
      "error : 0.722858, accuarcy : 0.791396\n",
      "total step : 1953 \n",
      "error : 0.722703, accuarcy : 0.791396\n",
      "total step : 1954 \n",
      "error : 0.722549, accuarcy : 0.791896\n",
      "total step : 1955 \n",
      "error : 0.722394, accuarcy : 0.791896\n",
      "total step : 1956 \n",
      "error : 0.722240, accuarcy : 0.792396\n",
      "total step : 1957 \n",
      "error : 0.722086, accuarcy : 0.792396\n",
      "total step : 1958 \n",
      "error : 0.721932, accuarcy : 0.792396\n",
      "total step : 1959 \n",
      "error : 0.721778, accuarcy : 0.792396\n",
      "total step : 1960 \n",
      "error : 0.721624, accuarcy : 0.792396\n",
      "total step : 1961 \n",
      "error : 0.721470, accuarcy : 0.792396\n",
      "total step : 1962 \n",
      "error : 0.721317, accuarcy : 0.792396\n",
      "total step : 1963 \n",
      "error : 0.721163, accuarcy : 0.792396\n",
      "total step : 1964 \n",
      "error : 0.721010, accuarcy : 0.792396\n",
      "total step : 1965 \n",
      "error : 0.720857, accuarcy : 0.792396\n",
      "total step : 1966 \n",
      "error : 0.720704, accuarcy : 0.792396\n",
      "total step : 1967 \n",
      "error : 0.720550, accuarcy : 0.792396\n",
      "total step : 1968 \n",
      "error : 0.720397, accuarcy : 0.792396\n",
      "total step : 1969 \n",
      "error : 0.720245, accuarcy : 0.793397\n",
      "total step : 1970 \n",
      "error : 0.720092, accuarcy : 0.793397\n",
      "total step : 1971 \n",
      "error : 0.719939, accuarcy : 0.793397\n",
      "total step : 1972 \n",
      "error : 0.719787, accuarcy : 0.793397\n",
      "total step : 1973 \n",
      "error : 0.719634, accuarcy : 0.793397\n",
      "total step : 1974 \n",
      "error : 0.719482, accuarcy : 0.793397\n",
      "total step : 1975 \n",
      "error : 0.719330, accuarcy : 0.793897\n",
      "total step : 1976 \n",
      "error : 0.719178, accuarcy : 0.793897\n",
      "total step : 1977 \n",
      "error : 0.719026, accuarcy : 0.793897\n",
      "total step : 1978 \n",
      "error : 0.718874, accuarcy : 0.793897\n",
      "total step : 1979 \n",
      "error : 0.718722, accuarcy : 0.793897\n",
      "total step : 1980 \n",
      "error : 0.718570, accuarcy : 0.793897\n",
      "total step : 1981 \n",
      "error : 0.718419, accuarcy : 0.793897\n",
      "total step : 1982 \n",
      "error : 0.718267, accuarcy : 0.793897\n",
      "total step : 1983 \n",
      "error : 0.718116, accuarcy : 0.793897\n",
      "total step : 1984 \n",
      "error : 0.717964, accuarcy : 0.793897\n",
      "total step : 1985 \n",
      "error : 0.717813, accuarcy : 0.794397\n",
      "total step : 1986 \n",
      "error : 0.717662, accuarcy : 0.794397\n",
      "total step : 1987 \n",
      "error : 0.717511, accuarcy : 0.794397\n",
      "total step : 1988 \n",
      "error : 0.717360, accuarcy : 0.794397\n",
      "total step : 1989 \n",
      "error : 0.717209, accuarcy : 0.794397\n",
      "total step : 1990 \n",
      "error : 0.717059, accuarcy : 0.794397\n",
      "total step : 1991 \n",
      "error : 0.716908, accuarcy : 0.794397\n",
      "total step : 1992 \n",
      "error : 0.716758, accuarcy : 0.794397\n",
      "total step : 1993 \n",
      "error : 0.716607, accuarcy : 0.794397\n",
      "total step : 1994 \n",
      "error : 0.716457, accuarcy : 0.794397\n",
      "total step : 1995 \n",
      "error : 0.716307, accuarcy : 0.794397\n",
      "total step : 1996 \n",
      "error : 0.716157, accuarcy : 0.794397\n",
      "total step : 1997 \n",
      "error : 0.716007, accuarcy : 0.794397\n",
      "total step : 1998 \n",
      "error : 0.715857, accuarcy : 0.794397\n",
      "total step : 1999 \n",
      "error : 0.715707, accuarcy : 0.794397\n",
      "total step : 2000 \n",
      "error : 0.715558, accuarcy : 0.794397\n",
      "total step : 2001 \n",
      "error : 0.715408, accuarcy : 0.794397\n",
      "total step : 2002 \n",
      "error : 0.715259, accuarcy : 0.794397\n",
      "total step : 2003 \n",
      "error : 0.715109, accuarcy : 0.794397\n",
      "total step : 2004 \n",
      "error : 0.714960, accuarcy : 0.794397\n",
      "total step : 2005 \n",
      "error : 0.714811, accuarcy : 0.794397\n",
      "total step : 2006 \n",
      "error : 0.714662, accuarcy : 0.794397\n",
      "total step : 2007 \n",
      "error : 0.714513, accuarcy : 0.794397\n",
      "total step : 2008 \n",
      "error : 0.714364, accuarcy : 0.794397\n",
      "total step : 2009 \n",
      "error : 0.714215, accuarcy : 0.794397\n",
      "total step : 2010 \n",
      "error : 0.714066, accuarcy : 0.794397\n",
      "total step : 2011 \n",
      "error : 0.713918, accuarcy : 0.794397\n",
      "total step : 2012 \n",
      "error : 0.713769, accuarcy : 0.794397\n",
      "total step : 2013 \n",
      "error : 0.713621, accuarcy : 0.794397\n",
      "total step : 2014 \n",
      "error : 0.713473, accuarcy : 0.794397\n",
      "total step : 2015 \n",
      "error : 0.713325, accuarcy : 0.794397\n",
      "total step : 2016 \n",
      "error : 0.713177, accuarcy : 0.794397\n",
      "total step : 2017 \n",
      "error : 0.713029, accuarcy : 0.794397\n",
      "total step : 2018 \n",
      "error : 0.712881, accuarcy : 0.794397\n",
      "total step : 2019 \n",
      "error : 0.712733, accuarcy : 0.794397\n",
      "total step : 2020 \n",
      "error : 0.712585, accuarcy : 0.794397\n",
      "total step : 2021 \n",
      "error : 0.712438, accuarcy : 0.794397\n",
      "total step : 2022 \n",
      "error : 0.712290, accuarcy : 0.794397\n",
      "total step : 2023 \n",
      "error : 0.712143, accuarcy : 0.794397\n",
      "total step : 2024 \n",
      "error : 0.711996, accuarcy : 0.794897\n",
      "total step : 2025 \n",
      "error : 0.711849, accuarcy : 0.795398\n",
      "total step : 2026 \n",
      "error : 0.711701, accuarcy : 0.795398\n",
      "total step : 2027 \n",
      "error : 0.711555, accuarcy : 0.795398\n",
      "total step : 2028 \n",
      "error : 0.711408, accuarcy : 0.795898\n",
      "total step : 2029 \n",
      "error : 0.711261, accuarcy : 0.795898\n",
      "total step : 2030 \n",
      "error : 0.711114, accuarcy : 0.795898\n",
      "total step : 2031 \n",
      "error : 0.710968, accuarcy : 0.795898\n",
      "total step : 2032 \n",
      "error : 0.710821, accuarcy : 0.795898\n",
      "total step : 2033 \n",
      "error : 0.710675, accuarcy : 0.795898\n",
      "total step : 2034 \n",
      "error : 0.710528, accuarcy : 0.795898\n",
      "total step : 2035 \n",
      "error : 0.710382, accuarcy : 0.795898\n",
      "total step : 2036 \n",
      "error : 0.710236, accuarcy : 0.795898\n",
      "total step : 2037 \n",
      "error : 0.710090, accuarcy : 0.795898\n",
      "total step : 2038 \n",
      "error : 0.709944, accuarcy : 0.795898\n",
      "total step : 2039 \n",
      "error : 0.709798, accuarcy : 0.795898\n",
      "total step : 2040 \n",
      "error : 0.709653, accuarcy : 0.795898\n",
      "total step : 2041 \n",
      "error : 0.709507, accuarcy : 0.795898\n",
      "total step : 2042 \n",
      "error : 0.709362, accuarcy : 0.795898\n",
      "total step : 2043 \n",
      "error : 0.709216, accuarcy : 0.795898\n",
      "total step : 2044 \n",
      "error : 0.709071, accuarcy : 0.795898\n",
      "total step : 2045 \n",
      "error : 0.708926, accuarcy : 0.795898\n",
      "total step : 2046 \n",
      "error : 0.708780, accuarcy : 0.795898\n",
      "total step : 2047 \n",
      "error : 0.708635, accuarcy : 0.795898\n",
      "total step : 2048 \n",
      "error : 0.708490, accuarcy : 0.795898\n",
      "total step : 2049 \n",
      "error : 0.708346, accuarcy : 0.795898\n",
      "total step : 2050 \n",
      "error : 0.708201, accuarcy : 0.795898\n",
      "total step : 2051 \n",
      "error : 0.708056, accuarcy : 0.795898\n",
      "total step : 2052 \n",
      "error : 0.707912, accuarcy : 0.795898\n",
      "total step : 2053 \n",
      "error : 0.707767, accuarcy : 0.795898\n",
      "total step : 2054 \n",
      "error : 0.707623, accuarcy : 0.795898\n",
      "total step : 2055 \n",
      "error : 0.707479, accuarcy : 0.795898\n",
      "total step : 2056 \n",
      "error : 0.707334, accuarcy : 0.796398\n",
      "total step : 2057 \n",
      "error : 0.707190, accuarcy : 0.796398\n",
      "total step : 2058 \n",
      "error : 0.707046, accuarcy : 0.796398\n",
      "total step : 2059 \n",
      "error : 0.706902, accuarcy : 0.796898\n",
      "total step : 2060 \n",
      "error : 0.706759, accuarcy : 0.796898\n",
      "total step : 2061 \n",
      "error : 0.706615, accuarcy : 0.796898\n",
      "total step : 2062 \n",
      "error : 0.706471, accuarcy : 0.796898\n",
      "total step : 2063 \n",
      "error : 0.706328, accuarcy : 0.796898\n",
      "total step : 2064 \n",
      "error : 0.706184, accuarcy : 0.796898\n",
      "total step : 2065 \n",
      "error : 0.706041, accuarcy : 0.797399\n",
      "total step : 2066 \n",
      "error : 0.705898, accuarcy : 0.797399\n",
      "total step : 2067 \n",
      "error : 0.705755, accuarcy : 0.797899\n",
      "total step : 2068 \n",
      "error : 0.705611, accuarcy : 0.797899\n",
      "total step : 2069 \n",
      "error : 0.705469, accuarcy : 0.797899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 2070 \n",
      "error : 0.705326, accuarcy : 0.797899\n",
      "total step : 2071 \n",
      "error : 0.705183, accuarcy : 0.797899\n",
      "total step : 2072 \n",
      "error : 0.705040, accuarcy : 0.797899\n",
      "total step : 2073 \n",
      "error : 0.704898, accuarcy : 0.797899\n",
      "total step : 2074 \n",
      "error : 0.704755, accuarcy : 0.797899\n",
      "total step : 2075 \n",
      "error : 0.704613, accuarcy : 0.797899\n",
      "total step : 2076 \n",
      "error : 0.704470, accuarcy : 0.797899\n",
      "total step : 2077 \n",
      "error : 0.704328, accuarcy : 0.797899\n",
      "total step : 2078 \n",
      "error : 0.704186, accuarcy : 0.797899\n",
      "total step : 2079 \n",
      "error : 0.704044, accuarcy : 0.797899\n",
      "total step : 2080 \n",
      "error : 0.703902, accuarcy : 0.797899\n",
      "total step : 2081 \n",
      "error : 0.703760, accuarcy : 0.797899\n",
      "total step : 2082 \n",
      "error : 0.703618, accuarcy : 0.797899\n",
      "total step : 2083 \n",
      "error : 0.703477, accuarcy : 0.797899\n",
      "total step : 2084 \n",
      "error : 0.703335, accuarcy : 0.797899\n",
      "total step : 2085 \n",
      "error : 0.703194, accuarcy : 0.797899\n",
      "total step : 2086 \n",
      "error : 0.703052, accuarcy : 0.797899\n",
      "total step : 2087 \n",
      "error : 0.702911, accuarcy : 0.797899\n",
      "total step : 2088 \n",
      "error : 0.702770, accuarcy : 0.797899\n",
      "total step : 2089 \n",
      "error : 0.702629, accuarcy : 0.797899\n",
      "total step : 2090 \n",
      "error : 0.702488, accuarcy : 0.797899\n",
      "total step : 2091 \n",
      "error : 0.702347, accuarcy : 0.797899\n",
      "total step : 2092 \n",
      "error : 0.702206, accuarcy : 0.797899\n",
      "total step : 2093 \n",
      "error : 0.702065, accuarcy : 0.797899\n",
      "total step : 2094 \n",
      "error : 0.701925, accuarcy : 0.797899\n",
      "total step : 2095 \n",
      "error : 0.701784, accuarcy : 0.797899\n",
      "total step : 2096 \n",
      "error : 0.701643, accuarcy : 0.797899\n",
      "total step : 2097 \n",
      "error : 0.701503, accuarcy : 0.797899\n",
      "total step : 2098 \n",
      "error : 0.701363, accuarcy : 0.797899\n",
      "total step : 2099 \n",
      "error : 0.701223, accuarcy : 0.797899\n",
      "total step : 2100 \n",
      "error : 0.701082, accuarcy : 0.797899\n",
      "total step : 2101 \n",
      "error : 0.700942, accuarcy : 0.797899\n",
      "total step : 2102 \n",
      "error : 0.700802, accuarcy : 0.797899\n",
      "total step : 2103 \n",
      "error : 0.700663, accuarcy : 0.797899\n",
      "total step : 2104 \n",
      "error : 0.700523, accuarcy : 0.797899\n",
      "total step : 2105 \n",
      "error : 0.700383, accuarcy : 0.797899\n",
      "total step : 2106 \n",
      "error : 0.700244, accuarcy : 0.797899\n",
      "total step : 2107 \n",
      "error : 0.700104, accuarcy : 0.798399\n",
      "total step : 2108 \n",
      "error : 0.699965, accuarcy : 0.798399\n",
      "total step : 2109 \n",
      "error : 0.699825, accuarcy : 0.798399\n",
      "total step : 2110 \n",
      "error : 0.699686, accuarcy : 0.798399\n",
      "total step : 2111 \n",
      "error : 0.699547, accuarcy : 0.798399\n",
      "total step : 2112 \n",
      "error : 0.699408, accuarcy : 0.798399\n",
      "total step : 2113 \n",
      "error : 0.699269, accuarcy : 0.798399\n",
      "total step : 2114 \n",
      "error : 0.699130, accuarcy : 0.798399\n",
      "total step : 2115 \n",
      "error : 0.698991, accuarcy : 0.798399\n",
      "total step : 2116 \n",
      "error : 0.698853, accuarcy : 0.798399\n",
      "total step : 2117 \n",
      "error : 0.698714, accuarcy : 0.798399\n",
      "total step : 2118 \n",
      "error : 0.698576, accuarcy : 0.798399\n",
      "total step : 2119 \n",
      "error : 0.698437, accuarcy : 0.798399\n",
      "total step : 2120 \n",
      "error : 0.698299, accuarcy : 0.798399\n",
      "total step : 2121 \n",
      "error : 0.698160, accuarcy : 0.798399\n",
      "total step : 2122 \n",
      "error : 0.698022, accuarcy : 0.798899\n",
      "total step : 2123 \n",
      "error : 0.697884, accuarcy : 0.798899\n",
      "total step : 2124 \n",
      "error : 0.697746, accuarcy : 0.798899\n",
      "total step : 2125 \n",
      "error : 0.697608, accuarcy : 0.798899\n",
      "total step : 2126 \n",
      "error : 0.697471, accuarcy : 0.798899\n",
      "total step : 2127 \n",
      "error : 0.697333, accuarcy : 0.798899\n",
      "total step : 2128 \n",
      "error : 0.697195, accuarcy : 0.798899\n",
      "total step : 2129 \n",
      "error : 0.697058, accuarcy : 0.798899\n",
      "total step : 2130 \n",
      "error : 0.696920, accuarcy : 0.798899\n",
      "total step : 2131 \n",
      "error : 0.696783, accuarcy : 0.798899\n",
      "total step : 2132 \n",
      "error : 0.696645, accuarcy : 0.798899\n",
      "total step : 2133 \n",
      "error : 0.696508, accuarcy : 0.798899\n",
      "total step : 2134 \n",
      "error : 0.696371, accuarcy : 0.799400\n",
      "total step : 2135 \n",
      "error : 0.696234, accuarcy : 0.799400\n",
      "total step : 2136 \n",
      "error : 0.696097, accuarcy : 0.799400\n",
      "total step : 2137 \n",
      "error : 0.695960, accuarcy : 0.799900\n",
      "total step : 2138 \n",
      "error : 0.695824, accuarcy : 0.799900\n",
      "total step : 2139 \n",
      "error : 0.695687, accuarcy : 0.800400\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for number 9\n",
      "total step : 1 \n",
      "error : 5.304839, accuarcy : 0.395698\n",
      "total step : 2 \n",
      "error : 5.271915, accuarcy : 0.396198\n",
      "total step : 3 \n",
      "error : 5.239165, accuarcy : 0.396198\n",
      "total step : 4 \n",
      "error : 5.206601, accuarcy : 0.396198\n",
      "total step : 5 \n",
      "error : 5.174229, accuarcy : 0.394197\n",
      "total step : 6 \n",
      "error : 5.142060, accuarcy : 0.393697\n",
      "total step : 7 \n",
      "error : 5.110101, accuarcy : 0.390695\n",
      "total step : 8 \n",
      "error : 5.078362, accuarcy : 0.388694\n",
      "total step : 9 \n",
      "error : 5.046851, accuarcy : 0.388694\n",
      "total step : 10 \n",
      "error : 5.015576, accuarcy : 0.387694\n",
      "total step : 11 \n",
      "error : 4.984545, accuarcy : 0.386193\n",
      "total step : 12 \n",
      "error : 4.953768, accuarcy : 0.384192\n",
      "total step : 13 \n",
      "error : 4.923252, accuarcy : 0.381191\n",
      "total step : 14 \n",
      "error : 4.893004, accuarcy : 0.382191\n",
      "total step : 15 \n",
      "error : 4.863032, accuarcy : 0.381191\n",
      "total step : 16 \n",
      "error : 4.833342, accuarcy : 0.380690\n",
      "total step : 17 \n",
      "error : 4.803941, accuarcy : 0.381191\n",
      "total step : 18 \n",
      "error : 4.774836, accuarcy : 0.379690\n",
      "total step : 19 \n",
      "error : 4.746032, accuarcy : 0.378189\n",
      "total step : 20 \n",
      "error : 4.717535, accuarcy : 0.378189\n",
      "total step : 21 \n",
      "error : 4.689349, accuarcy : 0.379190\n",
      "total step : 22 \n",
      "error : 4.661481, accuarcy : 0.380190\n",
      "total step : 23 \n",
      "error : 4.633933, accuarcy : 0.381191\n",
      "total step : 24 \n",
      "error : 4.606709, accuarcy : 0.380190\n",
      "total step : 25 \n",
      "error : 4.579814, accuarcy : 0.380690\n",
      "total step : 26 \n",
      "error : 4.553248, accuarcy : 0.379190\n",
      "total step : 27 \n",
      "error : 4.527014, accuarcy : 0.380690\n",
      "total step : 28 \n",
      "error : 4.501113, accuarcy : 0.381691\n",
      "total step : 29 \n",
      "error : 4.475544, accuarcy : 0.380690\n",
      "total step : 30 \n",
      "error : 4.450308, accuarcy : 0.382691\n",
      "total step : 31 \n",
      "error : 4.425403, accuarcy : 0.381691\n",
      "total step : 32 \n",
      "error : 4.400827, accuarcy : 0.378689\n",
      "total step : 33 \n",
      "error : 4.376579, accuarcy : 0.380690\n",
      "total step : 34 \n",
      "error : 4.352656, accuarcy : 0.381191\n",
      "total step : 35 \n",
      "error : 4.329054, accuarcy : 0.378189\n",
      "total step : 36 \n",
      "error : 4.305770, accuarcy : 0.377689\n",
      "total step : 37 \n",
      "error : 4.282800, accuarcy : 0.378689\n",
      "total step : 38 \n",
      "error : 4.260140, accuarcy : 0.377689\n",
      "total step : 39 \n",
      "error : 4.237785, accuarcy : 0.377189\n",
      "total step : 40 \n",
      "error : 4.215731, accuarcy : 0.378689\n",
      "total step : 41 \n",
      "error : 4.193972, accuarcy : 0.378689\n",
      "total step : 42 \n",
      "error : 4.172504, accuarcy : 0.378189\n",
      "total step : 43 \n",
      "error : 4.151321, accuarcy : 0.379690\n",
      "total step : 44 \n",
      "error : 4.130418, accuarcy : 0.378189\n",
      "total step : 45 \n",
      "error : 4.109790, accuarcy : 0.379190\n",
      "total step : 46 \n",
      "error : 4.089429, accuarcy : 0.381191\n",
      "total step : 47 \n",
      "error : 4.069332, accuarcy : 0.381691\n",
      "total step : 48 \n",
      "error : 4.049491, accuarcy : 0.381191\n",
      "total step : 49 \n",
      "error : 4.029901, accuarcy : 0.381191\n",
      "total step : 50 \n",
      "error : 4.010556, accuarcy : 0.381691\n",
      "total step : 51 \n",
      "error : 3.991450, accuarcy : 0.382191\n",
      "total step : 52 \n",
      "error : 3.972577, accuarcy : 0.382691\n",
      "total step : 53 \n",
      "error : 3.953930, accuarcy : 0.384692\n",
      "total step : 54 \n",
      "error : 3.935504, accuarcy : 0.384692\n",
      "total step : 55 \n",
      "error : 3.917292, accuarcy : 0.383192\n",
      "total step : 56 \n",
      "error : 3.899290, accuarcy : 0.383192\n",
      "total step : 57 \n",
      "error : 3.881491, accuarcy : 0.385693\n",
      "total step : 58 \n",
      "error : 3.863889, accuarcy : 0.385193\n",
      "total step : 59 \n",
      "error : 3.846479, accuarcy : 0.385693\n",
      "total step : 60 \n",
      "error : 3.829255, accuarcy : 0.383192\n",
      "total step : 61 \n",
      "error : 3.812213, accuarcy : 0.384692\n",
      "total step : 62 \n",
      "error : 3.795347, accuarcy : 0.386193\n",
      "total step : 63 \n",
      "error : 3.778652, accuarcy : 0.386693\n",
      "total step : 64 \n",
      "error : 3.762124, accuarcy : 0.387694\n",
      "total step : 65 \n",
      "error : 3.745757, accuarcy : 0.387194\n",
      "total step : 66 \n",
      "error : 3.729548, accuarcy : 0.387194\n",
      "total step : 67 \n",
      "error : 3.713492, accuarcy : 0.388694\n",
      "total step : 68 \n",
      "error : 3.697586, accuarcy : 0.388694\n",
      "total step : 69 \n",
      "error : 3.681825, accuarcy : 0.389195\n",
      "total step : 70 \n",
      "error : 3.666206, accuarcy : 0.387694\n",
      "total step : 71 \n",
      "error : 3.650724, accuarcy : 0.388194\n",
      "total step : 72 \n",
      "error : 3.635377, accuarcy : 0.389195\n",
      "total step : 73 \n",
      "error : 3.620162, accuarcy : 0.390695\n",
      "total step : 74 \n",
      "error : 3.605074, accuarcy : 0.390695\n",
      "total step : 75 \n",
      "error : 3.590112, accuarcy : 0.391196\n",
      "total step : 76 \n",
      "error : 3.575272, accuarcy : 0.391196\n",
      "total step : 77 \n",
      "error : 3.560551, accuarcy : 0.391696\n",
      "total step : 78 \n",
      "error : 3.545946, accuarcy : 0.392696\n",
      "total step : 79 \n",
      "error : 3.531456, accuarcy : 0.393697\n",
      "total step : 80 \n",
      "error : 3.517077, accuarcy : 0.394697\n",
      "total step : 81 \n",
      "error : 3.502807, accuarcy : 0.395198\n",
      "total step : 82 \n",
      "error : 3.488643, accuarcy : 0.395198\n",
      "total step : 83 \n",
      "error : 3.474584, accuarcy : 0.392696\n",
      "total step : 84 \n",
      "error : 3.460628, accuarcy : 0.393697\n",
      "total step : 85 \n",
      "error : 3.446771, accuarcy : 0.393697\n",
      "total step : 86 \n",
      "error : 3.433013, accuarcy : 0.394197\n",
      "total step : 87 \n",
      "error : 3.419350, accuarcy : 0.394697\n",
      "total step : 88 \n",
      "error : 3.405782, accuarcy : 0.395698\n",
      "total step : 89 \n",
      "error : 3.392307, accuarcy : 0.396198\n",
      "total step : 90 \n",
      "error : 3.378922, accuarcy : 0.396698\n",
      "total step : 91 \n",
      "error : 3.365626, accuarcy : 0.398199\n",
      "total step : 92 \n",
      "error : 3.352417, accuarcy : 0.399200\n",
      "total step : 93 \n",
      "error : 3.339294, accuarcy : 0.400200\n",
      "total step : 94 \n",
      "error : 3.326256, accuarcy : 0.403202\n",
      "total step : 95 \n",
      "error : 3.313300, accuarcy : 0.404702\n",
      "total step : 96 \n",
      "error : 3.300426, accuarcy : 0.405703\n",
      "total step : 97 \n",
      "error : 3.287631, accuarcy : 0.407204\n",
      "total step : 98 \n",
      "error : 3.274915, accuarcy : 0.407204\n",
      "total step : 99 \n",
      "error : 3.262277, accuarcy : 0.408704\n",
      "total step : 100 \n",
      "error : 3.249715, accuarcy : 0.409705\n",
      "total step : 101 \n",
      "error : 3.237227, accuarcy : 0.409705\n",
      "total step : 102 \n",
      "error : 3.224814, accuarcy : 0.410705\n",
      "total step : 103 \n",
      "error : 3.212473, accuarcy : 0.410705\n",
      "total step : 104 \n",
      "error : 3.200204, accuarcy : 0.411206\n",
      "total step : 105 \n",
      "error : 3.188005, accuarcy : 0.411706\n",
      "total step : 106 \n",
      "error : 3.175877, accuarcy : 0.413207\n",
      "total step : 107 \n",
      "error : 3.163816, accuarcy : 0.414207\n",
      "total step : 108 \n",
      "error : 3.151824, accuarcy : 0.414207\n",
      "total step : 109 \n",
      "error : 3.139899, accuarcy : 0.415708\n",
      "total step : 110 \n",
      "error : 3.128039, accuarcy : 0.417209\n",
      "total step : 111 \n",
      "error : 3.116245, accuarcy : 0.418209\n",
      "total step : 112 \n",
      "error : 3.104516, accuarcy : 0.419710\n",
      "total step : 113 \n",
      "error : 3.092850, accuarcy : 0.419210\n",
      "total step : 114 \n",
      "error : 3.081248, accuarcy : 0.419210\n",
      "total step : 115 \n",
      "error : 3.069708, accuarcy : 0.421711\n",
      "total step : 116 \n",
      "error : 3.058229, accuarcy : 0.422211\n",
      "total step : 117 \n",
      "error : 3.046812, accuarcy : 0.425213\n",
      "total step : 118 \n",
      "error : 3.035455, accuarcy : 0.425213\n",
      "total step : 119 \n",
      "error : 3.024158, accuarcy : 0.425713\n",
      "total step : 120 \n",
      "error : 3.012921, accuarcy : 0.426213\n",
      "total step : 121 \n",
      "error : 3.001742, accuarcy : 0.426213\n",
      "total step : 122 \n",
      "error : 2.990621, accuarcy : 0.426213\n",
      "total step : 123 \n",
      "error : 2.979559, accuarcy : 0.426713\n",
      "total step : 124 \n",
      "error : 2.968553, accuarcy : 0.428214\n",
      "total step : 125 \n",
      "error : 2.957605, accuarcy : 0.429215\n",
      "total step : 126 \n",
      "error : 2.946713, accuarcy : 0.430215\n",
      "total step : 127 \n",
      "error : 2.935877, accuarcy : 0.431716\n",
      "total step : 128 \n",
      "error : 2.925096, accuarcy : 0.432216\n",
      "total step : 129 \n",
      "error : 2.914371, accuarcy : 0.433217\n",
      "total step : 130 \n",
      "error : 2.903700, accuarcy : 0.433217\n",
      "total step : 131 \n",
      "error : 2.893083, accuarcy : 0.434217\n",
      "total step : 132 \n",
      "error : 2.882521, accuarcy : 0.435218\n",
      "total step : 133 \n",
      "error : 2.872012, accuarcy : 0.436718\n",
      "total step : 134 \n",
      "error : 2.861556, accuarcy : 0.438219\n",
      "total step : 135 \n",
      "error : 2.851154, accuarcy : 0.440220\n",
      "total step : 136 \n",
      "error : 2.840804, accuarcy : 0.440220\n",
      "total step : 137 \n",
      "error : 2.830506, accuarcy : 0.440220\n",
      "total step : 138 \n",
      "error : 2.820260, accuarcy : 0.440720\n",
      "total step : 139 \n",
      "error : 2.810066, accuarcy : 0.441221\n",
      "total step : 140 \n",
      "error : 2.799923, accuarcy : 0.441721\n",
      "total step : 141 \n",
      "error : 2.789831, accuarcy : 0.442721\n",
      "total step : 142 \n",
      "error : 2.779790, accuarcy : 0.444222\n",
      "total step : 143 \n",
      "error : 2.769800, accuarcy : 0.445223\n",
      "total step : 144 \n",
      "error : 2.759860, accuarcy : 0.445723\n",
      "total step : 145 \n",
      "error : 2.749970, accuarcy : 0.447224\n",
      "total step : 146 \n",
      "error : 2.740129, accuarcy : 0.447724\n",
      "total step : 147 \n",
      "error : 2.730338, accuarcy : 0.449225\n",
      "total step : 148 \n",
      "error : 2.720597, accuarcy : 0.449225\n",
      "total step : 149 \n",
      "error : 2.710904, accuarcy : 0.449225\n",
      "total step : 150 \n",
      "error : 2.701260, accuarcy : 0.450725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 151 \n",
      "error : 2.691664, accuarcy : 0.452726\n",
      "total step : 152 \n",
      "error : 2.682117, accuarcy : 0.453727\n",
      "total step : 153 \n",
      "error : 2.672618, accuarcy : 0.453227\n",
      "total step : 154 \n",
      "error : 2.663167, accuarcy : 0.453727\n",
      "total step : 155 \n",
      "error : 2.653764, accuarcy : 0.455228\n",
      "total step : 156 \n",
      "error : 2.644408, accuarcy : 0.455228\n",
      "total step : 157 \n",
      "error : 2.635099, accuarcy : 0.455228\n",
      "total step : 158 \n",
      "error : 2.625838, accuarcy : 0.457729\n",
      "total step : 159 \n",
      "error : 2.616623, accuarcy : 0.457729\n",
      "total step : 160 \n",
      "error : 2.607455, accuarcy : 0.458229\n",
      "total step : 161 \n",
      "error : 2.598333, accuarcy : 0.459730\n",
      "total step : 162 \n",
      "error : 2.589258, accuarcy : 0.459730\n",
      "total step : 163 \n",
      "error : 2.580228, accuarcy : 0.461731\n",
      "total step : 164 \n",
      "error : 2.571245, accuarcy : 0.461731\n",
      "total step : 165 \n",
      "error : 2.562307, accuarcy : 0.462731\n",
      "total step : 166 \n",
      "error : 2.553415, accuarcy : 0.462731\n",
      "total step : 167 \n",
      "error : 2.544568, accuarcy : 0.463732\n",
      "total step : 168 \n",
      "error : 2.535767, accuarcy : 0.463732\n",
      "total step : 169 \n",
      "error : 2.527010, accuarcy : 0.463732\n",
      "total step : 170 \n",
      "error : 2.518299, accuarcy : 0.464232\n",
      "total step : 171 \n",
      "error : 2.509632, accuarcy : 0.464232\n",
      "total step : 172 \n",
      "error : 2.501009, accuarcy : 0.463732\n",
      "total step : 173 \n",
      "error : 2.492431, accuarcy : 0.465233\n",
      "total step : 174 \n",
      "error : 2.483897, accuarcy : 0.466733\n",
      "total step : 175 \n",
      "error : 2.475408, accuarcy : 0.468234\n",
      "total step : 176 \n",
      "error : 2.466961, accuarcy : 0.469735\n",
      "total step : 177 \n",
      "error : 2.458559, accuarcy : 0.471236\n",
      "total step : 178 \n",
      "error : 2.450200, accuarcy : 0.473237\n",
      "total step : 179 \n",
      "error : 2.441885, accuarcy : 0.474737\n",
      "total step : 180 \n",
      "error : 2.433612, accuarcy : 0.475238\n",
      "total step : 181 \n",
      "error : 2.425383, accuarcy : 0.477239\n",
      "total step : 182 \n",
      "error : 2.417197, accuarcy : 0.478239\n",
      "total step : 183 \n",
      "error : 2.409053, accuarcy : 0.478739\n",
      "total step : 184 \n",
      "error : 2.400952, accuarcy : 0.480740\n",
      "total step : 185 \n",
      "error : 2.392893, accuarcy : 0.480740\n",
      "total step : 186 \n",
      "error : 2.384877, accuarcy : 0.481241\n",
      "total step : 187 \n",
      "error : 2.376902, accuarcy : 0.482741\n",
      "total step : 188 \n",
      "error : 2.368970, accuarcy : 0.484242\n",
      "total step : 189 \n",
      "error : 2.361079, accuarcy : 0.484742\n",
      "total step : 190 \n",
      "error : 2.353230, accuarcy : 0.485243\n",
      "total step : 191 \n",
      "error : 2.345422, accuarcy : 0.485243\n",
      "total step : 192 \n",
      "error : 2.337655, accuarcy : 0.487244\n",
      "total step : 193 \n",
      "error : 2.329930, accuarcy : 0.488244\n",
      "total step : 194 \n",
      "error : 2.322246, accuarcy : 0.490745\n",
      "total step : 195 \n",
      "error : 2.314602, accuarcy : 0.491246\n",
      "total step : 196 \n",
      "error : 2.306999, accuarcy : 0.492246\n",
      "total step : 197 \n",
      "error : 2.299437, accuarcy : 0.493247\n",
      "total step : 198 \n",
      "error : 2.291914, accuarcy : 0.494747\n",
      "total step : 199 \n",
      "error : 2.284432, accuarcy : 0.495248\n",
      "total step : 200 \n",
      "error : 2.276990, accuarcy : 0.496748\n",
      "total step : 201 \n",
      "error : 2.269588, accuarcy : 0.497749\n",
      "total step : 202 \n",
      "error : 2.262226, accuarcy : 0.498249\n",
      "total step : 203 \n",
      "error : 2.254903, accuarcy : 0.499750\n",
      "total step : 204 \n",
      "error : 2.247619, accuarcy : 0.500750\n",
      "total step : 205 \n",
      "error : 2.240375, accuarcy : 0.501751\n",
      "total step : 206 \n",
      "error : 2.233170, accuarcy : 0.502751\n",
      "total step : 207 \n",
      "error : 2.226004, accuarcy : 0.503252\n",
      "total step : 208 \n",
      "error : 2.218876, accuarcy : 0.503752\n",
      "total step : 209 \n",
      "error : 2.211787, accuarcy : 0.504752\n",
      "total step : 210 \n",
      "error : 2.204737, accuarcy : 0.504752\n",
      "total step : 211 \n",
      "error : 2.197724, accuarcy : 0.505253\n",
      "total step : 212 \n",
      "error : 2.190750, accuarcy : 0.505753\n",
      "total step : 213 \n",
      "error : 2.183814, accuarcy : 0.507754\n",
      "total step : 214 \n",
      "error : 2.176916, accuarcy : 0.508254\n",
      "total step : 215 \n",
      "error : 2.170055, accuarcy : 0.509755\n",
      "total step : 216 \n",
      "error : 2.163232, accuarcy : 0.510255\n",
      "total step : 217 \n",
      "error : 2.156446, accuarcy : 0.510755\n",
      "total step : 218 \n",
      "error : 2.149698, accuarcy : 0.512756\n",
      "total step : 219 \n",
      "error : 2.142986, accuarcy : 0.512756\n",
      "total step : 220 \n",
      "error : 2.136312, accuarcy : 0.512756\n",
      "total step : 221 \n",
      "error : 2.129674, accuarcy : 0.513257\n",
      "total step : 222 \n",
      "error : 2.123072, accuarcy : 0.513757\n",
      "total step : 223 \n",
      "error : 2.116507, accuarcy : 0.514257\n",
      "total step : 224 \n",
      "error : 2.109979, accuarcy : 0.516258\n",
      "total step : 225 \n",
      "error : 2.103486, accuarcy : 0.518759\n",
      "total step : 226 \n",
      "error : 2.097029, accuarcy : 0.519260\n",
      "total step : 227 \n",
      "error : 2.090609, accuarcy : 0.521261\n",
      "total step : 228 \n",
      "error : 2.084223, accuarcy : 0.521261\n",
      "total step : 229 \n",
      "error : 2.077873, accuarcy : 0.522761\n",
      "total step : 230 \n",
      "error : 2.071559, accuarcy : 0.523762\n",
      "total step : 231 \n",
      "error : 2.065280, accuarcy : 0.524262\n",
      "total step : 232 \n",
      "error : 2.059035, accuarcy : 0.525263\n",
      "total step : 233 \n",
      "error : 2.052826, accuarcy : 0.525263\n",
      "total step : 234 \n",
      "error : 2.046651, accuarcy : 0.525263\n",
      "total step : 235 \n",
      "error : 2.040510, accuarcy : 0.526263\n",
      "total step : 236 \n",
      "error : 2.034404, accuarcy : 0.526763\n",
      "total step : 237 \n",
      "error : 2.028332, accuarcy : 0.527264\n",
      "total step : 238 \n",
      "error : 2.022294, accuarcy : 0.528264\n",
      "total step : 239 \n",
      "error : 2.016290, accuarcy : 0.528764\n",
      "total step : 240 \n",
      "error : 2.010319, accuarcy : 0.529765\n",
      "total step : 241 \n",
      "error : 2.004382, accuarcy : 0.529765\n",
      "total step : 242 \n",
      "error : 1.998479, accuarcy : 0.530265\n",
      "total step : 243 \n",
      "error : 1.992608, accuarcy : 0.530265\n",
      "total step : 244 \n",
      "error : 1.986771, accuarcy : 0.530765\n",
      "total step : 245 \n",
      "error : 1.980966, accuarcy : 0.531266\n",
      "total step : 246 \n",
      "error : 1.975194, accuarcy : 0.532766\n",
      "total step : 247 \n",
      "error : 1.969455, accuarcy : 0.533767\n",
      "total step : 248 \n",
      "error : 1.963747, accuarcy : 0.535268\n",
      "total step : 249 \n",
      "error : 1.958072, accuarcy : 0.536768\n",
      "total step : 250 \n",
      "error : 1.952429, accuarcy : 0.538769\n",
      "total step : 251 \n",
      "error : 1.946818, accuarcy : 0.539270\n",
      "total step : 252 \n",
      "error : 1.941238, accuarcy : 0.541771\n",
      "total step : 253 \n",
      "error : 1.935690, accuarcy : 0.542771\n",
      "total step : 254 \n",
      "error : 1.930173, accuarcy : 0.544272\n",
      "total step : 255 \n",
      "error : 1.924687, accuarcy : 0.544272\n",
      "total step : 256 \n",
      "error : 1.919232, accuarcy : 0.545273\n",
      "total step : 257 \n",
      "error : 1.913808, accuarcy : 0.544772\n",
      "total step : 258 \n",
      "error : 1.908415, accuarcy : 0.547774\n",
      "total step : 259 \n",
      "error : 1.903051, accuarcy : 0.548774\n",
      "total step : 260 \n",
      "error : 1.897718, accuarcy : 0.549275\n",
      "total step : 261 \n",
      "error : 1.892415, accuarcy : 0.550275\n",
      "total step : 262 \n",
      "error : 1.887142, accuarcy : 0.550775\n",
      "total step : 263 \n",
      "error : 1.881899, accuarcy : 0.551776\n",
      "total step : 264 \n",
      "error : 1.876685, accuarcy : 0.553277\n",
      "total step : 265 \n",
      "error : 1.871500, accuarcy : 0.553777\n",
      "total step : 266 \n",
      "error : 1.866345, accuarcy : 0.553777\n",
      "total step : 267 \n",
      "error : 1.861218, accuarcy : 0.554777\n",
      "total step : 268 \n",
      "error : 1.856120, accuarcy : 0.555778\n",
      "total step : 269 \n",
      "error : 1.851051, accuarcy : 0.556278\n",
      "total step : 270 \n",
      "error : 1.846010, accuarcy : 0.558279\n",
      "total step : 271 \n",
      "error : 1.840998, accuarcy : 0.559280\n",
      "total step : 272 \n",
      "error : 1.836013, accuarcy : 0.560280\n",
      "total step : 273 \n",
      "error : 1.831057, accuarcy : 0.561781\n",
      "total step : 274 \n",
      "error : 1.826128, accuarcy : 0.563282\n",
      "total step : 275 \n",
      "error : 1.821226, accuarcy : 0.563782\n",
      "total step : 276 \n",
      "error : 1.816352, accuarcy : 0.564782\n",
      "total step : 277 \n",
      "error : 1.811505, accuarcy : 0.564782\n",
      "total step : 278 \n",
      "error : 1.806685, accuarcy : 0.565283\n",
      "total step : 279 \n",
      "error : 1.801892, accuarcy : 0.566283\n",
      "total step : 280 \n",
      "error : 1.797126, accuarcy : 0.566283\n",
      "total step : 281 \n",
      "error : 1.792385, accuarcy : 0.567284\n",
      "total step : 282 \n",
      "error : 1.787672, accuarcy : 0.568284\n",
      "total step : 283 \n",
      "error : 1.782984, accuarcy : 0.568284\n",
      "total step : 284 \n",
      "error : 1.778322, accuarcy : 0.568784\n",
      "total step : 285 \n",
      "error : 1.773686, accuarcy : 0.569785\n",
      "total step : 286 \n",
      "error : 1.769075, accuarcy : 0.569785\n",
      "total step : 287 \n",
      "error : 1.764490, accuarcy : 0.570785\n",
      "total step : 288 \n",
      "error : 1.759930, accuarcy : 0.570785\n",
      "total step : 289 \n",
      "error : 1.755395, accuarcy : 0.571286\n",
      "total step : 290 \n",
      "error : 1.750885, accuarcy : 0.573287\n",
      "total step : 291 \n",
      "error : 1.746400, accuarcy : 0.574787\n",
      "total step : 292 \n",
      "error : 1.741939, accuarcy : 0.574787\n",
      "total step : 293 \n",
      "error : 1.737503, accuarcy : 0.577289\n",
      "total step : 294 \n",
      "error : 1.733091, accuarcy : 0.577789\n",
      "total step : 295 \n",
      "error : 1.728703, accuarcy : 0.580290\n",
      "total step : 296 \n",
      "error : 1.724339, accuarcy : 0.580290\n",
      "total step : 297 \n",
      "error : 1.719998, accuarcy : 0.581291\n",
      "total step : 298 \n",
      "error : 1.715681, accuarcy : 0.581291\n",
      "total step : 299 \n",
      "error : 1.711388, accuarcy : 0.581291\n",
      "total step : 300 \n",
      "error : 1.707117, accuarcy : 0.581791\n",
      "total step : 301 \n",
      "error : 1.702870, accuarcy : 0.582291\n",
      "total step : 302 \n",
      "error : 1.698646, accuarcy : 0.582291\n",
      "total step : 303 \n",
      "error : 1.694444, accuarcy : 0.582791\n",
      "total step : 304 \n",
      "error : 1.690265, accuarcy : 0.583792\n",
      "total step : 305 \n",
      "error : 1.686109, accuarcy : 0.583792\n",
      "total step : 306 \n",
      "error : 1.681974, accuarcy : 0.584292\n",
      "total step : 307 \n",
      "error : 1.677862, accuarcy : 0.584292\n",
      "total step : 308 \n",
      "error : 1.673772, accuarcy : 0.583792\n",
      "total step : 309 \n",
      "error : 1.669704, accuarcy : 0.583792\n",
      "total step : 310 \n",
      "error : 1.665657, accuarcy : 0.583792\n",
      "total step : 311 \n",
      "error : 1.661632, accuarcy : 0.584792\n",
      "total step : 312 \n",
      "error : 1.657628, accuarcy : 0.584792\n",
      "total step : 313 \n",
      "error : 1.653646, accuarcy : 0.584792\n",
      "total step : 314 \n",
      "error : 1.649685, accuarcy : 0.585793\n",
      "total step : 315 \n",
      "error : 1.645744, accuarcy : 0.585793\n",
      "total step : 316 \n",
      "error : 1.641825, accuarcy : 0.586793\n",
      "total step : 317 \n",
      "error : 1.637926, accuarcy : 0.587794\n",
      "total step : 318 \n",
      "error : 1.634047, accuarcy : 0.588794\n",
      "total step : 319 \n",
      "error : 1.630189, accuarcy : 0.589295\n",
      "total step : 320 \n",
      "error : 1.626351, accuarcy : 0.590295\n",
      "total step : 321 \n",
      "error : 1.622534, accuarcy : 0.591296\n",
      "total step : 322 \n",
      "error : 1.618736, accuarcy : 0.592296\n",
      "total step : 323 \n",
      "error : 1.614958, accuarcy : 0.592296\n",
      "total step : 324 \n",
      "error : 1.611200, accuarcy : 0.591796\n",
      "total step : 325 \n",
      "error : 1.607461, accuarcy : 0.592796\n",
      "total step : 326 \n",
      "error : 1.603742, accuarcy : 0.592796\n",
      "total step : 327 \n",
      "error : 1.600042, accuarcy : 0.593297\n",
      "total step : 328 \n",
      "error : 1.596361, accuarcy : 0.594297\n",
      "total step : 329 \n",
      "error : 1.592700, accuarcy : 0.594797\n",
      "total step : 330 \n",
      "error : 1.589057, accuarcy : 0.595298\n",
      "total step : 331 \n",
      "error : 1.585433, accuarcy : 0.598299\n",
      "total step : 332 \n",
      "error : 1.581827, accuarcy : 0.598799\n",
      "total step : 333 \n",
      "error : 1.578240, accuarcy : 0.599800\n",
      "total step : 334 \n",
      "error : 1.574672, accuarcy : 0.599800\n",
      "total step : 335 \n",
      "error : 1.571121, accuarcy : 0.599800\n",
      "total step : 336 \n",
      "error : 1.567589, accuarcy : 0.601301\n",
      "total step : 337 \n",
      "error : 1.564075, accuarcy : 0.602301\n",
      "total step : 338 \n",
      "error : 1.560579, accuarcy : 0.603302\n",
      "total step : 339 \n",
      "error : 1.557100, accuarcy : 0.603802\n",
      "total step : 340 \n",
      "error : 1.553639, accuarcy : 0.605303\n",
      "total step : 341 \n",
      "error : 1.550196, accuarcy : 0.606303\n",
      "total step : 342 \n",
      "error : 1.546770, accuarcy : 0.607304\n",
      "total step : 343 \n",
      "error : 1.543361, accuarcy : 0.608804\n",
      "total step : 344 \n",
      "error : 1.539970, accuarcy : 0.609805\n",
      "total step : 345 \n",
      "error : 1.536595, accuarcy : 0.609805\n",
      "total step : 346 \n",
      "error : 1.533237, accuarcy : 0.610805\n",
      "total step : 347 \n",
      "error : 1.529896, accuarcy : 0.610805\n",
      "total step : 348 \n",
      "error : 1.526572, accuarcy : 0.611306\n",
      "total step : 349 \n",
      "error : 1.523265, accuarcy : 0.611306\n",
      "total step : 350 \n",
      "error : 1.519974, accuarcy : 0.612806\n",
      "total step : 351 \n",
      "error : 1.516699, accuarcy : 0.612806\n",
      "total step : 352 \n",
      "error : 1.513440, accuarcy : 0.613807\n",
      "total step : 353 \n",
      "error : 1.510198, accuarcy : 0.613307\n",
      "total step : 354 \n",
      "error : 1.506971, accuarcy : 0.614807\n",
      "total step : 355 \n",
      "error : 1.503761, accuarcy : 0.614307\n",
      "total step : 356 \n",
      "error : 1.500566, accuarcy : 0.614807\n",
      "total step : 357 \n",
      "error : 1.497387, accuarcy : 0.614807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 358 \n",
      "error : 1.494223, accuarcy : 0.614807\n",
      "total step : 359 \n",
      "error : 1.491075, accuarcy : 0.615308\n",
      "total step : 360 \n",
      "error : 1.487942, accuarcy : 0.615808\n",
      "total step : 361 \n",
      "error : 1.484825, accuarcy : 0.616308\n",
      "total step : 362 \n",
      "error : 1.481722, accuarcy : 0.617309\n",
      "total step : 363 \n",
      "error : 1.478635, accuarcy : 0.617809\n",
      "total step : 364 \n",
      "error : 1.475562, accuarcy : 0.618309\n",
      "total step : 365 \n",
      "error : 1.472505, accuarcy : 0.619810\n",
      "total step : 366 \n",
      "error : 1.469462, accuarcy : 0.620310\n",
      "total step : 367 \n",
      "error : 1.466433, accuarcy : 0.620310\n",
      "total step : 368 \n",
      "error : 1.463419, accuarcy : 0.621311\n",
      "total step : 369 \n",
      "error : 1.460420, accuarcy : 0.622311\n",
      "total step : 370 \n",
      "error : 1.457435, accuarcy : 0.623812\n",
      "total step : 371 \n",
      "error : 1.454464, accuarcy : 0.624312\n",
      "total step : 372 \n",
      "error : 1.451507, accuarcy : 0.624312\n",
      "total step : 373 \n",
      "error : 1.448564, accuarcy : 0.624312\n",
      "total step : 374 \n",
      "error : 1.445635, accuarcy : 0.625313\n",
      "total step : 375 \n",
      "error : 1.442720, accuarcy : 0.625813\n",
      "total step : 376 \n",
      "error : 1.439819, accuarcy : 0.626313\n",
      "total step : 377 \n",
      "error : 1.436931, accuarcy : 0.626313\n",
      "total step : 378 \n",
      "error : 1.434057, accuarcy : 0.628314\n",
      "total step : 379 \n",
      "error : 1.431196, accuarcy : 0.629315\n",
      "total step : 380 \n",
      "error : 1.428349, accuarcy : 0.629315\n",
      "total step : 381 \n",
      "error : 1.425515, accuarcy : 0.629815\n",
      "total step : 382 \n",
      "error : 1.422694, accuarcy : 0.629815\n",
      "total step : 383 \n",
      "error : 1.419886, accuarcy : 0.630315\n",
      "total step : 384 \n",
      "error : 1.417091, accuarcy : 0.630815\n",
      "total step : 385 \n",
      "error : 1.414309, accuarcy : 0.631816\n",
      "total step : 386 \n",
      "error : 1.411540, accuarcy : 0.632316\n",
      "total step : 387 \n",
      "error : 1.408783, accuarcy : 0.632316\n",
      "total step : 388 \n",
      "error : 1.406040, accuarcy : 0.632816\n",
      "total step : 389 \n",
      "error : 1.403308, accuarcy : 0.632816\n",
      "total step : 390 \n",
      "error : 1.400590, accuarcy : 0.633817\n",
      "total step : 391 \n",
      "error : 1.397883, accuarcy : 0.634817\n",
      "total step : 392 \n",
      "error : 1.395189, accuarcy : 0.635818\n",
      "total step : 393 \n",
      "error : 1.392507, accuarcy : 0.636318\n",
      "total step : 394 \n",
      "error : 1.389838, accuarcy : 0.636818\n",
      "total step : 395 \n",
      "error : 1.387180, accuarcy : 0.636818\n",
      "total step : 396 \n",
      "error : 1.384535, accuarcy : 0.637319\n",
      "total step : 397 \n",
      "error : 1.381901, accuarcy : 0.637319\n",
      "total step : 398 \n",
      "error : 1.379279, accuarcy : 0.637819\n",
      "total step : 399 \n",
      "error : 1.376669, accuarcy : 0.637819\n",
      "total step : 400 \n",
      "error : 1.374071, accuarcy : 0.638319\n",
      "total step : 401 \n",
      "error : 1.371484, accuarcy : 0.639820\n",
      "total step : 402 \n",
      "error : 1.368909, accuarcy : 0.639820\n",
      "total step : 403 \n",
      "error : 1.366345, accuarcy : 0.639820\n",
      "total step : 404 \n",
      "error : 1.363792, accuarcy : 0.639320\n",
      "total step : 405 \n",
      "error : 1.361251, accuarcy : 0.639820\n",
      "total step : 406 \n",
      "error : 1.358721, accuarcy : 0.639820\n",
      "total step : 407 \n",
      "error : 1.356203, accuarcy : 0.640320\n",
      "total step : 408 \n",
      "error : 1.353695, accuarcy : 0.640820\n",
      "total step : 409 \n",
      "error : 1.351199, accuarcy : 0.640820\n",
      "total step : 410 \n",
      "error : 1.348713, accuarcy : 0.641321\n",
      "total step : 411 \n",
      "error : 1.346238, accuarcy : 0.641321\n",
      "total step : 412 \n",
      "error : 1.343774, accuarcy : 0.641821\n",
      "total step : 413 \n",
      "error : 1.341321, accuarcy : 0.642321\n",
      "total step : 414 \n",
      "error : 1.338879, accuarcy : 0.642321\n",
      "total step : 415 \n",
      "error : 1.336447, accuarcy : 0.642321\n",
      "total step : 416 \n",
      "error : 1.334026, accuarcy : 0.642821\n",
      "total step : 417 \n",
      "error : 1.331615, accuarcy : 0.642821\n",
      "total step : 418 \n",
      "error : 1.329215, accuarcy : 0.643322\n",
      "total step : 419 \n",
      "error : 1.326825, accuarcy : 0.644322\n",
      "total step : 420 \n",
      "error : 1.324445, accuarcy : 0.644322\n",
      "total step : 421 \n",
      "error : 1.322076, accuarcy : 0.644322\n",
      "total step : 422 \n",
      "error : 1.319717, accuarcy : 0.645323\n",
      "total step : 423 \n",
      "error : 1.317368, accuarcy : 0.646823\n",
      "total step : 424 \n",
      "error : 1.315029, accuarcy : 0.646323\n",
      "total step : 425 \n",
      "error : 1.312700, accuarcy : 0.646323\n",
      "total step : 426 \n",
      "error : 1.310381, accuarcy : 0.646823\n",
      "total step : 427 \n",
      "error : 1.308072, accuarcy : 0.647324\n",
      "total step : 428 \n",
      "error : 1.305773, accuarcy : 0.647824\n",
      "total step : 429 \n",
      "error : 1.303484, accuarcy : 0.648824\n",
      "total step : 430 \n",
      "error : 1.301204, accuarcy : 0.649325\n",
      "total step : 431 \n",
      "error : 1.298934, accuarcy : 0.649325\n",
      "total step : 432 \n",
      "error : 1.296673, accuarcy : 0.649825\n",
      "total step : 433 \n",
      "error : 1.294422, accuarcy : 0.649825\n",
      "total step : 434 \n",
      "error : 1.292181, accuarcy : 0.649825\n",
      "total step : 435 \n",
      "error : 1.289949, accuarcy : 0.649825\n",
      "total step : 436 \n",
      "error : 1.287727, accuarcy : 0.650325\n",
      "total step : 437 \n",
      "error : 1.285513, accuarcy : 0.650825\n",
      "total step : 438 \n",
      "error : 1.283310, accuarcy : 0.651326\n",
      "total step : 439 \n",
      "error : 1.281115, accuarcy : 0.652826\n",
      "total step : 440 \n",
      "error : 1.278929, accuarcy : 0.653327\n",
      "total step : 441 \n",
      "error : 1.276753, accuarcy : 0.654327\n",
      "total step : 442 \n",
      "error : 1.274586, accuarcy : 0.654827\n",
      "total step : 443 \n",
      "error : 1.272428, accuarcy : 0.655328\n",
      "total step : 444 \n",
      "error : 1.270279, accuarcy : 0.655828\n",
      "total step : 445 \n",
      "error : 1.268138, accuarcy : 0.656828\n",
      "total step : 446 \n",
      "error : 1.266007, accuarcy : 0.657329\n",
      "total step : 447 \n",
      "error : 1.263884, accuarcy : 0.657329\n",
      "total step : 448 \n",
      "error : 1.261771, accuarcy : 0.657329\n",
      "total step : 449 \n",
      "error : 1.259666, accuarcy : 0.657329\n",
      "total step : 450 \n",
      "error : 1.257569, accuarcy : 0.657829\n",
      "total step : 451 \n",
      "error : 1.255482, accuarcy : 0.658829\n",
      "total step : 452 \n",
      "error : 1.253403, accuarcy : 0.660330\n",
      "total step : 453 \n",
      "error : 1.251332, accuarcy : 0.660830\n",
      "total step : 454 \n",
      "error : 1.249270, accuarcy : 0.660830\n",
      "total step : 455 \n",
      "error : 1.247217, accuarcy : 0.660830\n",
      "total step : 456 \n",
      "error : 1.245172, accuarcy : 0.661331\n",
      "total step : 457 \n",
      "error : 1.243135, accuarcy : 0.661331\n",
      "total step : 458 \n",
      "error : 1.241107, accuarcy : 0.661331\n",
      "total step : 459 \n",
      "error : 1.239087, accuarcy : 0.661331\n",
      "total step : 460 \n",
      "error : 1.237075, accuarcy : 0.661331\n",
      "total step : 461 \n",
      "error : 1.235071, accuarcy : 0.661331\n",
      "total step : 462 \n",
      "error : 1.233076, accuarcy : 0.661331\n",
      "total step : 463 \n",
      "error : 1.231088, accuarcy : 0.662331\n",
      "total step : 464 \n",
      "error : 1.229109, accuarcy : 0.662331\n",
      "total step : 465 \n",
      "error : 1.227138, accuarcy : 0.662331\n",
      "total step : 466 \n",
      "error : 1.225175, accuarcy : 0.662331\n",
      "total step : 467 \n",
      "error : 1.223219, accuarcy : 0.662331\n",
      "total step : 468 \n",
      "error : 1.221272, accuarcy : 0.662831\n",
      "total step : 469 \n",
      "error : 1.219333, accuarcy : 0.663332\n",
      "total step : 470 \n",
      "error : 1.217401, accuarcy : 0.663832\n",
      "total step : 471 \n",
      "error : 1.215477, accuarcy : 0.663832\n",
      "total step : 472 \n",
      "error : 1.213561, accuarcy : 0.664332\n",
      "total step : 473 \n",
      "error : 1.211653, accuarcy : 0.664332\n",
      "total step : 474 \n",
      "error : 1.209752, accuarcy : 0.663832\n",
      "total step : 475 \n",
      "error : 1.207859, accuarcy : 0.663832\n",
      "total step : 476 \n",
      "error : 1.205974, accuarcy : 0.663332\n",
      "total step : 477 \n",
      "error : 1.204096, accuarcy : 0.664332\n",
      "total step : 478 \n",
      "error : 1.202225, accuarcy : 0.664832\n",
      "total step : 479 \n",
      "error : 1.200362, accuarcy : 0.665833\n",
      "total step : 480 \n",
      "error : 1.198507, accuarcy : 0.666833\n",
      "total step : 481 \n",
      "error : 1.196659, accuarcy : 0.667834\n",
      "total step : 482 \n",
      "error : 1.194818, accuarcy : 0.667834\n",
      "total step : 483 \n",
      "error : 1.192985, accuarcy : 0.668334\n",
      "total step : 484 \n",
      "error : 1.191159, accuarcy : 0.668334\n",
      "total step : 485 \n",
      "error : 1.189340, accuarcy : 0.668334\n",
      "total step : 486 \n",
      "error : 1.187529, accuarcy : 0.668334\n",
      "total step : 487 \n",
      "error : 1.185724, accuarcy : 0.668334\n",
      "total step : 488 \n",
      "error : 1.183927, accuarcy : 0.668334\n",
      "total step : 489 \n",
      "error : 1.182137, accuarcy : 0.668334\n",
      "total step : 490 \n",
      "error : 1.180354, accuarcy : 0.668834\n",
      "total step : 491 \n",
      "error : 1.178578, accuarcy : 0.669335\n",
      "total step : 492 \n",
      "error : 1.176809, accuarcy : 0.669335\n",
      "total step : 493 \n",
      "error : 1.175047, accuarcy : 0.669835\n",
      "total step : 494 \n",
      "error : 1.173292, accuarcy : 0.670335\n",
      "total step : 495 \n",
      "error : 1.171544, accuarcy : 0.670835\n",
      "total step : 496 \n",
      "error : 1.169803, accuarcy : 0.671836\n",
      "total step : 497 \n",
      "error : 1.168069, accuarcy : 0.672336\n",
      "total step : 498 \n",
      "error : 1.166341, accuarcy : 0.673837\n",
      "total step : 499 \n",
      "error : 1.164621, accuarcy : 0.674837\n",
      "total step : 500 \n",
      "error : 1.162907, accuarcy : 0.674337\n",
      "total step : 501 \n",
      "error : 1.161199, accuarcy : 0.674837\n",
      "total step : 502 \n",
      "error : 1.159499, accuarcy : 0.674837\n",
      "total step : 503 \n",
      "error : 1.157805, accuarcy : 0.674837\n",
      "total step : 504 \n",
      "error : 1.156118, accuarcy : 0.675838\n",
      "total step : 505 \n",
      "error : 1.154437, accuarcy : 0.675838\n",
      "total step : 506 \n",
      "error : 1.152763, accuarcy : 0.675838\n",
      "total step : 507 \n",
      "error : 1.151095, accuarcy : 0.675838\n",
      "total step : 508 \n",
      "error : 1.149434, accuarcy : 0.676338\n",
      "total step : 509 \n",
      "error : 1.147779, accuarcy : 0.676838\n",
      "total step : 510 \n",
      "error : 1.146131, accuarcy : 0.676838\n",
      "total step : 511 \n",
      "error : 1.144489, accuarcy : 0.677339\n",
      "total step : 512 \n",
      "error : 1.142854, accuarcy : 0.677839\n",
      "total step : 513 \n",
      "error : 1.141224, accuarcy : 0.678839\n",
      "total step : 514 \n",
      "error : 1.139602, accuarcy : 0.679340\n",
      "total step : 515 \n",
      "error : 1.137985, accuarcy : 0.679340\n",
      "total step : 516 \n",
      "error : 1.136375, accuarcy : 0.679340\n",
      "total step : 517 \n",
      "error : 1.134771, accuarcy : 0.681841\n",
      "total step : 518 \n",
      "error : 1.133173, accuarcy : 0.681841\n",
      "total step : 519 \n",
      "error : 1.131581, accuarcy : 0.682341\n",
      "total step : 520 \n",
      "error : 1.129995, accuarcy : 0.682841\n",
      "total step : 521 \n",
      "error : 1.128416, accuarcy : 0.682841\n",
      "total step : 522 \n",
      "error : 1.126842, accuarcy : 0.682841\n",
      "total step : 523 \n",
      "error : 1.125275, accuarcy : 0.683342\n",
      "total step : 524 \n",
      "error : 1.123714, accuarcy : 0.683342\n",
      "total step : 525 \n",
      "error : 1.122158, accuarcy : 0.684342\n",
      "total step : 526 \n",
      "error : 1.120609, accuarcy : 0.684842\n",
      "total step : 527 \n",
      "error : 1.119065, accuarcy : 0.686343\n",
      "total step : 528 \n",
      "error : 1.117528, accuarcy : 0.687344\n",
      "total step : 529 \n",
      "error : 1.115996, accuarcy : 0.687844\n",
      "total step : 530 \n",
      "error : 1.114470, accuarcy : 0.687844\n",
      "total step : 531 \n",
      "error : 1.112950, accuarcy : 0.687844\n",
      "total step : 532 \n",
      "error : 1.111436, accuarcy : 0.688344\n",
      "total step : 533 \n",
      "error : 1.109927, accuarcy : 0.689345\n",
      "total step : 534 \n",
      "error : 1.108424, accuarcy : 0.689845\n",
      "total step : 535 \n",
      "error : 1.106927, accuarcy : 0.690345\n",
      "total step : 536 \n",
      "error : 1.105436, accuarcy : 0.690345\n",
      "total step : 537 \n",
      "error : 1.103950, accuarcy : 0.690345\n",
      "total step : 538 \n",
      "error : 1.102470, accuarcy : 0.690845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 539 \n",
      "error : 1.100996, accuarcy : 0.690845\n",
      "total step : 540 \n",
      "error : 1.099527, accuarcy : 0.690845\n",
      "total step : 541 \n",
      "error : 1.098063, accuarcy : 0.690845\n",
      "total step : 542 \n",
      "error : 1.096605, accuarcy : 0.690845\n",
      "total step : 543 \n",
      "error : 1.095153, accuarcy : 0.691346\n",
      "total step : 544 \n",
      "error : 1.093706, accuarcy : 0.691846\n",
      "total step : 545 \n",
      "error : 1.092265, accuarcy : 0.691846\n",
      "total step : 546 \n",
      "error : 1.090829, accuarcy : 0.691846\n",
      "total step : 547 \n",
      "error : 1.089398, accuarcy : 0.692346\n",
      "total step : 548 \n",
      "error : 1.087973, accuarcy : 0.693347\n",
      "total step : 549 \n",
      "error : 1.086553, accuarcy : 0.693847\n",
      "total step : 550 \n",
      "error : 1.085139, accuarcy : 0.693847\n",
      "total step : 551 \n",
      "error : 1.083729, accuarcy : 0.694847\n",
      "total step : 552 \n",
      "error : 1.082325, accuarcy : 0.696348\n",
      "total step : 553 \n",
      "error : 1.080927, accuarcy : 0.697349\n",
      "total step : 554 \n",
      "error : 1.079533, accuarcy : 0.697349\n",
      "total step : 555 \n",
      "error : 1.078145, accuarcy : 0.697349\n",
      "total step : 556 \n",
      "error : 1.076762, accuarcy : 0.697849\n",
      "total step : 557 \n",
      "error : 1.075384, accuarcy : 0.697849\n",
      "total step : 558 \n",
      "error : 1.074011, accuarcy : 0.697849\n",
      "total step : 559 \n",
      "error : 1.072643, accuarcy : 0.697849\n",
      "total step : 560 \n",
      "error : 1.071280, accuarcy : 0.697849\n",
      "total step : 561 \n",
      "error : 1.069923, accuarcy : 0.697849\n",
      "total step : 562 \n",
      "error : 1.068570, accuarcy : 0.697849\n",
      "total step : 563 \n",
      "error : 1.067223, accuarcy : 0.697849\n",
      "total step : 564 \n",
      "error : 1.065880, accuarcy : 0.697849\n",
      "total step : 565 \n",
      "error : 1.064543, accuarcy : 0.697849\n",
      "total step : 566 \n",
      "error : 1.063210, accuarcy : 0.697849\n",
      "total step : 567 \n",
      "error : 1.061882, accuarcy : 0.697849\n",
      "total step : 568 \n",
      "error : 1.060559, accuarcy : 0.697849\n",
      "total step : 569 \n",
      "error : 1.059241, accuarcy : 0.697349\n",
      "total step : 570 \n",
      "error : 1.057928, accuarcy : 0.697349\n",
      "total step : 571 \n",
      "error : 1.056620, accuarcy : 0.697849\n",
      "total step : 572 \n",
      "error : 1.055316, accuarcy : 0.697849\n",
      "total step : 573 \n",
      "error : 1.054018, accuarcy : 0.698349\n",
      "total step : 574 \n",
      "error : 1.052724, accuarcy : 0.699350\n",
      "total step : 575 \n",
      "error : 1.051435, accuarcy : 0.699350\n",
      "total step : 576 \n",
      "error : 1.050150, accuarcy : 0.699350\n",
      "total step : 577 \n",
      "error : 1.048870, accuarcy : 0.700350\n",
      "total step : 578 \n",
      "error : 1.047595, accuarcy : 0.700850\n",
      "total step : 579 \n",
      "error : 1.046325, accuarcy : 0.701351\n",
      "total step : 580 \n",
      "error : 1.045059, accuarcy : 0.701351\n",
      "total step : 581 \n",
      "error : 1.043798, accuarcy : 0.701851\n",
      "total step : 582 \n",
      "error : 1.042541, accuarcy : 0.702351\n",
      "total step : 583 \n",
      "error : 1.041289, accuarcy : 0.702351\n",
      "total step : 584 \n",
      "error : 1.040041, accuarcy : 0.702351\n",
      "total step : 585 \n",
      "error : 1.038798, accuarcy : 0.702351\n",
      "total step : 586 \n",
      "error : 1.037560, accuarcy : 0.702351\n",
      "total step : 587 \n",
      "error : 1.036326, accuarcy : 0.702851\n",
      "total step : 588 \n",
      "error : 1.035096, accuarcy : 0.702851\n",
      "total step : 589 \n",
      "error : 1.033871, accuarcy : 0.703852\n",
      "total step : 590 \n",
      "error : 1.032651, accuarcy : 0.703852\n",
      "total step : 591 \n",
      "error : 1.031434, accuarcy : 0.703852\n",
      "total step : 592 \n",
      "error : 1.030223, accuarcy : 0.703852\n",
      "total step : 593 \n",
      "error : 1.029015, accuarcy : 0.703852\n",
      "total step : 594 \n",
      "error : 1.027812, accuarcy : 0.703852\n",
      "total step : 595 \n",
      "error : 1.026613, accuarcy : 0.703852\n",
      "total step : 596 \n",
      "error : 1.025418, accuarcy : 0.703852\n",
      "total step : 597 \n",
      "error : 1.024228, accuarcy : 0.705353\n",
      "total step : 598 \n",
      "error : 1.023042, accuarcy : 0.706353\n",
      "total step : 599 \n",
      "error : 1.021860, accuarcy : 0.706353\n",
      "total step : 600 \n",
      "error : 1.020683, accuarcy : 0.706853\n",
      "total step : 601 \n",
      "error : 1.019509, accuarcy : 0.707354\n",
      "total step : 602 \n",
      "error : 1.018340, accuarcy : 0.707354\n",
      "total step : 603 \n",
      "error : 1.017175, accuarcy : 0.707354\n",
      "total step : 604 \n",
      "error : 1.016014, accuarcy : 0.707854\n",
      "total step : 605 \n",
      "error : 1.014858, accuarcy : 0.708354\n",
      "total step : 606 \n",
      "error : 1.013705, accuarcy : 0.708354\n",
      "total step : 607 \n",
      "error : 1.012556, accuarcy : 0.708354\n",
      "total step : 608 \n",
      "error : 1.011412, accuarcy : 0.708854\n",
      "total step : 609 \n",
      "error : 1.010272, accuarcy : 0.708854\n",
      "total step : 610 \n",
      "error : 1.009135, accuarcy : 0.708854\n",
      "total step : 611 \n",
      "error : 1.008003, accuarcy : 0.708854\n",
      "total step : 612 \n",
      "error : 1.006875, accuarcy : 0.709355\n",
      "total step : 613 \n",
      "error : 1.005750, accuarcy : 0.709355\n",
      "total step : 614 \n",
      "error : 1.004630, accuarcy : 0.709855\n",
      "total step : 615 \n",
      "error : 1.003513, accuarcy : 0.710355\n",
      "total step : 616 \n",
      "error : 1.002401, accuarcy : 0.710355\n",
      "total step : 617 \n",
      "error : 1.001292, accuarcy : 0.710355\n",
      "total step : 618 \n",
      "error : 1.000187, accuarcy : 0.710355\n",
      "total step : 619 \n",
      "error : 0.999086, accuarcy : 0.710355\n",
      "total step : 620 \n",
      "error : 0.997989, accuarcy : 0.710355\n",
      "total step : 621 \n",
      "error : 0.996896, accuarcy : 0.710355\n",
      "total step : 622 \n",
      "error : 0.995807, accuarcy : 0.710855\n",
      "total step : 623 \n",
      "error : 0.994721, accuarcy : 0.711856\n",
      "total step : 624 \n",
      "error : 0.993639, accuarcy : 0.711856\n",
      "total step : 625 \n",
      "error : 0.992561, accuarcy : 0.711856\n",
      "total step : 626 \n",
      "error : 0.991487, accuarcy : 0.711856\n",
      "total step : 627 \n",
      "error : 0.990416, accuarcy : 0.713357\n",
      "total step : 628 \n",
      "error : 0.989349, accuarcy : 0.713357\n",
      "total step : 629 \n",
      "error : 0.988286, accuarcy : 0.714357\n",
      "total step : 630 \n",
      "error : 0.987227, accuarcy : 0.714357\n",
      "total step : 631 \n",
      "error : 0.986171, accuarcy : 0.714857\n",
      "total step : 632 \n",
      "error : 0.985119, accuarcy : 0.715858\n",
      "total step : 633 \n",
      "error : 0.984070, accuarcy : 0.716358\n",
      "total step : 634 \n",
      "error : 0.983025, accuarcy : 0.716358\n",
      "total step : 635 \n",
      "error : 0.981984, accuarcy : 0.716858\n",
      "total step : 636 \n",
      "error : 0.980946, accuarcy : 0.716858\n",
      "total step : 637 \n",
      "error : 0.979912, accuarcy : 0.716858\n",
      "total step : 638 \n",
      "error : 0.978881, accuarcy : 0.716858\n",
      "total step : 639 \n",
      "error : 0.977854, accuarcy : 0.716858\n",
      "total step : 640 \n",
      "error : 0.976830, accuarcy : 0.716858\n",
      "total step : 641 \n",
      "error : 0.975810, accuarcy : 0.717859\n",
      "total step : 642 \n",
      "error : 0.974793, accuarcy : 0.717859\n",
      "total step : 643 \n",
      "error : 0.973780, accuarcy : 0.719860\n",
      "total step : 644 \n",
      "error : 0.972770, accuarcy : 0.719860\n",
      "total step : 645 \n",
      "error : 0.971764, accuarcy : 0.720360\n",
      "total step : 646 \n",
      "error : 0.970761, accuarcy : 0.720860\n",
      "total step : 647 \n",
      "error : 0.969761, accuarcy : 0.720860\n",
      "total step : 648 \n",
      "error : 0.968765, accuarcy : 0.721861\n",
      "total step : 649 \n",
      "error : 0.967772, accuarcy : 0.721861\n",
      "total step : 650 \n",
      "error : 0.966783, accuarcy : 0.721861\n",
      "total step : 651 \n",
      "error : 0.965797, accuarcy : 0.721861\n",
      "total step : 652 \n",
      "error : 0.964814, accuarcy : 0.721861\n",
      "total step : 653 \n",
      "error : 0.963835, accuarcy : 0.721861\n",
      "total step : 654 \n",
      "error : 0.962858, accuarcy : 0.721861\n",
      "total step : 655 \n",
      "error : 0.961885, accuarcy : 0.721861\n",
      "total step : 656 \n",
      "error : 0.960916, accuarcy : 0.723362\n",
      "total step : 657 \n",
      "error : 0.959949, accuarcy : 0.723362\n",
      "total step : 658 \n",
      "error : 0.958986, accuarcy : 0.723362\n",
      "total step : 659 \n",
      "error : 0.958026, accuarcy : 0.723362\n",
      "total step : 660 \n",
      "error : 0.957070, accuarcy : 0.723362\n",
      "total step : 661 \n",
      "error : 0.956116, accuarcy : 0.723362\n",
      "total step : 662 \n",
      "error : 0.955166, accuarcy : 0.723862\n",
      "total step : 663 \n",
      "error : 0.954218, accuarcy : 0.724362\n",
      "total step : 664 \n",
      "error : 0.953274, accuarcy : 0.724362\n",
      "total step : 665 \n",
      "error : 0.952333, accuarcy : 0.724362\n",
      "total step : 666 \n",
      "error : 0.951395, accuarcy : 0.724362\n",
      "total step : 667 \n",
      "error : 0.950461, accuarcy : 0.724862\n",
      "total step : 668 \n",
      "error : 0.949529, accuarcy : 0.724862\n",
      "total step : 669 \n",
      "error : 0.948600, accuarcy : 0.724862\n",
      "total step : 670 \n",
      "error : 0.947675, accuarcy : 0.725363\n",
      "total step : 671 \n",
      "error : 0.946752, accuarcy : 0.725363\n",
      "total step : 672 \n",
      "error : 0.945833, accuarcy : 0.725363\n",
      "total step : 673 \n",
      "error : 0.944916, accuarcy : 0.726363\n",
      "total step : 674 \n",
      "error : 0.944003, accuarcy : 0.726863\n",
      "total step : 675 \n",
      "error : 0.943092, accuarcy : 0.727364\n",
      "total step : 676 \n",
      "error : 0.942185, accuarcy : 0.727864\n",
      "total step : 677 \n",
      "error : 0.941280, accuarcy : 0.728364\n",
      "total step : 678 \n",
      "error : 0.940378, accuarcy : 0.728364\n",
      "total step : 679 \n",
      "error : 0.939480, accuarcy : 0.728364\n",
      "total step : 680 \n",
      "error : 0.938584, accuarcy : 0.728364\n",
      "total step : 681 \n",
      "error : 0.937691, accuarcy : 0.728364\n",
      "total step : 682 \n",
      "error : 0.936801, accuarcy : 0.728364\n",
      "total step : 683 \n",
      "error : 0.935914, accuarcy : 0.728364\n",
      "total step : 684 \n",
      "error : 0.935030, accuarcy : 0.729365\n",
      "total step : 685 \n",
      "error : 0.934148, accuarcy : 0.729865\n",
      "total step : 686 \n",
      "error : 0.933270, accuarcy : 0.730365\n",
      "total step : 687 \n",
      "error : 0.932394, accuarcy : 0.730865\n",
      "total step : 688 \n",
      "error : 0.931521, accuarcy : 0.730865\n",
      "total step : 689 \n",
      "error : 0.930651, accuarcy : 0.731366\n",
      "total step : 690 \n",
      "error : 0.929784, accuarcy : 0.731366\n",
      "total step : 691 \n",
      "error : 0.928919, accuarcy : 0.731366\n",
      "total step : 692 \n",
      "error : 0.928057, accuarcy : 0.731366\n",
      "total step : 693 \n",
      "error : 0.927198, accuarcy : 0.731366\n",
      "total step : 694 \n",
      "error : 0.926342, accuarcy : 0.731866\n",
      "total step : 695 \n",
      "error : 0.925488, accuarcy : 0.731866\n",
      "total step : 696 \n",
      "error : 0.924637, accuarcy : 0.731366\n",
      "total step : 697 \n",
      "error : 0.923789, accuarcy : 0.731366\n",
      "total step : 698 \n",
      "error : 0.922944, accuarcy : 0.731866\n",
      "total step : 699 \n",
      "error : 0.922101, accuarcy : 0.731866\n",
      "total step : 700 \n",
      "error : 0.921260, accuarcy : 0.731866\n",
      "total step : 701 \n",
      "error : 0.920423, accuarcy : 0.732366\n",
      "total step : 702 \n",
      "error : 0.919588, accuarcy : 0.732366\n",
      "total step : 703 \n",
      "error : 0.918756, accuarcy : 0.732866\n",
      "total step : 704 \n",
      "error : 0.917926, accuarcy : 0.732366\n",
      "total step : 705 \n",
      "error : 0.917099, accuarcy : 0.733367\n",
      "total step : 706 \n",
      "error : 0.916274, accuarcy : 0.733867\n",
      "total step : 707 \n",
      "error : 0.915453, accuarcy : 0.733867\n",
      "total step : 708 \n",
      "error : 0.914633, accuarcy : 0.733867\n",
      "total step : 709 \n",
      "error : 0.913816, accuarcy : 0.733867\n",
      "total step : 710 \n",
      "error : 0.913002, accuarcy : 0.734367\n",
      "total step : 711 \n",
      "error : 0.912190, accuarcy : 0.734367\n",
      "total step : 712 \n",
      "error : 0.911381, accuarcy : 0.734867\n",
      "total step : 713 \n",
      "error : 0.910574, accuarcy : 0.734867\n",
      "total step : 714 \n",
      "error : 0.909770, accuarcy : 0.735368\n",
      "total step : 715 \n",
      "error : 0.908968, accuarcy : 0.735868\n",
      "total step : 716 \n",
      "error : 0.908169, accuarcy : 0.735868\n",
      "total step : 717 \n",
      "error : 0.907372, accuarcy : 0.736868\n",
      "total step : 718 \n",
      "error : 0.906578, accuarcy : 0.736868\n",
      "total step : 719 \n",
      "error : 0.905786, accuarcy : 0.736868\n",
      "total step : 720 \n",
      "error : 0.904996, accuarcy : 0.736868\n",
      "total step : 721 \n",
      "error : 0.904209, accuarcy : 0.736868\n",
      "total step : 722 \n",
      "error : 0.903425, accuarcy : 0.736868\n",
      "total step : 723 \n",
      "error : 0.902642, accuarcy : 0.736868\n",
      "total step : 724 \n",
      "error : 0.901862, accuarcy : 0.736868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 725 \n",
      "error : 0.901085, accuarcy : 0.736868\n",
      "total step : 726 \n",
      "error : 0.900310, accuarcy : 0.737369\n",
      "total step : 727 \n",
      "error : 0.899537, accuarcy : 0.737869\n",
      "total step : 728 \n",
      "error : 0.898766, accuarcy : 0.738369\n",
      "total step : 729 \n",
      "error : 0.897998, accuarcy : 0.738869\n",
      "total step : 730 \n",
      "error : 0.897232, accuarcy : 0.739870\n",
      "total step : 731 \n",
      "error : 0.896469, accuarcy : 0.739870\n",
      "total step : 732 \n",
      "error : 0.895708, accuarcy : 0.739870\n",
      "total step : 733 \n",
      "error : 0.894949, accuarcy : 0.739870\n",
      "total step : 734 \n",
      "error : 0.894192, accuarcy : 0.740370\n",
      "total step : 735 \n",
      "error : 0.893438, accuarcy : 0.740870\n",
      "total step : 736 \n",
      "error : 0.892685, accuarcy : 0.740870\n",
      "total step : 737 \n",
      "error : 0.891935, accuarcy : 0.740870\n",
      "total step : 738 \n",
      "error : 0.891188, accuarcy : 0.740370\n",
      "total step : 739 \n",
      "error : 0.890442, accuarcy : 0.739870\n",
      "total step : 740 \n",
      "error : 0.889699, accuarcy : 0.740370\n",
      "total step : 741 \n",
      "error : 0.888958, accuarcy : 0.740370\n",
      "total step : 742 \n",
      "error : 0.888219, accuarcy : 0.740370\n",
      "total step : 743 \n",
      "error : 0.887482, accuarcy : 0.740370\n",
      "total step : 744 \n",
      "error : 0.886748, accuarcy : 0.741371\n",
      "total step : 745 \n",
      "error : 0.886016, accuarcy : 0.741371\n",
      "total step : 746 \n",
      "error : 0.885286, accuarcy : 0.741371\n",
      "total step : 747 \n",
      "error : 0.884558, accuarcy : 0.742371\n",
      "total step : 748 \n",
      "error : 0.883832, accuarcy : 0.742371\n",
      "total step : 749 \n",
      "error : 0.883108, accuarcy : 0.742371\n",
      "total step : 750 \n",
      "error : 0.882386, accuarcy : 0.743372\n",
      "total step : 751 \n",
      "error : 0.881667, accuarcy : 0.743372\n",
      "total step : 752 \n",
      "error : 0.880949, accuarcy : 0.743872\n",
      "total step : 753 \n",
      "error : 0.880234, accuarcy : 0.744372\n",
      "total step : 754 \n",
      "error : 0.879521, accuarcy : 0.744372\n",
      "total step : 755 \n",
      "error : 0.878809, accuarcy : 0.744872\n",
      "total step : 756 \n",
      "error : 0.878100, accuarcy : 0.744872\n",
      "total step : 757 \n",
      "error : 0.877393, accuarcy : 0.744872\n",
      "total step : 758 \n",
      "error : 0.876688, accuarcy : 0.744872\n",
      "total step : 759 \n",
      "error : 0.875985, accuarcy : 0.744872\n",
      "total step : 760 \n",
      "error : 0.875284, accuarcy : 0.745373\n",
      "total step : 761 \n",
      "error : 0.874585, accuarcy : 0.745373\n",
      "total step : 762 \n",
      "error : 0.873888, accuarcy : 0.745373\n",
      "total step : 763 \n",
      "error : 0.873193, accuarcy : 0.745373\n",
      "total step : 764 \n",
      "error : 0.872500, accuarcy : 0.745373\n",
      "total step : 765 \n",
      "error : 0.871809, accuarcy : 0.745373\n",
      "total step : 766 \n",
      "error : 0.871120, accuarcy : 0.745373\n",
      "total step : 767 \n",
      "error : 0.870433, accuarcy : 0.745373\n",
      "total step : 768 \n",
      "error : 0.869748, accuarcy : 0.745873\n",
      "total step : 769 \n",
      "error : 0.869065, accuarcy : 0.746373\n",
      "total step : 770 \n",
      "error : 0.868384, accuarcy : 0.746873\n",
      "total step : 771 \n",
      "error : 0.867705, accuarcy : 0.746873\n",
      "total step : 772 \n",
      "error : 0.867027, accuarcy : 0.746873\n",
      "total step : 773 \n",
      "error : 0.866352, accuarcy : 0.746873\n",
      "total step : 774 \n",
      "error : 0.865678, accuarcy : 0.746873\n",
      "total step : 775 \n",
      "error : 0.865007, accuarcy : 0.746873\n",
      "total step : 776 \n",
      "error : 0.864337, accuarcy : 0.747874\n",
      "total step : 777 \n",
      "error : 0.863669, accuarcy : 0.747874\n",
      "total step : 778 \n",
      "error : 0.863003, accuarcy : 0.748374\n",
      "total step : 779 \n",
      "error : 0.862339, accuarcy : 0.748374\n",
      "total step : 780 \n",
      "error : 0.861676, accuarcy : 0.749375\n",
      "total step : 781 \n",
      "error : 0.861016, accuarcy : 0.749375\n",
      "total step : 782 \n",
      "error : 0.860357, accuarcy : 0.749375\n",
      "total step : 783 \n",
      "error : 0.859700, accuarcy : 0.749375\n",
      "total step : 784 \n",
      "error : 0.859045, accuarcy : 0.749375\n",
      "total step : 785 \n",
      "error : 0.858392, accuarcy : 0.749375\n",
      "total step : 786 \n",
      "error : 0.857741, accuarcy : 0.749375\n",
      "total step : 787 \n",
      "error : 0.857091, accuarcy : 0.749375\n",
      "total step : 788 \n",
      "error : 0.856443, accuarcy : 0.749375\n",
      "total step : 789 \n",
      "error : 0.855797, accuarcy : 0.749375\n",
      "total step : 790 \n",
      "error : 0.855153, accuarcy : 0.749375\n",
      "total step : 791 \n",
      "error : 0.854511, accuarcy : 0.749375\n",
      "total step : 792 \n",
      "error : 0.853870, accuarcy : 0.750375\n",
      "total step : 793 \n",
      "error : 0.853231, accuarcy : 0.750375\n",
      "total step : 794 \n",
      "error : 0.852594, accuarcy : 0.750375\n",
      "total step : 795 \n",
      "error : 0.851958, accuarcy : 0.750875\n",
      "total step : 796 \n",
      "error : 0.851325, accuarcy : 0.751376\n",
      "total step : 797 \n",
      "error : 0.850693, accuarcy : 0.751876\n",
      "total step : 798 \n",
      "error : 0.850062, accuarcy : 0.751876\n",
      "total step : 799 \n",
      "error : 0.849434, accuarcy : 0.751876\n",
      "total step : 800 \n",
      "error : 0.848807, accuarcy : 0.751876\n",
      "total step : 801 \n",
      "error : 0.848182, accuarcy : 0.752376\n",
      "total step : 802 \n",
      "error : 0.847558, accuarcy : 0.752376\n",
      "total step : 803 \n",
      "error : 0.846936, accuarcy : 0.752876\n",
      "total step : 804 \n",
      "error : 0.846316, accuarcy : 0.752876\n",
      "total step : 805 \n",
      "error : 0.845697, accuarcy : 0.752876\n",
      "total step : 806 \n",
      "error : 0.845081, accuarcy : 0.753377\n",
      "total step : 807 \n",
      "error : 0.844465, accuarcy : 0.753377\n",
      "total step : 808 \n",
      "error : 0.843852, accuarcy : 0.753377\n",
      "total step : 809 \n",
      "error : 0.843240, accuarcy : 0.753377\n",
      "total step : 810 \n",
      "error : 0.842630, accuarcy : 0.753377\n",
      "total step : 811 \n",
      "error : 0.842021, accuarcy : 0.753377\n",
      "total step : 812 \n",
      "error : 0.841414, accuarcy : 0.753377\n",
      "total step : 813 \n",
      "error : 0.840808, accuarcy : 0.753377\n",
      "total step : 814 \n",
      "error : 0.840204, accuarcy : 0.753377\n",
      "total step : 815 \n",
      "error : 0.839602, accuarcy : 0.753377\n",
      "total step : 816 \n",
      "error : 0.839001, accuarcy : 0.753377\n",
      "total step : 817 \n",
      "error : 0.838402, accuarcy : 0.753377\n",
      "total step : 818 \n",
      "error : 0.837805, accuarcy : 0.753877\n",
      "total step : 819 \n",
      "error : 0.837209, accuarcy : 0.753877\n",
      "total step : 820 \n",
      "error : 0.836614, accuarcy : 0.753877\n",
      "total step : 821 \n",
      "error : 0.836021, accuarcy : 0.753877\n",
      "total step : 822 \n",
      "error : 0.835430, accuarcy : 0.753877\n",
      "total step : 823 \n",
      "error : 0.834840, accuarcy : 0.754377\n",
      "total step : 824 \n",
      "error : 0.834252, accuarcy : 0.754377\n",
      "total step : 825 \n",
      "error : 0.833665, accuarcy : 0.754877\n",
      "total step : 826 \n",
      "error : 0.833080, accuarcy : 0.754877\n",
      "total step : 827 \n",
      "error : 0.832496, accuarcy : 0.755378\n",
      "total step : 828 \n",
      "error : 0.831914, accuarcy : 0.755878\n",
      "total step : 829 \n",
      "error : 0.831333, accuarcy : 0.755878\n",
      "total step : 830 \n",
      "error : 0.830754, accuarcy : 0.755378\n",
      "total step : 831 \n",
      "error : 0.830176, accuarcy : 0.755878\n",
      "total step : 832 \n",
      "error : 0.829600, accuarcy : 0.755878\n",
      "total step : 833 \n",
      "error : 0.829025, accuarcy : 0.755878\n",
      "total step : 834 \n",
      "error : 0.828452, accuarcy : 0.755878\n",
      "total step : 835 \n",
      "error : 0.827880, accuarcy : 0.755878\n",
      "total step : 836 \n",
      "error : 0.827310, accuarcy : 0.755878\n",
      "total step : 837 \n",
      "error : 0.826741, accuarcy : 0.755878\n",
      "total step : 838 \n",
      "error : 0.826173, accuarcy : 0.756378\n",
      "total step : 839 \n",
      "error : 0.825607, accuarcy : 0.757379\n",
      "total step : 840 \n",
      "error : 0.825043, accuarcy : 0.757379\n",
      "total step : 841 \n",
      "error : 0.824479, accuarcy : 0.758879\n",
      "total step : 842 \n",
      "error : 0.823918, accuarcy : 0.758879\n",
      "total step : 843 \n",
      "error : 0.823357, accuarcy : 0.758379\n",
      "total step : 844 \n",
      "error : 0.822798, accuarcy : 0.758379\n",
      "total step : 845 \n",
      "error : 0.822241, accuarcy : 0.758379\n",
      "total step : 846 \n",
      "error : 0.821685, accuarcy : 0.758379\n",
      "total step : 847 \n",
      "error : 0.821130, accuarcy : 0.758879\n",
      "total step : 848 \n",
      "error : 0.820577, accuarcy : 0.760380\n",
      "total step : 849 \n",
      "error : 0.820025, accuarcy : 0.760380\n",
      "total step : 850 \n",
      "error : 0.819474, accuarcy : 0.760880\n",
      "total step : 851 \n",
      "error : 0.818925, accuarcy : 0.760880\n",
      "total step : 852 \n",
      "error : 0.818377, accuarcy : 0.760880\n",
      "total step : 853 \n",
      "error : 0.817830, accuarcy : 0.760880\n",
      "total step : 854 \n",
      "error : 0.817285, accuarcy : 0.760380\n",
      "total step : 855 \n",
      "error : 0.816741, accuarcy : 0.760380\n",
      "total step : 856 \n",
      "error : 0.816199, accuarcy : 0.759880\n",
      "total step : 857 \n",
      "error : 0.815658, accuarcy : 0.760380\n",
      "total step : 858 \n",
      "error : 0.815118, accuarcy : 0.760880\n",
      "total step : 859 \n",
      "error : 0.814579, accuarcy : 0.761381\n",
      "total step : 860 \n",
      "error : 0.814042, accuarcy : 0.761381\n",
      "total step : 861 \n",
      "error : 0.813506, accuarcy : 0.761381\n",
      "total step : 862 \n",
      "error : 0.812972, accuarcy : 0.761881\n",
      "total step : 863 \n",
      "error : 0.812439, accuarcy : 0.761881\n",
      "total step : 864 \n",
      "error : 0.811907, accuarcy : 0.761881\n",
      "total step : 865 \n",
      "error : 0.811376, accuarcy : 0.761881\n",
      "total step : 866 \n",
      "error : 0.810847, accuarcy : 0.761881\n",
      "total step : 867 \n",
      "error : 0.810318, accuarcy : 0.761881\n",
      "total step : 868 \n",
      "error : 0.809792, accuarcy : 0.761881\n",
      "total step : 869 \n",
      "error : 0.809266, accuarcy : 0.762881\n",
      "total step : 870 \n",
      "error : 0.808742, accuarcy : 0.762881\n",
      "total step : 871 \n",
      "error : 0.808219, accuarcy : 0.762881\n",
      "total step : 872 \n",
      "error : 0.807697, accuarcy : 0.763882\n",
      "total step : 873 \n",
      "error : 0.807176, accuarcy : 0.763882\n",
      "total step : 874 \n",
      "error : 0.806657, accuarcy : 0.763882\n",
      "total step : 875 \n",
      "error : 0.806139, accuarcy : 0.763882\n",
      "total step : 876 \n",
      "error : 0.805622, accuarcy : 0.764882\n",
      "total step : 877 \n",
      "error : 0.805107, accuarcy : 0.764882\n",
      "total step : 878 \n",
      "error : 0.804593, accuarcy : 0.765383\n",
      "total step : 879 \n",
      "error : 0.804079, accuarcy : 0.765383\n",
      "total step : 880 \n",
      "error : 0.803568, accuarcy : 0.765383\n",
      "total step : 881 \n",
      "error : 0.803057, accuarcy : 0.765383\n",
      "total step : 882 \n",
      "error : 0.802547, accuarcy : 0.765383\n",
      "total step : 883 \n",
      "error : 0.802039, accuarcy : 0.765383\n",
      "total step : 884 \n",
      "error : 0.801532, accuarcy : 0.765383\n",
      "total step : 885 \n",
      "error : 0.801026, accuarcy : 0.765883\n",
      "total step : 886 \n",
      "error : 0.800521, accuarcy : 0.765883\n",
      "total step : 887 \n",
      "error : 0.800018, accuarcy : 0.765883\n",
      "total step : 888 \n",
      "error : 0.799516, accuarcy : 0.765883\n",
      "total step : 889 \n",
      "error : 0.799014, accuarcy : 0.766383\n",
      "total step : 890 \n",
      "error : 0.798514, accuarcy : 0.766383\n",
      "total step : 891 \n",
      "error : 0.798016, accuarcy : 0.766383\n",
      "total step : 892 \n",
      "error : 0.797518, accuarcy : 0.766383\n",
      "total step : 893 \n",
      "error : 0.797021, accuarcy : 0.766383\n",
      "total step : 894 \n",
      "error : 0.796526, accuarcy : 0.766383\n",
      "total step : 895 \n",
      "error : 0.796032, accuarcy : 0.766383\n",
      "total step : 896 \n",
      "error : 0.795539, accuarcy : 0.766383\n",
      "total step : 897 \n",
      "error : 0.795047, accuarcy : 0.766383\n",
      "total step : 898 \n",
      "error : 0.794556, accuarcy : 0.766383\n",
      "total step : 899 \n",
      "error : 0.794066, accuarcy : 0.766383\n",
      "total step : 900 \n",
      "error : 0.793578, accuarcy : 0.766383\n",
      "total step : 901 \n",
      "error : 0.793090, accuarcy : 0.766383\n",
      "total step : 902 \n",
      "error : 0.792604, accuarcy : 0.766383\n",
      "total step : 903 \n",
      "error : 0.792119, accuarcy : 0.766383\n",
      "total step : 904 \n",
      "error : 0.791635, accuarcy : 0.766383\n",
      "total step : 905 \n",
      "error : 0.791152, accuarcy : 0.766383\n",
      "total step : 906 \n",
      "error : 0.790670, accuarcy : 0.766383\n",
      "total step : 907 \n",
      "error : 0.790189, accuarcy : 0.766383\n",
      "total step : 908 \n",
      "error : 0.789709, accuarcy : 0.766383\n",
      "total step : 909 \n",
      "error : 0.789231, accuarcy : 0.766383\n",
      "total step : 910 \n",
      "error : 0.788753, accuarcy : 0.766383\n",
      "total step : 911 \n",
      "error : 0.788277, accuarcy : 0.766383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 912 \n",
      "error : 0.787801, accuarcy : 0.766383\n",
      "total step : 913 \n",
      "error : 0.787327, accuarcy : 0.766883\n",
      "total step : 914 \n",
      "error : 0.786854, accuarcy : 0.766883\n",
      "total step : 915 \n",
      "error : 0.786382, accuarcy : 0.766883\n",
      "total step : 916 \n",
      "error : 0.785910, accuarcy : 0.766883\n",
      "total step : 917 \n",
      "error : 0.785440, accuarcy : 0.767384\n",
      "total step : 918 \n",
      "error : 0.784971, accuarcy : 0.767384\n",
      "total step : 919 \n",
      "error : 0.784503, accuarcy : 0.767384\n",
      "total step : 920 \n",
      "error : 0.784036, accuarcy : 0.767384\n",
      "total step : 921 \n",
      "error : 0.783570, accuarcy : 0.767384\n",
      "total step : 922 \n",
      "error : 0.783105, accuarcy : 0.768384\n",
      "total step : 923 \n",
      "error : 0.782642, accuarcy : 0.768384\n",
      "total step : 924 \n",
      "error : 0.782179, accuarcy : 0.768384\n",
      "total step : 925 \n",
      "error : 0.781717, accuarcy : 0.768384\n",
      "total step : 926 \n",
      "error : 0.781256, accuarcy : 0.768384\n",
      "total step : 927 \n",
      "error : 0.780796, accuarcy : 0.768884\n",
      "total step : 928 \n",
      "error : 0.780338, accuarcy : 0.768884\n",
      "total step : 929 \n",
      "error : 0.779880, accuarcy : 0.768884\n",
      "total step : 930 \n",
      "error : 0.779423, accuarcy : 0.769385\n",
      "total step : 931 \n",
      "error : 0.778967, accuarcy : 0.769385\n",
      "total step : 932 \n",
      "error : 0.778513, accuarcy : 0.769885\n",
      "total step : 933 \n",
      "error : 0.778059, accuarcy : 0.770385\n",
      "total step : 934 \n",
      "error : 0.777606, accuarcy : 0.770885\n",
      "total step : 935 \n",
      "error : 0.777154, accuarcy : 0.770885\n",
      "total step : 936 \n",
      "error : 0.776703, accuarcy : 0.771386\n",
      "total step : 937 \n",
      "error : 0.776254, accuarcy : 0.771386\n",
      "total step : 938 \n",
      "error : 0.775805, accuarcy : 0.771386\n",
      "total step : 939 \n",
      "error : 0.775357, accuarcy : 0.771886\n",
      "total step : 940 \n",
      "error : 0.774910, accuarcy : 0.771886\n",
      "total step : 941 \n",
      "error : 0.774464, accuarcy : 0.772386\n",
      "total step : 942 \n",
      "error : 0.774019, accuarcy : 0.772386\n",
      "total step : 943 \n",
      "error : 0.773575, accuarcy : 0.772886\n",
      "total step : 944 \n",
      "error : 0.773132, accuarcy : 0.772886\n",
      "total step : 945 \n",
      "error : 0.772689, accuarcy : 0.773387\n",
      "total step : 946 \n",
      "error : 0.772248, accuarcy : 0.773887\n",
      "total step : 947 \n",
      "error : 0.771808, accuarcy : 0.773887\n",
      "total step : 948 \n",
      "error : 0.771369, accuarcy : 0.774387\n",
      "total step : 949 \n",
      "error : 0.770930, accuarcy : 0.774887\n",
      "total step : 950 \n",
      "error : 0.770493, accuarcy : 0.774887\n",
      "total step : 951 \n",
      "error : 0.770056, accuarcy : 0.774887\n",
      "total step : 952 \n",
      "error : 0.769620, accuarcy : 0.774887\n",
      "total step : 953 \n",
      "error : 0.769186, accuarcy : 0.774887\n",
      "total step : 954 \n",
      "error : 0.768752, accuarcy : 0.774887\n",
      "total step : 955 \n",
      "error : 0.768319, accuarcy : 0.774887\n",
      "total step : 956 \n",
      "error : 0.767887, accuarcy : 0.775388\n",
      "total step : 957 \n",
      "error : 0.767456, accuarcy : 0.775388\n",
      "total step : 958 \n",
      "error : 0.767026, accuarcy : 0.775388\n",
      "total step : 959 \n",
      "error : 0.766597, accuarcy : 0.775888\n",
      "total step : 960 \n",
      "error : 0.766168, accuarcy : 0.775888\n",
      "total step : 961 \n",
      "error : 0.765741, accuarcy : 0.775888\n",
      "total step : 962 \n",
      "error : 0.765314, accuarcy : 0.775888\n",
      "total step : 963 \n",
      "error : 0.764889, accuarcy : 0.775888\n",
      "total step : 964 \n",
      "error : 0.764464, accuarcy : 0.775888\n",
      "total step : 965 \n",
      "error : 0.764040, accuarcy : 0.775888\n",
      "total step : 966 \n",
      "error : 0.763617, accuarcy : 0.776388\n",
      "total step : 967 \n",
      "error : 0.763195, accuarcy : 0.776388\n",
      "total step : 968 \n",
      "error : 0.762773, accuarcy : 0.776388\n",
      "total step : 969 \n",
      "error : 0.762353, accuarcy : 0.776888\n",
      "total step : 970 \n",
      "error : 0.761933, accuarcy : 0.776888\n",
      "total step : 971 \n",
      "error : 0.761515, accuarcy : 0.777389\n",
      "total step : 972 \n",
      "error : 0.761097, accuarcy : 0.777389\n",
      "total step : 973 \n",
      "error : 0.760680, accuarcy : 0.777389\n",
      "total step : 974 \n",
      "error : 0.760264, accuarcy : 0.777389\n",
      "total step : 975 \n",
      "error : 0.759848, accuarcy : 0.777389\n",
      "total step : 976 \n",
      "error : 0.759434, accuarcy : 0.777389\n",
      "total step : 977 \n",
      "error : 0.759020, accuarcy : 0.777389\n",
      "total step : 978 \n",
      "error : 0.758608, accuarcy : 0.777389\n",
      "total step : 979 \n",
      "error : 0.758196, accuarcy : 0.777389\n",
      "total step : 980 \n",
      "error : 0.757785, accuarcy : 0.777889\n",
      "total step : 981 \n",
      "error : 0.757374, accuarcy : 0.777889\n",
      "total step : 982 \n",
      "error : 0.756965, accuarcy : 0.777889\n",
      "total step : 983 \n",
      "error : 0.756556, accuarcy : 0.777889\n",
      "total step : 984 \n",
      "error : 0.756149, accuarcy : 0.777889\n",
      "total step : 985 \n",
      "error : 0.755742, accuarcy : 0.777889\n",
      "total step : 986 \n",
      "error : 0.755336, accuarcy : 0.777889\n",
      "total step : 987 \n",
      "error : 0.754930, accuarcy : 0.777889\n",
      "total step : 988 \n",
      "error : 0.754526, accuarcy : 0.777889\n",
      "total step : 989 \n",
      "error : 0.754122, accuarcy : 0.777889\n",
      "total step : 990 \n",
      "error : 0.753719, accuarcy : 0.777889\n",
      "total step : 991 \n",
      "error : 0.753317, accuarcy : 0.777889\n",
      "total step : 992 \n",
      "error : 0.752916, accuarcy : 0.777889\n",
      "total step : 993 \n",
      "error : 0.752516, accuarcy : 0.777889\n",
      "total step : 994 \n",
      "error : 0.752116, accuarcy : 0.778389\n",
      "total step : 995 \n",
      "error : 0.751717, accuarcy : 0.778389\n",
      "total step : 996 \n",
      "error : 0.751319, accuarcy : 0.778389\n",
      "total step : 997 \n",
      "error : 0.750922, accuarcy : 0.778389\n",
      "total step : 998 \n",
      "error : 0.750525, accuarcy : 0.778389\n",
      "total step : 999 \n",
      "error : 0.750129, accuarcy : 0.778389\n",
      "total step : 1000 \n",
      "error : 0.749734, accuarcy : 0.778389\n",
      "total step : 1001 \n",
      "error : 0.749340, accuarcy : 0.778389\n",
      "total step : 1002 \n",
      "error : 0.748947, accuarcy : 0.778389\n",
      "total step : 1003 \n",
      "error : 0.748554, accuarcy : 0.779390\n",
      "total step : 1004 \n",
      "error : 0.748162, accuarcy : 0.779890\n",
      "total step : 1005 \n",
      "error : 0.747771, accuarcy : 0.779890\n",
      "total step : 1006 \n",
      "error : 0.747381, accuarcy : 0.779890\n",
      "total step : 1007 \n",
      "error : 0.746991, accuarcy : 0.779890\n",
      "total step : 1008 \n",
      "error : 0.746603, accuarcy : 0.779890\n",
      "total step : 1009 \n",
      "error : 0.746215, accuarcy : 0.779890\n",
      "total step : 1010 \n",
      "error : 0.745827, accuarcy : 0.779890\n",
      "total step : 1011 \n",
      "error : 0.745441, accuarcy : 0.779890\n",
      "total step : 1012 \n",
      "error : 0.745055, accuarcy : 0.779890\n",
      "total step : 1013 \n",
      "error : 0.744670, accuarcy : 0.779890\n",
      "total step : 1014 \n",
      "error : 0.744286, accuarcy : 0.779890\n",
      "total step : 1015 \n",
      "error : 0.743902, accuarcy : 0.779890\n",
      "total step : 1016 \n",
      "error : 0.743519, accuarcy : 0.779890\n",
      "total step : 1017 \n",
      "error : 0.743137, accuarcy : 0.779890\n",
      "total step : 1018 \n",
      "error : 0.742756, accuarcy : 0.779890\n",
      "total step : 1019 \n",
      "error : 0.742375, accuarcy : 0.779890\n",
      "total step : 1020 \n",
      "error : 0.741995, accuarcy : 0.779890\n",
      "total step : 1021 \n",
      "error : 0.741616, accuarcy : 0.779890\n",
      "total step : 1022 \n",
      "error : 0.741238, accuarcy : 0.779890\n",
      "total step : 1023 \n",
      "error : 0.740860, accuarcy : 0.779890\n",
      "total step : 1024 \n",
      "error : 0.740483, accuarcy : 0.780390\n",
      "total step : 1025 \n",
      "error : 0.740107, accuarcy : 0.780390\n",
      "total step : 1026 \n",
      "error : 0.739731, accuarcy : 0.780390\n",
      "total step : 1027 \n",
      "error : 0.739356, accuarcy : 0.780390\n",
      "total step : 1028 \n",
      "error : 0.738982, accuarcy : 0.780390\n",
      "total step : 1029 \n",
      "error : 0.738609, accuarcy : 0.780890\n",
      "total step : 1030 \n",
      "error : 0.738236, accuarcy : 0.780890\n",
      "total step : 1031 \n",
      "error : 0.737864, accuarcy : 0.781891\n",
      "total step : 1032 \n",
      "error : 0.737493, accuarcy : 0.781891\n",
      "total step : 1033 \n",
      "error : 0.737122, accuarcy : 0.781891\n",
      "total step : 1034 \n",
      "error : 0.736752, accuarcy : 0.781891\n",
      "total step : 1035 \n",
      "error : 0.736383, accuarcy : 0.781891\n",
      "total step : 1036 \n",
      "error : 0.736015, accuarcy : 0.781891\n",
      "total step : 1037 \n",
      "error : 0.735647, accuarcy : 0.781891\n",
      "total step : 1038 \n",
      "error : 0.735280, accuarcy : 0.782891\n",
      "total step : 1039 \n",
      "error : 0.734913, accuarcy : 0.782891\n",
      "total step : 1040 \n",
      "error : 0.734547, accuarcy : 0.782891\n",
      "total step : 1041 \n",
      "error : 0.734182, accuarcy : 0.782891\n",
      "total step : 1042 \n",
      "error : 0.733818, accuarcy : 0.783392\n",
      "total step : 1043 \n",
      "error : 0.733454, accuarcy : 0.783392\n",
      "total step : 1044 \n",
      "error : 0.733091, accuarcy : 0.783392\n",
      "total step : 1045 \n",
      "error : 0.732729, accuarcy : 0.783392\n",
      "total step : 1046 \n",
      "error : 0.732367, accuarcy : 0.783392\n",
      "total step : 1047 \n",
      "error : 0.732006, accuarcy : 0.783392\n",
      "total step : 1048 \n",
      "error : 0.731645, accuarcy : 0.783892\n",
      "total step : 1049 \n",
      "error : 0.731286, accuarcy : 0.783892\n",
      "total step : 1050 \n",
      "error : 0.730927, accuarcy : 0.784392\n",
      "total step : 1051 \n",
      "error : 0.730568, accuarcy : 0.784392\n",
      "total step : 1052 \n",
      "error : 0.730210, accuarcy : 0.785393\n",
      "total step : 1053 \n",
      "error : 0.729853, accuarcy : 0.785393\n",
      "total step : 1054 \n",
      "error : 0.729497, accuarcy : 0.785393\n",
      "total step : 1055 \n",
      "error : 0.729141, accuarcy : 0.785393\n",
      "total step : 1056 \n",
      "error : 0.728786, accuarcy : 0.785893\n",
      "total step : 1057 \n",
      "error : 0.728431, accuarcy : 0.786393\n",
      "total step : 1058 \n",
      "error : 0.728078, accuarcy : 0.786893\n",
      "total step : 1059 \n",
      "error : 0.727724, accuarcy : 0.786893\n",
      "total step : 1060 \n",
      "error : 0.727372, accuarcy : 0.786893\n",
      "total step : 1061 \n",
      "error : 0.727020, accuarcy : 0.786893\n",
      "total step : 1062 \n",
      "error : 0.726669, accuarcy : 0.786893\n",
      "total step : 1063 \n",
      "error : 0.726318, accuarcy : 0.787394\n",
      "total step : 1064 \n",
      "error : 0.725968, accuarcy : 0.787394\n",
      "total step : 1065 \n",
      "error : 0.725619, accuarcy : 0.787394\n",
      "total step : 1066 \n",
      "error : 0.725270, accuarcy : 0.787394\n",
      "total step : 1067 \n",
      "error : 0.724922, accuarcy : 0.787394\n",
      "total step : 1068 \n",
      "error : 0.724574, accuarcy : 0.787894\n",
      "total step : 1069 \n",
      "error : 0.724227, accuarcy : 0.787894\n",
      "total step : 1070 \n",
      "error : 0.723881, accuarcy : 0.787894\n",
      "total step : 1071 \n",
      "error : 0.723535, accuarcy : 0.787894\n",
      "total step : 1072 \n",
      "error : 0.723190, accuarcy : 0.787894\n",
      "total step : 1073 \n",
      "error : 0.722846, accuarcy : 0.787894\n",
      "total step : 1074 \n",
      "error : 0.722502, accuarcy : 0.787894\n",
      "total step : 1075 \n",
      "error : 0.722159, accuarcy : 0.787894\n",
      "total step : 1076 \n",
      "error : 0.721816, accuarcy : 0.787894\n",
      "total step : 1077 \n",
      "error : 0.721474, accuarcy : 0.788394\n",
      "total step : 1078 \n",
      "error : 0.721133, accuarcy : 0.788394\n",
      "total step : 1079 \n",
      "error : 0.720792, accuarcy : 0.788394\n",
      "total step : 1080 \n",
      "error : 0.720452, accuarcy : 0.788894\n",
      "total step : 1081 \n",
      "error : 0.720113, accuarcy : 0.788894\n",
      "total step : 1082 \n",
      "error : 0.719774, accuarcy : 0.788894\n",
      "total step : 1083 \n",
      "error : 0.719435, accuarcy : 0.788894\n",
      "total step : 1084 \n",
      "error : 0.719098, accuarcy : 0.788894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 1085 \n",
      "error : 0.718760, accuarcy : 0.789395\n",
      "total step : 1086 \n",
      "error : 0.718424, accuarcy : 0.789395\n",
      "total step : 1087 \n",
      "error : 0.718088, accuarcy : 0.789395\n",
      "total step : 1088 \n",
      "error : 0.717752, accuarcy : 0.789895\n",
      "total step : 1089 \n",
      "error : 0.717418, accuarcy : 0.789895\n",
      "total step : 1090 \n",
      "error : 0.717083, accuarcy : 0.789895\n",
      "total step : 1091 \n",
      "error : 0.716750, accuarcy : 0.790395\n",
      "total step : 1092 \n",
      "error : 0.716417, accuarcy : 0.790395\n",
      "total step : 1093 \n",
      "error : 0.716084, accuarcy : 0.790395\n",
      "total step : 1094 \n",
      "error : 0.715752, accuarcy : 0.790395\n",
      "total step : 1095 \n",
      "error : 0.715421, accuarcy : 0.790395\n",
      "total step : 1096 \n",
      "error : 0.715090, accuarcy : 0.790395\n",
      "total step : 1097 \n",
      "error : 0.714760, accuarcy : 0.790395\n",
      "total step : 1098 \n",
      "error : 0.714431, accuarcy : 0.791396\n",
      "total step : 1099 \n",
      "error : 0.714102, accuarcy : 0.791396\n",
      "total step : 1100 \n",
      "error : 0.713773, accuarcy : 0.791396\n",
      "total step : 1101 \n",
      "error : 0.713445, accuarcy : 0.791896\n",
      "total step : 1102 \n",
      "error : 0.713118, accuarcy : 0.792396\n",
      "total step : 1103 \n",
      "error : 0.712791, accuarcy : 0.792396\n",
      "total step : 1104 \n",
      "error : 0.712465, accuarcy : 0.792396\n",
      "total step : 1105 \n",
      "error : 0.712139, accuarcy : 0.792396\n",
      "total step : 1106 \n",
      "error : 0.711814, accuarcy : 0.792396\n",
      "total step : 1107 \n",
      "error : 0.711490, accuarcy : 0.792396\n",
      "total step : 1108 \n",
      "error : 0.711166, accuarcy : 0.792396\n",
      "total step : 1109 \n",
      "error : 0.710842, accuarcy : 0.792396\n",
      "total step : 1110 \n",
      "error : 0.710519, accuarcy : 0.792396\n",
      "total step : 1111 \n",
      "error : 0.710197, accuarcy : 0.792396\n",
      "total step : 1112 \n",
      "error : 0.709875, accuarcy : 0.792396\n",
      "total step : 1113 \n",
      "error : 0.709554, accuarcy : 0.792396\n",
      "total step : 1114 \n",
      "error : 0.709233, accuarcy : 0.792396\n",
      "total step : 1115 \n",
      "error : 0.708913, accuarcy : 0.792396\n",
      "total step : 1116 \n",
      "error : 0.708594, accuarcy : 0.792396\n",
      "total step : 1117 \n",
      "error : 0.708274, accuarcy : 0.792396\n",
      "total step : 1118 \n",
      "error : 0.707956, accuarcy : 0.792396\n",
      "total step : 1119 \n",
      "error : 0.707638, accuarcy : 0.792396\n",
      "total step : 1120 \n",
      "error : 0.707321, accuarcy : 0.792396\n",
      "total step : 1121 \n",
      "error : 0.707004, accuarcy : 0.792396\n",
      "total step : 1122 \n",
      "error : 0.706687, accuarcy : 0.792396\n",
      "total step : 1123 \n",
      "error : 0.706371, accuarcy : 0.792396\n",
      "total step : 1124 \n",
      "error : 0.706056, accuarcy : 0.792396\n",
      "total step : 1125 \n",
      "error : 0.705741, accuarcy : 0.792396\n",
      "total step : 1126 \n",
      "error : 0.705427, accuarcy : 0.792896\n",
      "total step : 1127 \n",
      "error : 0.705113, accuarcy : 0.793397\n",
      "total step : 1128 \n",
      "error : 0.704800, accuarcy : 0.793397\n",
      "total step : 1129 \n",
      "error : 0.704487, accuarcy : 0.793897\n",
      "total step : 1130 \n",
      "error : 0.704175, accuarcy : 0.793897\n",
      "total step : 1131 \n",
      "error : 0.703863, accuarcy : 0.793897\n",
      "total step : 1132 \n",
      "error : 0.703552, accuarcy : 0.794397\n",
      "total step : 1133 \n",
      "error : 0.703242, accuarcy : 0.794897\n",
      "total step : 1134 \n",
      "error : 0.702932, accuarcy : 0.794897\n",
      "total step : 1135 \n",
      "error : 0.702622, accuarcy : 0.794897\n",
      "total step : 1136 \n",
      "error : 0.702313, accuarcy : 0.795398\n",
      "total step : 1137 \n",
      "error : 0.702004, accuarcy : 0.795898\n",
      "total step : 1138 \n",
      "error : 0.701696, accuarcy : 0.795898\n",
      "total step : 1139 \n",
      "error : 0.701389, accuarcy : 0.796398\n",
      "total step : 1140 \n",
      "error : 0.701082, accuarcy : 0.796398\n",
      "total step : 1141 \n",
      "error : 0.700775, accuarcy : 0.796398\n",
      "total step : 1142 \n",
      "error : 0.700469, accuarcy : 0.796398\n",
      "total step : 1143 \n",
      "error : 0.700164, accuarcy : 0.796398\n",
      "total step : 1144 \n",
      "error : 0.699858, accuarcy : 0.796398\n",
      "total step : 1145 \n",
      "error : 0.699554, accuarcy : 0.796898\n",
      "total step : 1146 \n",
      "error : 0.699250, accuarcy : 0.796898\n",
      "total step : 1147 \n",
      "error : 0.698946, accuarcy : 0.796898\n",
      "total step : 1148 \n",
      "error : 0.698643, accuarcy : 0.796898\n",
      "total step : 1149 \n",
      "error : 0.698341, accuarcy : 0.796898\n",
      "total step : 1150 \n",
      "error : 0.698039, accuarcy : 0.796898\n",
      "total step : 1151 \n",
      "error : 0.697737, accuarcy : 0.796898\n",
      "total step : 1152 \n",
      "error : 0.697436, accuarcy : 0.797399\n",
      "total step : 1153 \n",
      "error : 0.697135, accuarcy : 0.797399\n",
      "total step : 1154 \n",
      "error : 0.696835, accuarcy : 0.797399\n",
      "total step : 1155 \n",
      "error : 0.696536, accuarcy : 0.797399\n",
      "total step : 1156 \n",
      "error : 0.696236, accuarcy : 0.797399\n",
      "total step : 1157 \n",
      "error : 0.695938, accuarcy : 0.797399\n",
      "total step : 1158 \n",
      "error : 0.695640, accuarcy : 0.797899\n",
      "total step : 1159 \n",
      "error : 0.695342, accuarcy : 0.798399\n",
      "total step : 1160 \n",
      "error : 0.695045, accuarcy : 0.799400\n",
      "total step : 1161 \n",
      "error : 0.694748, accuarcy : 0.799900\n",
      "total step : 1162 \n",
      "error : 0.694452, accuarcy : 0.799900\n",
      "total step : 1163 \n",
      "error : 0.694156, accuarcy : 0.799900\n",
      "total step : 1164 \n",
      "error : 0.693860, accuarcy : 0.799900\n",
      "total step : 1165 \n",
      "error : 0.693566, accuarcy : 0.799900\n",
      "total step : 1166 \n",
      "error : 0.693271, accuarcy : 0.799900\n",
      "total step : 1167 \n",
      "error : 0.692977, accuarcy : 0.799900\n",
      "total step : 1168 \n",
      "error : 0.692684, accuarcy : 0.799900\n",
      "total step : 1169 \n",
      "error : 0.692391, accuarcy : 0.799900\n",
      "total step : 1170 \n",
      "error : 0.692098, accuarcy : 0.799900\n",
      "total step : 1171 \n",
      "error : 0.691806, accuarcy : 0.800400\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 각 숫자에 대해 이진 분류 모델 학습 및 가중치 저장\n",
    "weights = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Training model for number {i}\")\n",
    "    train_X, train_y = make_sample(idx = i)\n",
    "    train_X = np.insert(train_X, 0, 1, axis=1)\n",
    "    w = train(train_X, train_y)\n",
    "    weights.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 이미지의 숫자를 예측하는 함수\n",
    "def predict_number(image, weights):\n",
    "    test_X = image.astype('float') / 255\n",
    "    test_X = test_X.reshape(-1)\n",
    "    test_X = np.insert(test_X, 0, 1)\n",
    "    \n",
    "    predictions = []\n",
    "    for w in weights:\n",
    "        pred = 1 / (1 + np.exp(-test_X.dot(w)))\n",
    "        predictions.append(pred[0])\n",
    "    \n",
    "    return np.argmax(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of test_X :  (10000, 785)\n",
      "shape of test_label :  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "test_X = test_raw_img.astype('float')/255    \n",
    "test_X = test_X.reshape(len(test_X.squeeze()), -1)\n",
    "test_X = np.insert(test_X, 0, 1, axis=1) # + bias\n",
    "print('shape of test_X : ', test_X.shape)\n",
    "print('shape of test_label : ', test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  0.5091\n"
     ]
    }
   ],
   "source": [
    "# make prediction using argmax\n",
    "max_preds = []\n",
    "for test_img in test_raw_img:\n",
    "    pred = predict_number(test_img, weights)\n",
    "    max_preds.append(pred)\n",
    "\n",
    "max_pred = np.array(max_preds).reshape(-1)\n",
    "acc = np.sum(np.where(test_label==max_pred, True, False))/len(test_X)\n",
    "print('accuracy : ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted number: 6\n",
      "Actual number: 6\n"
     ]
    }
   ],
   "source": [
    "# 테스트 이미지를 사용하여 숫자 예측\n",
    "test_image_idx = 100\n",
    "predicted_number = predict_number(test_raw_img[test_image_idx], weights)\n",
    "print(f\"Predicted number: {predicted_number}\")\n",
    "print(f\"Actual number: {test_label[test_image_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted number: 0\n",
      "Actual number: 9\n"
     ]
    }
   ],
   "source": [
    "# 테스트 이미지를 사용하여 숫자 예측\n",
    "test_image_idx = 150\n",
    "predicted_number = predict_number(test_raw_img[test_image_idx], weights)\n",
    "print(f\"Predicted number: {predicted_number}\")\n",
    "print(f\"Actual number: {test_label[test_image_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(train_raw_img, train_label), (test_raw_img, test_label) = load_mnist(flatten=False, normalize=False)\n",
    "print(train_raw_img.shape)\n",
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5923\n",
      "(1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# train_dataset split according to the number\n",
    "new_train_img = [[] for _ in range(10)]\n",
    "new_train_label = [[] for _ in range(10)]\n",
    "\n",
    "for i in range(len(train_label)) :\n",
    "    new_train_img[train_label[i]].append(train_raw_img[i])\n",
    "    new_train_label[train_label[i]].append(train_label[i])\n",
    "\n",
    "print(len(new_train_img[0])) # 0에 해당하는 image 개수\n",
    "print(new_train_img[0][0].shape) # 0에 해당하는 image중 첫번째 image의 shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx에 해당하는 숫자를 대상으로 샘플 데이터 생성\n",
    "def make_sample(idx) :\n",
    "    sample_img = []\n",
    "    sample_label = []\n",
    "    \n",
    "    # data sampling \n",
    "    for i in range(10) :\n",
    "        if i == idx :\n",
    "            sample_img += new_train_img[i][:1000]\n",
    "            sample_label += (new_train_label[i][:1000])\n",
    "        else :\n",
    "            sample_img += new_train_img[i][:111]\n",
    "            sample_label += (new_train_label[i][:111])\n",
    "\n",
    "    sample_img = np.array(sample_img)\n",
    "    sample_label = np.array(sample_label)\n",
    "    \n",
    "    # normalization (set value 0 ~ 1)\n",
    "    sample_img = sample_img.astype('float')/255\n",
    "    \n",
    "    # target number는 1, 아니면 0\n",
    "    sample_label = np.where(sample_label==idx, 1 ,0)\n",
    "    \n",
    "    # reshape\n",
    "    sample_img = sample_img.reshape(len(sample_img.squeeze()), -1)\n",
    "    sample_label = sample_label.reshape(len(sample_label.squeeze()), -1)\n",
    "    \n",
    "    return sample_img, sample_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999, 1)\n"
     ]
    }
   ],
   "source": [
    "# idx = target number\n",
    "train_X, train_y = make_sample(idx = 0)\n",
    "# bias 추가\n",
    "train_X = np.insert(train_X, 0, 1, axis=1)\n",
    "\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_regularization(w, lamb = 0.01) :\n",
    "    # L2 regularization\n",
    "    l2_reg = lamb * np.sum(np.square(w))\n",
    "    return l2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "def train(X, y) :\n",
    "    w = np.random.randn(len(X[0]), 1) # \n",
    "    lr = 0.01 # learning rate(수정)\n",
    "    step = 0\n",
    "    acc = 0\n",
    "    \n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    \n",
    "    while (acc < 0.85) :\n",
    "        step += 1\n",
    "        correct = 0\n",
    "        \n",
    "        # predict\n",
    "        preds = 1 / (1+np.exp(-X.dot(w)))\n",
    "        loss = CrossEntropyLoss(preds, y)+L2_regularization(w,0.01)\n",
    "        \n",
    "        result = np.where(preds>0.5, 1, 0)\n",
    "        acc = np.sum(np.where(result==y, True, False))/len(preds)\n",
    "        \n",
    "        print(\"total step : %d \" % step)\n",
    "        print(\"error : %f, accuarcy : %f\" % (loss, acc))\n",
    "        \n",
    "        loss_history.append(loss)\n",
    "        acc_history.append(acc)\n",
    "        \n",
    "        # gradient descent 수행\n",
    "        gradient = np.dot(X.T, (preds - y)) / len(X)\n",
    "        gradient += 0.01 * w\n",
    "        w -= lr * gradient\n",
    "    \n",
    "    # loss와 accuracy 시각화\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(loss_history)\n",
    "    plt.title(\"Loss history\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(acc_history)\n",
    "    plt.title(\"Accuracy history\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "\n",
    "    plt.show()\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step : 1 \n",
      "error : 12.196674, accuarcy : 0.520260\n",
      "total step : 2 \n",
      "error : 12.078851, accuarcy : 0.526263\n",
      "total step : 3 \n",
      "error : 11.963972, accuarcy : 0.531766\n",
      "total step : 4 \n",
      "error : 11.852293, accuarcy : 0.538769\n",
      "total step : 5 \n",
      "error : 11.744051, accuarcy : 0.542271\n",
      "total step : 6 \n",
      "error : 11.639455, accuarcy : 0.546273\n",
      "total step : 7 \n",
      "error : 11.538690, accuarcy : 0.551776\n",
      "total step : 8 \n",
      "error : 11.441911, accuarcy : 0.552776\n",
      "total step : 9 \n",
      "error : 11.349230, accuarcy : 0.563782\n",
      "total step : 10 \n",
      "error : 11.260714, accuarcy : 0.567784\n",
      "total step : 11 \n",
      "error : 11.176383, accuarcy : 0.576288\n",
      "total step : 12 \n",
      "error : 11.096208, accuarcy : 0.583792\n",
      "total step : 13 \n",
      "error : 11.020124, accuarcy : 0.593297\n",
      "total step : 14 \n",
      "error : 10.948034, accuarcy : 0.599800\n",
      "total step : 15 \n",
      "error : 10.879815, accuarcy : 0.606803\n",
      "total step : 16 \n",
      "error : 10.815326, accuarcy : 0.613307\n",
      "total step : 17 \n",
      "error : 10.754410, accuarcy : 0.617309\n",
      "total step : 18 \n",
      "error : 10.696899, accuarcy : 0.624812\n",
      "total step : 19 \n",
      "error : 10.642618, accuarcy : 0.631316\n",
      "total step : 20 \n",
      "error : 10.591392, accuarcy : 0.639320\n",
      "total step : 21 \n",
      "error : 10.543043, accuarcy : 0.648324\n",
      "total step : 22 \n",
      "error : 10.497401, accuarcy : 0.650825\n",
      "total step : 23 \n",
      "error : 10.454301, accuarcy : 0.655828\n",
      "total step : 24 \n",
      "error : 10.413585, accuarcy : 0.660330\n",
      "total step : 25 \n",
      "error : 10.375106, accuarcy : 0.663832\n",
      "total step : 26 \n",
      "error : 10.338723, accuarcy : 0.669335\n",
      "total step : 27 \n",
      "error : 10.304306, accuarcy : 0.675838\n",
      "total step : 28 \n",
      "error : 10.271731, accuarcy : 0.678339\n",
      "total step : 29 \n",
      "error : 10.240881, accuarcy : 0.682841\n",
      "total step : 30 \n",
      "error : 10.211647, accuarcy : 0.684842\n",
      "total step : 31 \n",
      "error : 10.183926, accuarcy : 0.686843\n",
      "total step : 32 \n",
      "error : 10.157620, accuarcy : 0.691846\n",
      "total step : 33 \n",
      "error : 10.132638, accuarcy : 0.692346\n",
      "total step : 34 \n",
      "error : 10.108894, accuarcy : 0.697349\n",
      "total step : 35 \n",
      "error : 10.086307, accuarcy : 0.704852\n",
      "total step : 36 \n",
      "error : 10.064801, accuarcy : 0.708354\n",
      "total step : 37 \n",
      "error : 10.044306, accuarcy : 0.707854\n",
      "total step : 38 \n",
      "error : 10.024755, accuarcy : 0.710855\n",
      "total step : 39 \n",
      "error : 10.006088, accuarcy : 0.712356\n",
      "total step : 40 \n",
      "error : 9.988247, accuarcy : 0.715358\n",
      "total step : 41 \n",
      "error : 9.971178, accuarcy : 0.717359\n",
      "total step : 42 \n",
      "error : 9.954832, accuarcy : 0.720360\n",
      "total step : 43 \n",
      "error : 9.939164, accuarcy : 0.724862\n",
      "total step : 44 \n",
      "error : 9.924131, accuarcy : 0.726863\n",
      "total step : 45 \n",
      "error : 9.909692, accuarcy : 0.730865\n",
      "total step : 46 \n",
      "error : 9.895811, accuarcy : 0.731366\n",
      "total step : 47 \n",
      "error : 9.882454, accuarcy : 0.732366\n",
      "total step : 48 \n",
      "error : 9.869587, accuarcy : 0.733367\n",
      "total step : 49 \n",
      "error : 9.857183, accuarcy : 0.734867\n",
      "total step : 50 \n",
      "error : 9.845211, accuarcy : 0.735868\n",
      "total step : 51 \n",
      "error : 9.833647, accuarcy : 0.737369\n",
      "total step : 52 \n",
      "error : 9.822466, accuarcy : 0.739870\n",
      "total step : 53 \n",
      "error : 9.811645, accuarcy : 0.740370\n",
      "total step : 54 \n",
      "error : 9.801163, accuarcy : 0.741871\n",
      "total step : 55 \n",
      "error : 9.791000, accuarcy : 0.743372\n",
      "total step : 56 \n",
      "error : 9.781138, accuarcy : 0.744372\n",
      "total step : 57 \n",
      "error : 9.771559, accuarcy : 0.745373\n",
      "total step : 58 \n",
      "error : 9.762246, accuarcy : 0.744372\n",
      "total step : 59 \n",
      "error : 9.753185, accuarcy : 0.748374\n",
      "total step : 60 \n",
      "error : 9.744362, accuarcy : 0.749375\n",
      "total step : 61 \n",
      "error : 9.735762, accuarcy : 0.749875\n",
      "total step : 62 \n",
      "error : 9.727374, accuarcy : 0.751876\n",
      "total step : 63 \n",
      "error : 9.719185, accuarcy : 0.753377\n",
      "total step : 64 \n",
      "error : 9.711185, accuarcy : 0.753377\n",
      "total step : 65 \n",
      "error : 9.703364, accuarcy : 0.754877\n",
      "total step : 66 \n",
      "error : 9.695711, accuarcy : 0.755378\n",
      "total step : 67 \n",
      "error : 9.688218, accuarcy : 0.756378\n",
      "total step : 68 \n",
      "error : 9.680876, accuarcy : 0.756878\n",
      "total step : 69 \n",
      "error : 9.673678, accuarcy : 0.755878\n",
      "total step : 70 \n",
      "error : 9.666616, accuarcy : 0.757379\n",
      "total step : 71 \n",
      "error : 9.659682, accuarcy : 0.758379\n",
      "total step : 72 \n",
      "error : 9.652870, accuarcy : 0.758379\n",
      "total step : 73 \n",
      "error : 9.646175, accuarcy : 0.759880\n",
      "total step : 74 \n",
      "error : 9.639590, accuarcy : 0.761881\n",
      "total step : 75 \n",
      "error : 9.633110, accuarcy : 0.762881\n",
      "total step : 76 \n",
      "error : 9.626729, accuarcy : 0.763882\n",
      "total step : 77 \n",
      "error : 9.620444, accuarcy : 0.763882\n",
      "total step : 78 \n",
      "error : 9.614249, accuarcy : 0.765383\n",
      "total step : 79 \n",
      "error : 9.608140, accuarcy : 0.766883\n",
      "total step : 80 \n",
      "error : 9.602113, accuarcy : 0.767884\n",
      "total step : 81 \n",
      "error : 9.596165, accuarcy : 0.768384\n",
      "total step : 82 \n",
      "error : 9.590291, accuarcy : 0.768884\n",
      "total step : 83 \n",
      "error : 9.584489, accuarcy : 0.770385\n",
      "total step : 84 \n",
      "error : 9.578756, accuarcy : 0.771386\n",
      "total step : 85 \n",
      "error : 9.573087, accuarcy : 0.771386\n",
      "total step : 86 \n",
      "error : 9.567482, accuarcy : 0.771886\n",
      "total step : 87 \n",
      "error : 9.561936, accuarcy : 0.771886\n",
      "total step : 88 \n",
      "error : 9.556447, accuarcy : 0.771886\n",
      "total step : 89 \n",
      "error : 9.551014, accuarcy : 0.771886\n",
      "total step : 90 \n",
      "error : 9.545633, accuarcy : 0.772386\n",
      "total step : 91 \n",
      "error : 9.540304, accuarcy : 0.772886\n",
      "total step : 92 \n",
      "error : 9.535023, accuarcy : 0.774387\n",
      "total step : 93 \n",
      "error : 9.529788, accuarcy : 0.774887\n",
      "total step : 94 \n",
      "error : 9.524599, accuarcy : 0.775388\n",
      "total step : 95 \n",
      "error : 9.519453, accuarcy : 0.775388\n",
      "total step : 96 \n",
      "error : 9.514349, accuarcy : 0.775388\n",
      "total step : 97 \n",
      "error : 9.509285, accuarcy : 0.775888\n",
      "total step : 98 \n",
      "error : 9.504260, accuarcy : 0.775888\n",
      "total step : 99 \n",
      "error : 9.499273, accuarcy : 0.776388\n",
      "total step : 100 \n",
      "error : 9.494321, accuarcy : 0.776888\n",
      "total step : 101 \n",
      "error : 9.489405, accuarcy : 0.777389\n",
      "total step : 102 \n",
      "error : 9.484522, accuarcy : 0.777389\n",
      "total step : 103 \n",
      "error : 9.479672, accuarcy : 0.778389\n",
      "total step : 104 \n",
      "error : 9.474853, accuarcy : 0.778889\n",
      "total step : 105 \n",
      "error : 9.470065, accuarcy : 0.777889\n",
      "total step : 106 \n",
      "error : 9.465307, accuarcy : 0.777889\n",
      "total step : 107 \n",
      "error : 9.460577, accuarcy : 0.777889\n",
      "total step : 108 \n",
      "error : 9.455875, accuarcy : 0.778889\n",
      "total step : 109 \n",
      "error : 9.451200, accuarcy : 0.778889\n",
      "total step : 110 \n",
      "error : 9.446551, accuarcy : 0.778889\n",
      "total step : 111 \n",
      "error : 9.441928, accuarcy : 0.779890\n",
      "total step : 112 \n",
      "error : 9.437330, accuarcy : 0.780390\n",
      "total step : 113 \n",
      "error : 9.432756, accuarcy : 0.780390\n",
      "total step : 114 \n",
      "error : 9.428205, accuarcy : 0.780390\n",
      "total step : 115 \n",
      "error : 9.423677, accuarcy : 0.781391\n",
      "total step : 116 \n",
      "error : 9.419171, accuarcy : 0.781391\n",
      "total step : 117 \n",
      "error : 9.414687, accuarcy : 0.781891\n",
      "total step : 118 \n",
      "error : 9.410224, accuarcy : 0.782391\n",
      "total step : 119 \n",
      "error : 9.405781, accuarcy : 0.782391\n",
      "total step : 120 \n",
      "error : 9.401359, accuarcy : 0.783892\n",
      "total step : 121 \n",
      "error : 9.396956, accuarcy : 0.784392\n",
      "total step : 122 \n",
      "error : 9.392573, accuarcy : 0.785393\n",
      "total step : 123 \n",
      "error : 9.388208, accuarcy : 0.785893\n",
      "total step : 124 \n",
      "error : 9.383861, accuarcy : 0.786393\n",
      "total step : 125 \n",
      "error : 9.379533, accuarcy : 0.787394\n",
      "total step : 126 \n",
      "error : 9.375222, accuarcy : 0.787394\n",
      "total step : 127 \n",
      "error : 9.370928, accuarcy : 0.787894\n",
      "total step : 128 \n",
      "error : 9.366651, accuarcy : 0.788394\n",
      "total step : 129 \n",
      "error : 9.362391, accuarcy : 0.788394\n",
      "total step : 130 \n",
      "error : 9.358146, accuarcy : 0.788394\n",
      "total step : 131 \n",
      "error : 9.353918, accuarcy : 0.788394\n",
      "total step : 132 \n",
      "error : 9.349705, accuarcy : 0.788394\n",
      "total step : 133 \n",
      "error : 9.345508, accuarcy : 0.788394\n",
      "total step : 134 \n",
      "error : 9.341325, accuarcy : 0.788394\n",
      "total step : 135 \n",
      "error : 9.337157, accuarcy : 0.789395\n",
      "total step : 136 \n",
      "error : 9.333004, accuarcy : 0.791396\n",
      "total step : 137 \n",
      "error : 9.328865, accuarcy : 0.791396\n",
      "total step : 138 \n",
      "error : 9.324739, accuarcy : 0.791396\n",
      "total step : 139 \n",
      "error : 9.320628, accuarcy : 0.792896\n",
      "total step : 140 \n",
      "error : 9.316530, accuarcy : 0.792896\n",
      "total step : 141 \n",
      "error : 9.312446, accuarcy : 0.793397\n",
      "total step : 142 \n",
      "error : 9.308374, accuarcy : 0.793397\n",
      "total step : 143 \n",
      "error : 9.304316, accuarcy : 0.793397\n",
      "total step : 144 \n",
      "error : 9.300270, accuarcy : 0.793897\n",
      "total step : 145 \n",
      "error : 9.296237, accuarcy : 0.793897\n",
      "total step : 146 \n",
      "error : 9.292216, accuarcy : 0.793897\n",
      "total step : 147 \n",
      "error : 9.288208, accuarcy : 0.793897\n",
      "total step : 148 \n",
      "error : 9.284211, accuarcy : 0.794397\n",
      "total step : 149 \n",
      "error : 9.280226, accuarcy : 0.794397\n",
      "total step : 150 \n",
      "error : 9.276253, accuarcy : 0.795898\n",
      "total step : 151 \n",
      "error : 9.272292, accuarcy : 0.795898\n",
      "total step : 152 \n",
      "error : 9.268342, accuarcy : 0.795398\n",
      "total step : 153 \n",
      "error : 9.264404, accuarcy : 0.795898\n",
      "total step : 154 \n",
      "error : 9.260476, accuarcy : 0.795898\n",
      "total step : 155 \n",
      "error : 9.256560, accuarcy : 0.796398\n",
      "total step : 156 \n",
      "error : 9.252654, accuarcy : 0.796398\n",
      "total step : 157 \n",
      "error : 9.248760, accuarcy : 0.796398\n",
      "total step : 158 \n",
      "error : 9.244876, accuarcy : 0.795898\n",
      "total step : 159 \n",
      "error : 9.241002, accuarcy : 0.795898\n",
      "total step : 160 \n",
      "error : 9.237139, accuarcy : 0.795898\n",
      "total step : 161 \n",
      "error : 9.233286, accuarcy : 0.796398\n",
      "total step : 162 \n",
      "error : 9.229443, accuarcy : 0.796398\n",
      "total step : 163 \n",
      "error : 9.225611, accuarcy : 0.796898\n",
      "total step : 164 \n",
      "error : 9.221788, accuarcy : 0.797399\n",
      "total step : 165 \n",
      "error : 9.217976, accuarcy : 0.797399\n",
      "total step : 166 \n",
      "error : 9.214173, accuarcy : 0.798399\n",
      "total step : 167 \n",
      "error : 9.210380, accuarcy : 0.798899\n",
      "total step : 168 \n",
      "error : 9.206596, accuarcy : 0.799900\n",
      "total step : 169 \n",
      "error : 9.202822, accuarcy : 0.801401\n",
      "total step : 170 \n",
      "error : 9.199058, accuarcy : 0.801401\n",
      "total step : 171 \n",
      "error : 9.195303, accuarcy : 0.801401\n",
      "total step : 172 \n",
      "error : 9.191557, accuarcy : 0.801901\n",
      "total step : 173 \n",
      "error : 9.187820, accuarcy : 0.801901\n",
      "total step : 174 \n",
      "error : 9.184092, accuarcy : 0.801901\n",
      "total step : 175 \n",
      "error : 9.180374, accuarcy : 0.801901\n",
      "total step : 176 \n",
      "error : 9.176664, accuarcy : 0.801901\n",
      "total step : 177 \n",
      "error : 9.172963, accuarcy : 0.802401\n",
      "total step : 178 \n",
      "error : 9.169272, accuarcy : 0.802401\n",
      "total step : 179 \n",
      "error : 9.165588, accuarcy : 0.802401\n",
      "total step : 180 \n",
      "error : 9.161914, accuarcy : 0.802901\n",
      "total step : 181 \n",
      "error : 9.158248, accuarcy : 0.803402\n",
      "total step : 182 \n",
      "error : 9.154591, accuarcy : 0.803402\n",
      "total step : 183 \n",
      "error : 9.150942, accuarcy : 0.803402\n",
      "total step : 184 \n",
      "error : 9.147301, accuarcy : 0.803902\n",
      "total step : 185 \n",
      "error : 9.143669, accuarcy : 0.804402\n",
      "total step : 186 \n",
      "error : 9.140045, accuarcy : 0.805403\n",
      "total step : 187 \n",
      "error : 9.136430, accuarcy : 0.806403\n",
      "total step : 188 \n",
      "error : 9.132822, accuarcy : 0.806403\n",
      "total step : 189 \n",
      "error : 9.129223, accuarcy : 0.807904\n",
      "total step : 190 \n",
      "error : 9.125632, accuarcy : 0.807904\n",
      "total step : 191 \n",
      "error : 9.122049, accuarcy : 0.808904\n",
      "total step : 192 \n",
      "error : 9.118473, accuarcy : 0.809405\n",
      "total step : 193 \n",
      "error : 9.114906, accuarcy : 0.809905\n",
      "total step : 194 \n",
      "error : 9.111347, accuarcy : 0.810405\n",
      "total step : 195 \n",
      "error : 9.107795, accuarcy : 0.810905\n",
      "total step : 196 \n",
      "error : 9.104251, accuarcy : 0.811406\n",
      "total step : 197 \n",
      "error : 9.100715, accuarcy : 0.811406\n",
      "total step : 198 \n",
      "error : 9.097186, accuarcy : 0.811906\n",
      "total step : 199 \n",
      "error : 9.093665, accuarcy : 0.812406\n",
      "total step : 200 \n",
      "error : 9.090151, accuarcy : 0.812406\n",
      "total step : 201 \n",
      "error : 9.086645, accuarcy : 0.812406\n",
      "total step : 202 \n",
      "error : 9.083147, accuarcy : 0.812406\n",
      "total step : 203 \n",
      "error : 9.079655, accuarcy : 0.812406\n",
      "total step : 204 \n",
      "error : 9.076172, accuarcy : 0.812906\n",
      "total step : 205 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error : 9.072695, accuarcy : 0.813407\n",
      "total step : 206 \n",
      "error : 9.069226, accuarcy : 0.813907\n",
      "total step : 207 \n",
      "error : 9.065764, accuarcy : 0.813907\n",
      "total step : 208 \n",
      "error : 9.062309, accuarcy : 0.814407\n",
      "total step : 209 \n",
      "error : 9.058862, accuarcy : 0.814407\n",
      "total step : 210 \n",
      "error : 9.055421, accuarcy : 0.813907\n",
      "total step : 211 \n",
      "error : 9.051987, accuarcy : 0.814407\n",
      "total step : 212 \n",
      "error : 9.048561, accuarcy : 0.814907\n",
      "total step : 213 \n",
      "error : 9.045142, accuarcy : 0.815408\n",
      "total step : 214 \n",
      "error : 9.041729, accuarcy : 0.815408\n",
      "total step : 215 \n",
      "error : 9.038323, accuarcy : 0.815408\n",
      "total step : 216 \n",
      "error : 9.034925, accuarcy : 0.815408\n",
      "total step : 217 \n",
      "error : 9.031533, accuarcy : 0.815408\n",
      "total step : 218 \n",
      "error : 9.028148, accuarcy : 0.815408\n",
      "total step : 219 \n",
      "error : 9.024769, accuarcy : 0.815408\n",
      "total step : 220 \n",
      "error : 9.021398, accuarcy : 0.815408\n",
      "total step : 221 \n",
      "error : 9.018033, accuarcy : 0.815408\n",
      "total step : 222 \n",
      "error : 9.014674, accuarcy : 0.815908\n",
      "total step : 223 \n",
      "error : 9.011323, accuarcy : 0.816408\n",
      "total step : 224 \n",
      "error : 9.007978, accuarcy : 0.816408\n",
      "total step : 225 \n",
      "error : 9.004639, accuarcy : 0.816908\n",
      "total step : 226 \n",
      "error : 9.001307, accuarcy : 0.816908\n",
      "total step : 227 \n",
      "error : 8.997982, accuarcy : 0.817409\n",
      "total step : 228 \n",
      "error : 8.994662, accuarcy : 0.817409\n",
      "total step : 229 \n",
      "error : 8.991350, accuarcy : 0.817909\n",
      "total step : 230 \n",
      "error : 8.988043, accuarcy : 0.818909\n",
      "total step : 231 \n",
      "error : 8.984743, accuarcy : 0.818909\n",
      "total step : 232 \n",
      "error : 8.981450, accuarcy : 0.818909\n",
      "total step : 233 \n",
      "error : 8.978162, accuarcy : 0.819910\n",
      "total step : 234 \n",
      "error : 8.974881, accuarcy : 0.819910\n",
      "total step : 235 \n",
      "error : 8.971606, accuarcy : 0.819910\n",
      "total step : 236 \n",
      "error : 8.968338, accuarcy : 0.820410\n",
      "total step : 237 \n",
      "error : 8.965075, accuarcy : 0.820410\n",
      "total step : 238 \n",
      "error : 8.961819, accuarcy : 0.821411\n",
      "total step : 239 \n",
      "error : 8.958569, accuarcy : 0.821411\n",
      "total step : 240 \n",
      "error : 8.955324, accuarcy : 0.821911\n",
      "total step : 241 \n",
      "error : 8.952086, accuarcy : 0.821911\n",
      "total step : 242 \n",
      "error : 8.948854, accuarcy : 0.822411\n",
      "total step : 243 \n",
      "error : 8.945628, accuarcy : 0.822411\n",
      "total step : 244 \n",
      "error : 8.942408, accuarcy : 0.822411\n",
      "total step : 245 \n",
      "error : 8.939193, accuarcy : 0.822411\n",
      "total step : 246 \n",
      "error : 8.935985, accuarcy : 0.822411\n",
      "total step : 247 \n",
      "error : 8.932782, accuarcy : 0.822411\n",
      "total step : 248 \n",
      "error : 8.929586, accuarcy : 0.822411\n",
      "total step : 249 \n",
      "error : 8.926395, accuarcy : 0.822911\n",
      "total step : 250 \n",
      "error : 8.923210, accuarcy : 0.822911\n",
      "total step : 251 \n",
      "error : 8.920031, accuarcy : 0.822911\n",
      "total step : 252 \n",
      "error : 8.916857, accuarcy : 0.823412\n",
      "total step : 253 \n",
      "error : 8.913689, accuarcy : 0.823412\n",
      "total step : 254 \n",
      "error : 8.910527, accuarcy : 0.823412\n",
      "total step : 255 \n",
      "error : 8.907371, accuarcy : 0.823412\n",
      "total step : 256 \n",
      "error : 8.904220, accuarcy : 0.823412\n",
      "total step : 257 \n",
      "error : 8.901075, accuarcy : 0.823412\n",
      "total step : 258 \n",
      "error : 8.897935, accuarcy : 0.823412\n",
      "total step : 259 \n",
      "error : 8.894801, accuarcy : 0.823412\n",
      "total step : 260 \n",
      "error : 8.891672, accuarcy : 0.823912\n",
      "total step : 261 \n",
      "error : 8.888549, accuarcy : 0.824412\n",
      "total step : 262 \n",
      "error : 8.885432, accuarcy : 0.824412\n",
      "total step : 263 \n",
      "error : 8.882320, accuarcy : 0.824412\n",
      "total step : 264 \n",
      "error : 8.879213, accuarcy : 0.825413\n",
      "total step : 265 \n",
      "error : 8.876112, accuarcy : 0.825413\n",
      "total step : 266 \n",
      "error : 8.873016, accuarcy : 0.825413\n",
      "total step : 267 \n",
      "error : 8.869925, accuarcy : 0.825413\n",
      "total step : 268 \n",
      "error : 8.866840, accuarcy : 0.826413\n",
      "total step : 269 \n",
      "error : 8.863760, accuarcy : 0.826413\n",
      "total step : 270 \n",
      "error : 8.860686, accuarcy : 0.826413\n",
      "total step : 271 \n",
      "error : 8.857616, accuarcy : 0.826413\n",
      "total step : 272 \n",
      "error : 8.854552, accuarcy : 0.826413\n",
      "total step : 273 \n",
      "error : 8.851494, accuarcy : 0.826413\n",
      "total step : 274 \n",
      "error : 8.848440, accuarcy : 0.826413\n",
      "total step : 275 \n",
      "error : 8.845392, accuarcy : 0.826413\n",
      "total step : 276 \n",
      "error : 8.842348, accuarcy : 0.826913\n",
      "total step : 277 \n",
      "error : 8.839310, accuarcy : 0.827414\n",
      "total step : 278 \n",
      "error : 8.836277, accuarcy : 0.827414\n",
      "total step : 279 \n",
      "error : 8.833250, accuarcy : 0.827414\n",
      "total step : 280 \n",
      "error : 8.830227, accuarcy : 0.827914\n",
      "total step : 281 \n",
      "error : 8.827209, accuarcy : 0.827914\n",
      "total step : 282 \n",
      "error : 8.824196, accuarcy : 0.829415\n",
      "total step : 283 \n",
      "error : 8.821189, accuarcy : 0.829915\n",
      "total step : 284 \n",
      "error : 8.818186, accuarcy : 0.829915\n",
      "total step : 285 \n",
      "error : 8.815188, accuarcy : 0.830415\n",
      "total step : 286 \n",
      "error : 8.812195, accuarcy : 0.830415\n",
      "total step : 287 \n",
      "error : 8.809207, accuarcy : 0.830915\n",
      "total step : 288 \n",
      "error : 8.806224, accuarcy : 0.831916\n",
      "total step : 289 \n",
      "error : 8.803246, accuarcy : 0.831916\n",
      "total step : 290 \n",
      "error : 8.800273, accuarcy : 0.832416\n",
      "total step : 291 \n",
      "error : 8.797305, accuarcy : 0.832416\n",
      "total step : 292 \n",
      "error : 8.794341, accuarcy : 0.832416\n",
      "total step : 293 \n",
      "error : 8.791382, accuarcy : 0.832916\n",
      "total step : 294 \n",
      "error : 8.788428, accuarcy : 0.833917\n",
      "total step : 295 \n",
      "error : 8.785479, accuarcy : 0.833917\n",
      "total step : 296 \n",
      "error : 8.782535, accuarcy : 0.833917\n",
      "total step : 297 \n",
      "error : 8.779595, accuarcy : 0.833917\n",
      "total step : 298 \n",
      "error : 8.776660, accuarcy : 0.833917\n",
      "total step : 299 \n",
      "error : 8.773730, accuarcy : 0.834417\n",
      "total step : 300 \n",
      "error : 8.770804, accuarcy : 0.834417\n",
      "total step : 301 \n",
      "error : 8.767883, accuarcy : 0.835418\n",
      "total step : 302 \n",
      "error : 8.764966, accuarcy : 0.835418\n",
      "total step : 303 \n",
      "error : 8.762055, accuarcy : 0.835418\n",
      "total step : 304 \n",
      "error : 8.759148, accuarcy : 0.836418\n",
      "total step : 305 \n",
      "error : 8.756245, accuarcy : 0.836918\n",
      "total step : 306 \n",
      "error : 8.753347, accuarcy : 0.836918\n",
      "total step : 307 \n",
      "error : 8.750453, accuarcy : 0.836918\n",
      "total step : 308 \n",
      "error : 8.747564, accuarcy : 0.836918\n",
      "total step : 309 \n",
      "error : 8.744680, accuarcy : 0.836918\n",
      "total step : 310 \n",
      "error : 8.741800, accuarcy : 0.836918\n",
      "total step : 311 \n",
      "error : 8.738924, accuarcy : 0.836918\n",
      "total step : 312 \n",
      "error : 8.736053, accuarcy : 0.837419\n",
      "total step : 313 \n",
      "error : 8.733187, accuarcy : 0.837419\n",
      "total step : 314 \n",
      "error : 8.730324, accuarcy : 0.837419\n",
      "total step : 315 \n",
      "error : 8.727467, accuarcy : 0.837919\n",
      "total step : 316 \n",
      "error : 8.724613, accuarcy : 0.838419\n",
      "total step : 317 \n",
      "error : 8.721764, accuarcy : 0.838419\n",
      "total step : 318 \n",
      "error : 8.718919, accuarcy : 0.838419\n",
      "total step : 319 \n",
      "error : 8.716079, accuarcy : 0.838419\n",
      "total step : 320 \n",
      "error : 8.713243, accuarcy : 0.838419\n",
      "total step : 321 \n",
      "error : 8.710411, accuarcy : 0.838419\n",
      "total step : 322 \n",
      "error : 8.707583, accuarcy : 0.838919\n",
      "total step : 323 \n",
      "error : 8.704760, accuarcy : 0.839420\n",
      "total step : 324 \n",
      "error : 8.701941, accuarcy : 0.839420\n",
      "total step : 325 \n",
      "error : 8.699126, accuarcy : 0.839420\n",
      "total step : 326 \n",
      "error : 8.696316, accuarcy : 0.839420\n",
      "total step : 327 \n",
      "error : 8.693509, accuarcy : 0.839420\n",
      "total step : 328 \n",
      "error : 8.690707, accuarcy : 0.840420\n",
      "total step : 329 \n",
      "error : 8.687909, accuarcy : 0.840420\n",
      "total step : 330 \n",
      "error : 8.685115, accuarcy : 0.840420\n",
      "total step : 331 \n",
      "error : 8.682325, accuarcy : 0.840420\n",
      "total step : 332 \n",
      "error : 8.679540, accuarcy : 0.840920\n",
      "total step : 333 \n",
      "error : 8.676758, accuarcy : 0.840920\n",
      "total step : 334 \n",
      "error : 8.673980, accuarcy : 0.840920\n",
      "total step : 335 \n",
      "error : 8.671207, accuarcy : 0.840920\n",
      "total step : 336 \n",
      "error : 8.668438, accuarcy : 0.840920\n",
      "total step : 337 \n",
      "error : 8.665672, accuarcy : 0.840920\n",
      "total step : 338 \n",
      "error : 8.662911, accuarcy : 0.841421\n",
      "total step : 339 \n",
      "error : 8.660154, accuarcy : 0.841921\n",
      "total step : 340 \n",
      "error : 8.657400, accuarcy : 0.841921\n",
      "total step : 341 \n",
      "error : 8.654651, accuarcy : 0.842421\n",
      "total step : 342 \n",
      "error : 8.651905, accuarcy : 0.842421\n",
      "total step : 343 \n",
      "error : 8.649164, accuarcy : 0.842421\n",
      "total step : 344 \n",
      "error : 8.646426, accuarcy : 0.842921\n",
      "total step : 345 \n",
      "error : 8.643693, accuarcy : 0.843422\n",
      "total step : 346 \n",
      "error : 8.640963, accuarcy : 0.843422\n",
      "total step : 347 \n",
      "error : 8.638237, accuarcy : 0.843422\n",
      "total step : 348 \n",
      "error : 8.635515, accuarcy : 0.843422\n",
      "total step : 349 \n",
      "error : 8.632797, accuarcy : 0.843422\n",
      "total step : 350 \n",
      "error : 8.630083, accuarcy : 0.843922\n",
      "total step : 351 \n",
      "error : 8.627372, accuarcy : 0.844922\n",
      "total step : 352 \n",
      "error : 8.624666, accuarcy : 0.844922\n",
      "total step : 353 \n",
      "error : 8.621963, accuarcy : 0.844922\n",
      "total step : 354 \n",
      "error : 8.619264, accuarcy : 0.845423\n",
      "total step : 355 \n",
      "error : 8.616568, accuarcy : 0.845423\n",
      "total step : 356 \n",
      "error : 8.613877, accuarcy : 0.845423\n",
      "total step : 357 \n",
      "error : 8.611189, accuarcy : 0.845423\n",
      "total step : 358 \n",
      "error : 8.608505, accuarcy : 0.845423\n",
      "total step : 359 \n",
      "error : 8.605824, accuarcy : 0.845423\n",
      "total step : 360 \n",
      "error : 8.603147, accuarcy : 0.845923\n",
      "total step : 361 \n",
      "error : 8.600474, accuarcy : 0.845923\n",
      "total step : 362 \n",
      "error : 8.597805, accuarcy : 0.845923\n",
      "total step : 363 \n",
      "error : 8.595139, accuarcy : 0.845923\n",
      "total step : 364 \n",
      "error : 8.592477, accuarcy : 0.846423\n",
      "total step : 365 \n",
      "error : 8.589819, accuarcy : 0.846923\n",
      "total step : 366 \n",
      "error : 8.587164, accuarcy : 0.846923\n",
      "total step : 367 \n",
      "error : 8.584513, accuarcy : 0.847424\n",
      "total step : 368 \n",
      "error : 8.581865, accuarcy : 0.847424\n",
      "total step : 369 \n",
      "error : 8.579221, accuarcy : 0.847424\n",
      "total step : 370 \n",
      "error : 8.576580, accuarcy : 0.847424\n",
      "total step : 371 \n",
      "error : 8.573943, accuarcy : 0.847424\n",
      "total step : 372 \n",
      "error : 8.571310, accuarcy : 0.847424\n",
      "total step : 373 \n",
      "error : 8.568680, accuarcy : 0.847924\n",
      "total step : 374 \n",
      "error : 8.566054, accuarcy : 0.847924\n",
      "total step : 375 \n",
      "error : 8.563431, accuarcy : 0.848924\n",
      "total step : 376 \n",
      "error : 8.560811, accuarcy : 0.848924\n",
      "total step : 377 \n",
      "error : 8.558195, accuarcy : 0.848924\n",
      "total step : 378 \n",
      "error : 8.555583, accuarcy : 0.848924\n",
      "total step : 379 \n",
      "error : 8.552974, accuarcy : 0.849425\n",
      "total step : 380 \n",
      "error : 8.550368, accuarcy : 0.849925\n",
      "total step : 381 \n",
      "error : 8.547766, accuarcy : 0.849925\n",
      "total step : 382 \n",
      "error : 8.545167, accuarcy : 0.850925\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save weight\n",
    "w = train(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  0.8238\n"
     ]
    }
   ],
   "source": [
    "# eval (accuracy)\n",
    "def eval(idx, w) :\n",
    "    test_X = test_raw_img.astype('float')/255    \n",
    "    test_X = test_X.reshape(len(test_X.squeeze()), -1)\n",
    "    # bias\n",
    "    test_X = np.insert(test_X, 0, 1, axis=1) \n",
    "\n",
    "    test_y = np.where(test_label==idx, 1 ,0)\n",
    "    test_y = test_y.reshape(len(test_y.squeeze()), -1)\n",
    "    \n",
    "    preds = 1/(1+np.exp(-test_X.dot(w)))\n",
    "    result = np.where(preds>0.5, 1, 0)\n",
    "    \n",
    "    acc = np.sum(np.where(result==test_y, True, False))/len(preds)\n",
    "    print('accuracy : ', acc)\n",
    "\n",
    "eval(idx=0, w=w)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ac989702ee82cf85834fc904c407af640e06b0902e988e9cd8a067d885b3b790"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
